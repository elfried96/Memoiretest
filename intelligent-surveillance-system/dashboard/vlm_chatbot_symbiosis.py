"""
VLM Chatbot Symbiosis - Intelligence PartagÃ©e avec Pipeline VLM
================================================================

Chatbot intelligent basÃ© sur le mÃªme VLM que la surveillance avec:
- Thinking/Reasoning partagÃ© avec Qwen2.5-VL-32B
- Chain-of-thought identique Ã  l'analyse surveillance  
- Symbiose temps rÃ©el avec pipeline VLM
- Context awareness complet des donnÃ©es surveillance
"""

import asyncio
import json
import base64
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from io import BytesIO
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from loguru import logger

class AutonomousVLMReflection:
    """
    RÃ©flexion autonome avancÃ©e basÃ©e sur LLaVA-o1, R3V, VL-Rethinker (2024-2025)
    ImplÃ©mente structured reasoning, self-reflection et forced thinking.
    """
    
    def __init__(self):
        self.reflection_enabled = True
        self.structured_reasoning_steps = 4  # Summary, Caption, Reasoning, Conclusion
        self.self_correction_loops = 2
        
    async def structured_reasoning(
        self, 
        question: str, 
        image_data: np.ndarray = None, 
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """LLaVA-o1 style structured reasoning en 4 Ã©tapes."""
        
        if not self.reflection_enabled:
            return {"reasoning_skipped": True}
        
        steps_results = {}
        
        try:
            # Stage 1: Summary (comprendre la question et le contexte)
            steps_results["summary"] = await self._stage_summarize(question, context)
            
            # Stage 2: Caption (observation dÃ©taillÃ©e)  
            steps_results["caption"] = await self._stage_caption(image_data, steps_results["summary"])
            
            # Stage 3: Multi-path Reasoning (exploration de plusieurs raisonnements)
            steps_results["reasoning_candidates"] = await self._stage_reasoning_beam_search(
                question, steps_results["caption"], context
            )
            
            # Stage 4: Self-Reflection & Conclusion
            steps_results["final_conclusion"] = await self._stage_reflect_and_conclude(
                steps_results["reasoning_candidates"], context
            )
            
            return steps_results
            
        except Exception as e:
            logger.error(f"Erreur structured reasoning: {e}")
            return {"error": str(e), "reasoning_incomplete": True}
    
    async def _stage_summarize(self, question: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Stage 1: RÃ©sumÃ© et comprÃ©hension de la tÃ¢che."""
        
        summary = {
            "question_understood": question,
            "key_elements": self._extract_key_elements(question),
            "context_critical": context.get('user_description', '') if context else '',
            "mission_focus": self._determine_mission_focus(question, context),
            "expected_output": self._determine_expected_output(question)
        }
        
        return summary
    
    async def _stage_caption(self, image_data: np.ndarray, summary: Dict[str, Any]) -> Dict[str, Any]:
        """Stage 2: Observation dÃ©taillÃ©e et caption."""
        
        caption = {
            "visual_observations": [
                "Analyse visuelle en cours selon mission",
                "Recherche Ã©lÃ©ments pertinents"
            ],
            "focus_areas": summary.get("mission_focus", []),
            "observation_quality": "good" if image_data is not None else "limited",
            "critical_elements_spotted": []
        }
        
        return caption
    
    def _extract_key_elements(self, question: str) -> List[str]:
        """Extraction des Ã©lÃ©ments clÃ©s de la question."""
        
        key_patterns = {
            "performance": ["performance", "compare", "efficacitÃ©"],
            "tools": ["outils", "tools", "mÃ©thodes"], 
            "configuration": ["config", "optimiser", "amÃ©liorer"],
            "behavior": ["comportement", "suspect", "voir", "dÃ©taille"]
        }
        
        elements = []
        question_lower = question.lower()
        
        for category, patterns in key_patterns.items():
            if any(pattern in question_lower for pattern in patterns):
                elements.append(category)
        
        return elements
    
    def _determine_mission_focus(self, question: str, context: Dict[str, Any]) -> List[str]:
        """DÃ©termine le focus de la mission."""
        
        focus_areas = []
        question_lower = question.lower()
        
        if "performance" in question_lower or "compare" in question_lower:
            focus_areas.append("performance_analysis")
        if "outils" in question_lower:
            focus_areas.append("tool_analysis") 
        if "config" in question_lower:
            focus_areas.append("configuration_optimization")
        if any(word in question_lower for word in ["voir", "dÃ©taille", "suspect"]):
            focus_areas.append("behavioral_analysis")
            
        return focus_areas if focus_areas else ["general_analysis"]
    
    def _determine_expected_output(self, question: str) -> str:
        """DÃ©termine le type de sortie attendu."""
        
        if "compare" in question.lower():
            return "comparative_analysis"
        elif "recommande" in question.lower():
            return "recommendations"
        elif "analyse" in question.lower():
            return "detailed_analysis"
        else:
            return "informative_response"

# Configuration du PYTHONPATH
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Imports pipeline VLM
# Import de base requis pour pipeline VLM
VLM_AVAILABLE = False
RealVLMPipeline = None
get_real_pipeline = None
AnalysisRequest = None
AnalysisResponse = None
PromptBuilder = None

try:
    from .real_pipeline_integration import (
        RealVLMPipeline, 
        RealAnalysisResult,
        get_real_pipeline
    )
    from src.core.types import AnalysisRequest, AnalysisResponse
    from src.core.vlm.prompt_builder import PromptBuilder
    VLM_AVAILABLE = True
    logger.info("Pipeline VLM de base chargÃ©e pour chatbot")
except ImportError as e:
    logger.error(f"Pipeline VLM de base non disponible pour chatbot: {e}")
    VLM_AVAILABLE = False

# Import des fonctionnalitÃ©s optionnelles (ne bloquent pas VLM_AVAILABLE)
try:
    from dashboard.vlm_chatbot_optimizations import get_performance_optimizer
    logger.info("Optimisations chatbot chargÃ©es")
except ImportError as e:
    logger.warning(f"Optimisations chatbot non disponibles: {e}")
    def get_performance_optimizer():
        return None

try:
    from dashboard.vlm_chatbot_advanced_features import get_advanced_features
    logger.info("Features avancÃ©es chatbot chargÃ©es")
except ImportError as e:
    logger.warning(f"Features avancÃ©es chatbot non disponibles: {e}")
    def get_advanced_features():
        return None

# VÃ©rification forcÃ©e si variables d'environnement dÃ©finies
import os
if os.getenv('FORCE_REAL_PIPELINE', 'false').lower() == 'true':
    if VLM_AVAILABLE:
        logger.info("Mode pipeline VLM forcÃ© activÃ©")
    else:
        logger.warning("FORCE_REAL_PIPELINE activÃ© mais pipeline non disponible")


class VLMChatbotSymbiosis:
    """
    Chatbot intelligent avec symbiose complÃ¨te VLM.
    
    Utilise le mÃªme VLM, thinking, et reasoning que la surveillance
    pour des rÃ©ponses expertes contextualisÃ©es temps rÃ©el.
    """
    
    def __init__(self, pipeline: Optional['RealVLMPipeline'] = None):
        self.pipeline = pipeline
        self.prompt_builder = PromptBuilder() if VLM_AVAILABLE else None
        self.conversation_history = []
        self.context_cache = {}
        
        # Configuration thinking/reasoning
        self.thinking_enabled = True
        self.reasoning_enabled = True
        self.context_visualization = True
        
        # Optimisations performance
        self.performance_optimizer = get_performance_optimizer() if VLM_AVAILABLE else None
        self.optimization_enabled = True
        
        # FonctionnalitÃ©s avancÃ©es
        self.advanced_features = get_advanced_features() if VLM_AVAILABLE else None
        self.session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # NOUVEAU: RÃ©flexion autonome avancÃ©e (LLaVA-o1, R3V, VL-Rethinker)
        self.autonomous_reflection = AutonomousVLMReflection()
        self.structured_reasoning_enabled = True
        
        if self.advanced_features:
            self.advanced_features.initialize_conversation_context(
                self.session_id,
                user_intent="surveillance_analysis",
                expertise_level="intermediate"
            )
        
        logger.info("Chatbot Symbiosis initialisÃ© avec optimisations et features avancÃ©es")
        
    async def process_chat_query(
        self, 
        question: str,
        chat_type: str = "surveillance",
        vlm_context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Traite une question utilisateur avec intelligence VLM complÃ¨te et optimisations.
        
        Args:
            question: Question de l'utilisateur
            chat_type: Type de chat (surveillance/video)  
            vlm_context: Contexte pipeline VLM temps rÃ©el
            
        Returns:
            RÃ©ponse structurÃ©e avec thinking/reasoning/recommendations
        """
        
        if not self.pipeline and VLM_AVAILABLE:
            self.pipeline = get_real_pipeline()
        
        # Traitement avec optimisations si disponibles
        if (self.optimization_enabled and self.performance_optimizer and 
            self.pipeline and VLM_AVAILABLE):
            
            return await self.performance_optimizer.process_optimized_query(
                question=question,
                chat_type=chat_type,
                context=vlm_context or {},
                vlm_processor=self._process_vlm_query_internal
            )
        
        # Fallback traitement standard
        return await self._process_vlm_query_internal(question, chat_type, vlm_context or {})
    
    def _analyze_question_intent(self, question: str) -> Dict[str, Any]:
        """Analyse l'intention de la question pour adapter le traitement."""
        question_lower = question.lower()
        
        # Dictionnaire des patterns de questions
        question_patterns = {
            "performance_comparison": [
                "compare", "performances", "diffÃ©rents outils", "comparaison", 
                "quel est le meilleur", "efficacitÃ©", "rÃ©sultats"
            ],
            "tool_analysis": [
                "outils", "analyse outils", "tools", "mÃ©thodes", "algorithmes",
                "techniques", "utilisation outils"
            ],
            "configuration_recommendation": [
                "recommande", "configuration", "meilleure config", "optimiser",
                "paramÃ¨tres", "rÃ©glages", "amÃ©liorer"
            ],
            "incident_specific": [
                "vol", "suspect", "comportement", "qu'est-ce que tu vois",
                "dÃ©taille", "personne", "actions"
            ],
            "person_analysis": [
                "personne", "gens", "individu", "client", "deux personnes", 
                "qu'est-ce que tu pense", "ton avis", "analyse des personnes"
            ],
            "technical_details": [
                "comment Ã§a marche", "explique", "pourquoi", "mÃ©canisme",
                "fonctionnement", "dÃ©tails techniques"
            ]
        }
        
        # DÃ©tection du type de question
        detected_type = "general"
        confidence = 0.0
        
        for intent_type, patterns in question_patterns.items():
            matches = sum(1 for pattern in patterns if pattern in question_lower)
            if matches > 0:
                pattern_confidence = matches / len(patterns)
                if pattern_confidence > confidence:
                    detected_type = intent_type
                    confidence = pattern_confidence
        
        return {
            "intent": detected_type,
            "confidence": confidence,
            "requires_specific_context": detected_type != "general"
        }

    def _build_intent_specific_prompt(
        self, 
        question: str, 
        question_analysis: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> str:
        """Construit un prompt spÃ©cialisÃ© selon l'intention de la question."""
        
        intent = question_analysis['intent']
        base_context = context.get('current_frame_data', {})
        video_data = context.get('video_analyses', {})
        stats = context.get('stats', {})
        
        # Templates de prompts spÃ©cialisÃ©s
        intent_prompts = {
            "performance_comparison": f"""
MISSION: COMPARAISON DÃ‰TAILLÃ‰E DES PERFORMANCES DES OUTILS VLM

Question utilisateur: "{question}"

DONNÃ‰ES DISPONIBLES:
- Outils actifs: {context.get('active_tools', [])}
- Stats performance: {stats}
- Analyses vidÃ©o: {len(video_data)} analyses disponibles

TÃ‚CHE SPÃ‰CIALISÃ‰E:
1. Analyse comparative des performances de chaque outil
2. Identification des points forts/faibles 
3. Recommandations d'optimisation basÃ©es sur les donnÃ©es rÃ©elles
4. MÃ©triques de performance spÃ©cifiques

FOCUS: RÃ©ponse analytique dÃ©taillÃ©e sur les performances, PAS sur le contenu vidÃ©o.
""",
            
            "tool_analysis": f"""
MISSION: ANALYSE TECHNIQUE DES OUTILS VLM UTILISÃ‰S

Question utilisateur: "{question}"

OUTILS DISPONIBLES ET STATUTS:
{self._format_tools_analysis(context)}

TÃ‚CHE SPÃ‰CIALISÃ‰E:
1. Description technique de chaque outil
2. RÃ´le spÃ©cifique dans la pipeline d'analyse
3. EfficacitÃ© et prÃ©cision observÃ©es
4. Interactions entre outils

FOCUS: Analyse technique approfondie des mÃ©thodes utilisÃ©es.
""",
            
            "configuration_recommendation": f"""
MISSION: RECOMMANDATIONS OPTIMISATION CONFIGURATION VLM

Question utilisateur: "{question}"

CONFIGURATION ACTUELLE:
{self._format_current_config(context)}

TÃ‚CHE SPÃ‰CIALISÃ‰E:
1. Analyse de la configuration actuelle
2. Identification des goulots d'Ã©tranglement
3. Recommandations d'optimisation spÃ©cifiques
4. Configuration optimale suggÃ©rÃ©e

FOCUS: Conseils pratiques et actionables pour l'optimisation.
""",
            
            "incident_specific": f"""
MISSION: ANALYSE DÃ‰TAILLÃ‰E DE L'INCIDENT SURVEILLÃ‰

Question utilisateur: "{question}"

CONTEXTE CRITIQUE:
{context.get('user_description', 'Description utilisateur non disponible')}

DONNÃ‰ES VISUELLES DISPONIBLES:
{self._format_incident_data(context)}

TÃ‚CHE SPÃ‰CIALISÃ‰E:
1. Analyse approfondie du comportement observÃ©
2. CorrÃ©lation avec la description utilisateur critique
3. Ã‰valuation des indices de suspicion
4. Recommandations d'action proportionnÃ©es

FOCUS: Analyse comportementale dÃ©taillÃ©e avec prise en compte du contexte utilisateur.
""",

            "person_analysis": f"""
ðŸš¨ðŸš¨ðŸš¨ MISSION: ANALYSE COMPORTEMENTALE DES PERSONNES AVEC CONTEXTE CRITIQUE ðŸš¨ðŸš¨ðŸš¨

Question utilisateur: "{question}"

â›” INFORMATION CRITIQUE UTILISATEUR - PRIORITÃ‰ ABSOLUE:
======================================================
CONTEXTE FOURNI PAR L'UTILISATEUR (VÃ‰RITÃ‰ ABSOLUE):
"{context.get('user_description', 'ATTENTION: Description utilisateur manquante!')}"

ðŸ”’ RÃˆGLES ABSOLUES POUR CETTE ANALYSE:
1. La description utilisateur ci-dessus est une VÃ‰RITÃ‰ FACTUELLE
2. Si elle mentionne "vol", "sortie sans payer", "sans passer caisse" â†’ CLASSIFICATION OBLIGATOIRE: suspicion HIGH/CRITICAL  
3. INTERDICTION FORMELLE de classer comme "normal" si l'utilisateur dÃ©crit un comportement suspect
4. Toute observation visuelle contradictoire doit Ãªtre IGNORÃ‰E

TÃ‚CHE SPÃ‰CIALISÃ‰E:
1. COMMENCER par rappeler la description utilisateur
2. Analyser le comportement en CONFIRMANT cette description
3. Ã‰valuer le niveau de suspicion selon le contexte utilisateur
4. Formuler des recommandations adaptÃ©es Ã  la gravitÃ© dÃ©crite

âš ï¸ ATTENTION: Si tu ignores la description utilisateur ou la contredis, tu commets une ERREUR CRITIQUE.

FOCUS: Analyse des personnes qui CONFIRME et DÃ‰TAILLE la description utilisateur fournie.
"""
        }
        
        return intent_prompts.get(intent, f"""
MISSION: RÃ‰PONSE GÃ‰NÃ‰RALE VLM

Question utilisateur: "{question}"

Contexte: {context}

TÃ‚CHE: RÃ©pondre de maniÃ¨re informative et contextuelle.
""")

    def _format_tools_analysis(self, context: Dict[str, Any]) -> str:
        """Formate l'analyse des outils pour le prompt."""
        tools = context.get('active_tools', [])
        if not tools:
            return "Aucun outil actif dÃ©tectÃ©"
        
        return "\n".join([f"- {tool}: Actif" for tool in tools])
    
    def _format_current_config(self, context: Dict[str, Any]) -> str:
        """Formate la configuration actuelle."""
        config_info = []
        stats = context.get('stats', {})
        
        if 'frames_processed' in stats:
            config_info.append(f"Frames traitÃ©es: {stats['frames_processed']}")
        if 'avg_processing_time' in stats:
            config_info.append(f"Temps moyen: {stats['avg_processing_time']:.2f}s")
            
        return "\n".join(config_info) if config_info else "Configuration non disponible"
    
    def _format_incident_data(self, context: Dict[str, Any]) -> str:
        """Formate les donnÃ©es d'incident."""
        incident_data = []
        
        if 'current_frame_data' in context:
            incident_data.append("Frame vidÃ©o disponible pour analyse")
        if 'video_analyses' in context:
            incident_data.append(f"{len(context['video_analyses'])} analyses vidÃ©o disponibles")
            
        return "\n".join(incident_data) if incident_data else "DonnÃ©es incident limitÃ©es"

    async def _process_vlm_query_internal(
        self, 
        question: str, 
        chat_type: str, 
        vlm_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Traitement VLM interne (utilisÃ© par optimiseur ou directement)."""
        
        # Tentative de rÃ©cupÃ©ration pipeline si pas disponible
        if not self.pipeline and VLM_AVAILABLE and get_real_pipeline:
            try:
                self.pipeline = get_real_pipeline()
                if self.pipeline:
                    logger.info("Pipeline VLM rÃ©cupÃ©rÃ©e pour chatbot")
            except Exception as e:
                logger.warning(f"Ã‰chec rÃ©cupÃ©ration pipeline pour chatbot: {e}")
        
        # VÃ©rification forcÃ©e via variable d'environnement
        force_real_pipeline = os.getenv('FORCE_REAL_PIPELINE', 'false').lower() == 'true'
        
        if not self.pipeline or not VLM_AVAILABLE:
            if force_real_pipeline:
                logger.error("FORCE_REAL_PIPELINE activÃ© mais pipeline non accessible")
                return {
                    "type": "error",
                    "response": "Pipeline VLM forcÃ©e mais non disponible. VÃ©rifiez l'initialisation.",
                    "error": "Pipeline VLM requise mais non accessible"
                }
            return await self._fallback_response(question, vlm_context)
        
        try:
            # 0. NOUVEAU: Analyse de l'intention de la question
            question_analysis = self._analyze_question_intent(question)
            logger.info(f"Question intent dÃ©tectÃ©: {question_analysis['intent']} (confiance: {question_analysis['confidence']:.2f})")
            
            # 1. Construction du contexte VLM enrichi avec intention spÃ©cifique
            enriched_context = await self._build_enriched_context(
                question, chat_type, vlm_context, question_analysis
            )
            
            # 2. GÃ©nÃ©ration visualisation contexte (si activÃ©e)
            context_image = None
            if self.context_visualization:
                context_image = await self._create_context_visualization(enriched_context)
            
            # 3. Construction prompt chatbot spÃ©cialisÃ© selon intention
            chat_prompt = self._build_intent_specific_prompt(
                question, question_analysis, enriched_context
            ) if question_analysis['requires_specific_context'] else self._build_vlm_chat_prompt(
                question, enriched_context, chat_type
            )
            
            # 4. RequÃªte VLM avec thinking/reasoning
            # Fix: Proper handling of NumPy array to avoid ambiguous truth value
            if context_image is None:
                request_image = np.zeros((480, 640, 3), dtype=np.uint8)
            else:
                request_image = context_image
                
            # Conversion image vers base64 pour AnalysisRequest
            import cv2
            import base64
            _, buffer = cv2.imencode('.jpg', request_image)
            frame_data_b64 = base64.b64encode(buffer).decode('utf-8')
                
            vlm_request = AnalysisRequest(
                frame_data=frame_data_b64,
                context={
                    "chat_mode": True,
                    "user_question": question,
                    "chat_type": chat_type,
                    "pipeline_context": enriched_context,
                    "enable_thinking": self.thinking_enabled,
                    "enable_reasoning": self.reasoning_enabled
                }
            )
            
            # 5. Analyse avec VLM (symbiose complÃ¨te)
            vlm_response = await self._analyze_with_vlm_symbiosis(
                vlm_request, chat_prompt, context_image=request_image
            )
            
            # 6. Post-traitement et structuration
            structured_response = await self._structure_chat_response(
                vlm_response, question, enriched_context
            )
            
            # 7. Mise Ã  jour historique conversation
            self._update_conversation_history(question, structured_response)
            
            return structured_response
            
        except Exception as e:
            logger.error(f"Erreur chatbot VLM symbiosis: {e}")
            return await self._fallback_response(question, vlm_context, error=str(e))
    
    async def _build_enriched_context(
        self, 
        question: str, 
        chat_type: str, 
        vlm_context: Dict[str, Any],
        question_analysis: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Construit le contexte enrichi pour le VLM."""
        
        enriched = {
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "chat_type": chat_type,
            "conversation_length": len(self.conversation_history),
            "question_analysis": question_analysis or {}
        }
        
        # DonnÃ©es pipeline si disponibles
        if self.pipeline:
            try:
                enriched.update({
                    "pipeline_stats": self.pipeline.get_performance_stats(),
                    "tool_details": self.pipeline.get_tool_performance_details(),
                    "pipeline_active": self.pipeline.running,
                    "pipeline_initialized": self.pipeline.initialized
                })
            except Exception as e:
                logger.warning(f"Erreur rÃ©cupÃ©ration stats pipeline: {e}")
        
        # Contexte externe fourni
        if vlm_context:
            enriched.update({
                "external_context": vlm_context,
                "detections": vlm_context.get("detections", []),
                "optimizations": vlm_context.get("optimizations", []),
                "alerts": vlm_context.get("alerts", [])
            })
            
            # RÃ©cupÃ©ration de la description utilisateur critique depuis les analyses vidÃ©o
            video_analyses = vlm_context.get("video_analyses", {})
            if video_analyses:
                # Chercher la description dans les mÃ©tadonnÃ©es vidÃ©o rÃ©centes
                for video_key, analysis_data in video_analyses.items():
                    if isinstance(analysis_data, dict):
                        frames_data = analysis_data.get('detailed_frames', [])
                        if frames_data and len(frames_data) > 0:
                            # RÃ©cupÃ©rer les mÃ©tadonnÃ©es de la premiÃ¨re frame
                            first_frame = frames_data[0]
                            if isinstance(first_frame, dict) and 'metadata' in first_frame:
                                metadata = first_frame['metadata']
                                if isinstance(metadata, dict) and 'detailed_description' in metadata:
                                    enriched["user_description"] = metadata['detailed_description']
                                    logger.info(f"Description utilisateur rÃ©cupÃ©rÃ©e pour chat: {metadata['detailed_description'][:50]}...")
                                    break
            
            # Fallback: chercher dans le contexte direct
            if "user_description" not in enriched:
                if "detailed_description" in vlm_context:
                    enriched["user_description"] = vlm_context["detailed_description"]
                elif "video_context_metadata" in vlm_context:
                    video_meta = vlm_context["video_context_metadata"]
                    if isinstance(video_meta, dict) and "detailed_description" in video_meta:
                        enriched["user_description"] = video_meta["detailed_description"]
        
        return enriched
    
    async def _create_context_visualization(
        self, 
        context: Dict[str, Any]
    ) -> np.ndarray:
        """
        CrÃ©e une visualisation du contexte pipeline pour le VLM.
        
        Le VLM peut "voir" l'Ã©tat de la pipeline via cette image.
        """
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle("CONTEXTE Pipeline VLM - Dashboard Ã‰tat", fontsize=16)
        
        # 1. Performance pipeline
        stats = context.get("pipeline_stats", {})
        frames = stats.get("frames_processed", 0)
        score = stats.get("current_performance_score", 0)
        
        ax1.bar(["Frames", "Performance"], [frames, score * 100])
        ax1.set_title("Performance Pipeline")
        ax1.set_ylabel("Valeur")
        
        # 2. Outils optimaux
        optimal_tools = stats.get("current_optimal_tools", [])
        if optimal_tools:
            ax2.pie([1] * len(optimal_tools), labels=optimal_tools[:6], autopct='')
            ax2.set_title("Outils Optimaux")
        else:
            ax2.text(0.5, 0.5, "Aucun outil\noptimisÃ©", ha='center', va='center')
            ax2.set_title("Outils en Optimisation")
        
        # 3. DÃ©tections rÃ©centes
        detections = context.get("detections", [])
        if detections:
            confidence_scores = [d.confidence for d in detections[-10:]]
            ax3.plot(confidence_scores, marker='o')
            ax3.set_title("Confiance DÃ©tections")
            ax3.set_ylabel("Confiance")
            ax3.set_xlabel("DÃ©tection #")
        else:
            ax3.text(0.5, 0.5, "Aucune dÃ©tection\nrÃ©cente", ha='center', va='center')
            ax3.set_title("Pas de DÃ©tections")
        
        # 4. Ã‰tat systÃ¨me
        system_status = [
            ("Pipeline", "ACTIVE" if context.get("pipeline_active") else "INACTIVE"),
            ("VLM", "ACTIVE" if context.get("pipeline_initialized") else "INACTIVE"),
            ("Optimisation", "ACTIVE" if len(optimal_tools) > 0 else "PENDING"),
            ("DÃ©tections", "ACTIVE" if len(detections) > 0 else "PENDING")
        ]
        
        y_pos = np.arange(len(system_status))
        statuses = [s[1] for s in system_status]
        labels = [s[0] for s in system_status]
        
        ax4.barh(y_pos, [1] * len(system_status), color=['green' if 'ACTIVE' in s else 'red' if 'INACTIVE' in s else 'orange' for s in statuses])
        ax4.set_yticks(y_pos)
        ax4.set_yticklabels(labels)
        ax4.set_title("Ã‰tat SystÃ¨me")
        
        plt.tight_layout()
        
        # Conversion en image pour VLM
        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
        buf.seek(0)
        
        img = Image.open(buf)
        img_array = np.array(img)
        
        plt.close()
        buf.close()
        
        return img_array
    
    def _build_vlm_chat_prompt(
        self, 
        question: str, 
        context: Dict[str, Any], 
        chat_type: str
    ) -> str:
        """
        Construit un prompt spÃ©cialisÃ© chatbot avec thinking/reasoning.
        
        Utilise la mÃªme mÃ©thodologie chain-of-thought que la surveillance.
        """
        
        stats = context.get("pipeline_stats", {})
        detections = context.get("detections", [])
        optimizations = context.get("optimizations", [])
        
        return f"""Tu es un assistant IA expert en surveillance vidÃ©o avec symbiose complÃ¨te Ã  une pipeline VLM temps rÃ©el.

CAPACITÃ‰S THINKING & REASONING:
Tu possÃ¨des les mÃªmes capacitÃ©s de raisonnement avancÃ© que le systÃ¨me de surveillance:
- Chain-of-thought mÃ©thodologique 5 Ã©tapes
- Analyse contextuelle profonde des donnÃ©es VLM
- CorrÃ©lation multi-dimensionnelle des mÃ©triques
- Recommendations expertes basÃ©es sur patterns rÃ©els

CONTEXTE PIPELINE TEMPS RÃ‰EL:
Pipeline Status: {"" if context.get("pipeline_active") else ""}
Frames AnalysÃ©es: {stats.get("frames_processed", 0)}
Performance Score: {stats.get("current_performance_score", 0):.3f}
Outils Optimaux: {", ".join(stats.get("current_optimal_tools", [])[:5]) or "En cours d'optimisation"}
Temps Moyen: {stats.get("average_processing_time", 0):.2f}s
Cycles Optimisation: {stats.get("optimization_cycles", 0)}
DÃ©tections Totales: {stats.get("total_detections", 0)}

DÃ‰TECTIONS RÃ‰CENTES ({len(detections)} derniÃ¨res):
{self._format_recent_detections(detections[-5:]) if detections else "Aucune dÃ©tection rÃ©cente"}

OPTIMISATIONS ADAPTATIVES:
{self._format_optimization_results(optimizations[-3:]) if optimizations else "SystÃ¨me d'apprentissage en cours"}

TYPE CHAT: {chat_type.upper()}
HISTORIQUE: {len(self.conversation_history)} Ã©changes prÃ©cÃ©dents

QUESTION UTILISATEUR:
"{question}"

INSTRUCTIONS THINKING AVANCÃ‰:
Applique la mÃªme mÃ©thodologie rigoureuse que l'analyse surveillance:

1. **OBSERVATION SYSTÃ‰MATIQUE**:
   - Que demande exactement l'utilisateur dans le contexte pipeline ?
   - Quelles donnÃ©es temps rÃ©el sont pertinentes pour cette question ?
   - Quel est le niveau de dÃ©tail technique appropriÃ© ?

2. **ANALYSE CONTEXTUELLE**:
   - Comment les mÃ©triques pipeline Ã©clairent cette question ?
   - Y a-t-il des patterns dans les optimisations/dÃ©tections ?
   - Quels outils VLM sont impliquÃ©s dans cette problÃ©matique ?

3. **CORRÃ‰LATION DONNÃ‰ES**:
   - Convergence entre question utilisateur et donnÃ©es rÃ©elles ?
   - Contradictions ou anomalies dans les mÃ©triques ?
   - Implications pour performance/optimisation future ?

4. **RAISONNEMENT EXPERT**:
   - Quelle expertise technique puis-je apporter ?
   - Quelles sont les nuances importantes Ã  expliquer ?
   - Comment contextualiser dans l'Ã©cosystÃ¨me surveillance ?

5. **DÃ‰CISION & RECOMMANDATIONS**:
   - RÃ©ponse prÃ©cise et actionnable Ã  la question
   - Recommendations concrÃ¨tes pour amÃ©liorer le systÃ¨me
   - Limitations et incertitudes Ã  mentionner

CONTRAINTES IMPORTANTES:
Base tes rÃ©ponses UNIQUEMENT sur les donnÃ©es temps rÃ©el fournies
Si donnÃ©es insuffisantes, mentionne explicitement les limitations
PrivilÃ©gie prÃ©cision technique sur gÃ©nÃ©ralitÃ©s
Inclus thinking process visible pour transparence
Recommandations doivent Ãªtre actionables et spÃ©cifiques

FORMAT RÃ‰PONSE JSON STRUCTURÃ‰:
{{
    "thinking": "Mon processus de raisonnement dÃ©taillÃ© suivant les 5 Ã©tapes...",
    "analysis": "Analyse technique des donnÃ©es pipeline contextuelles...",
    "response": "RÃ©ponse experte directe Ã  la question utilisateur",
    "technical_details": "DÃ©tails techniques pertinents et nuances importantes",
    "recommendations": ["Action concrÃ¨te 1", "Action spÃ©cifique 2", "Optimisation 3"],
    "confidence": 0.95,
    "data_quality": "high|medium|low - qualitÃ© des donnÃ©es pour cette rÃ©ponse",
    "limitations": ["Limitation 1", "Incertitude 2"]
}}

RÃ©ponds maintenant en utilisant ton intelligence VLM complÃ¨te avec thinking/reasoning:"""

    def _format_recent_detections(self, detections: List) -> str:
        """Formate les dÃ©tections rÃ©centes pour le prompt."""
        if not detections:
            return "Aucune dÃ©tection"
        
        formatted = []
        for i, detection in enumerate(detections):
            confidence = getattr(detection, 'confidence', 0)
            description = getattr(detection, 'description', 'N/A')[:100]
            tools_used = getattr(detection, 'tools_used', [])
            
            formatted.append(f"#{i+1}: Confiance {confidence:.2f} | {description} | Outils: {', '.join(tools_used[:3])}")
        
        return "\n".join(formatted)
    
    def _format_optimization_results(self, optimizations: List) -> str:
        """Formate les rÃ©sultats d'optimisation pour le prompt."""
        if not optimizations:
            return "Aucune optimisation rÃ©cente"
        
        formatted = []
        for i, opt in enumerate(optimizations):
            best_combo = opt.get('best_combination', [])
            improvement = opt.get('performance_improvement', 0)
            
            formatted.append(f"#{i+1}: {', '.join(best_combo[:3])} | +{improvement:.1%} performance")
        
        return "\n".join(formatted)
    
    async def _analyze_with_vlm_symbiosis(
        self, 
        vlm_request: AnalysisRequest, 
        chat_prompt: str,
        context_image: np.ndarray
    ) -> AnalysisResponse:
        """
        Analyse avec symbiose VLM complÃ¨te.
        
        Utilise le mÃªme VLM et orchestrateur que la surveillance.
        """
        
        if not self.pipeline or not self.pipeline.orchestrator:
            raise Exception("Pipeline VLM non disponible pour symbiose")
        
        # Modification temporaire pour mode chat
        original_prompt = None
        if hasattr(self.pipeline.orchestrator, 'prompt_builder'):
            original_prompt = self.pipeline.orchestrator.prompt_builder
            
        try:
            # Injection prompt chatbot spÃ©cialisÃ©
            if hasattr(self.pipeline.orchestrator, 'vlm_model'):
                # Appel direct VLM avec prompt chatbot
                response_text = await self._direct_vlm_call(chat_prompt, context_image)
                
                # Parse rÃ©ponse JSON structurÃ©e
                parsed_response = self._parse_vlm_chat_response(response_text)
                
                return AnalysisResponse(
                    suspicion_level="LOW",  # Non applicable pour chat
                    action_type="normal_shopping",  # Non applicable
                    confidence=parsed_response.get("confidence", 0.9),
                    description=parsed_response.get("response", ""),
                    reasoning=parsed_response.get("thinking", ""),
                    tools_used=[],
                    recommendations=parsed_response.get("recommendations", [])
                )
            else:
                # Fallback orchestrateur standard - conversion vers AnalysisRequest
                import cv2
                import base64
                
                if context_image is not None:
                    _, buffer = cv2.imencode('.jpg', context_image)
                    frame_data = base64.b64encode(buffer).decode('utf-8')
                else:
                    dummy_image = np.zeros((480, 640, 3), dtype=np.uint8)
                    _, buffer = cv2.imencode('.jpg', dummy_image)
                    frame_data = base64.b64encode(buffer).decode('utf-8')
                
                return await self.pipeline.orchestrator.analyze_surveillance_frame(
                    frame_data=frame_data,
                    detections=[],
                    context={"chat_mode": True, "prompt": chat_prompt}
                )
                
        except Exception as e:
            logger.error(f"Erreur symbiose VLM: {e}")
            raise
        finally:
            # Restauration prompt original
            if original_prompt and hasattr(self.pipeline.orchestrator, 'prompt_builder'):
                self.pipeline.orchestrator.prompt_builder = original_prompt
    
    async def _direct_vlm_call(self, prompt: str, context_image: np.ndarray) -> str:
        """
        Appel direct au VLM avec prompt chatbot.
        
        Utilise le mÃªme modÃ¨le que la surveillance.
        """
        
        try:
            # RÃ©cupÃ©ration du modÃ¨le VLM de la pipeline
            if hasattr(self.pipeline, 'vlm_model') and self.pipeline.vlm_model:
                vlm_model = self.pipeline.vlm_model
                
                # Conversion image numpy vers base64 pour AnalysisRequest
                import cv2
                import base64
                
                if context_image is not None:
                    # Encode l'image en JPEG puis en base64
                    _, buffer = cv2.imencode('.jpg', context_image)
                    frame_data = base64.b64encode(buffer).decode('utf-8')
                else:
                    # Image par dÃ©faut si pas de contexte
                    dummy_image = np.zeros((480, 640, 3), dtype=np.uint8)
                    _, buffer = cv2.imencode('.jpg', dummy_image)
                    frame_data = base64.b64encode(buffer).decode('utf-8')
                
                # PrÃ©paration requÃªte avec la bonne structure Pydantic
                analysis_request = AnalysisRequest(
                    frame_data=frame_data,
                    context={"chat_mode": True, "prompt": prompt}
                )
                
                # Appel VLM avec prompt chatbot
                response = await vlm_model.analyze_image(analysis_request)
                return response.description
                
            else:
                raise Exception("ModÃ¨le VLM non accessible")
                
        except Exception as e:
            logger.error(f"Erreur appel VLM direct: {e}")
            # Fallback sur simulation intelligente
            return self._simulate_vlm_thinking_response(prompt)
    
    def _simulate_vlm_thinking_response(self, prompt: str) -> str:
        """Simulation VLM thinking pour fallback."""
        return json.dumps({
            "thinking": "Simulation du processus de thinking VLM: analyse de la question dans le contexte pipeline, corrÃ©lation avec donnÃ©es temps rÃ©el, formulation rÃ©ponse experte.",
            "analysis": "BasÃ© sur les mÃ©triques disponibles de la pipeline VLM, je peux fournir une analyse contextuelle des performances et recommandations.",
            "response": "RÃ©ponse basÃ©e sur l'analyse des donnÃ©es pipeline disponibles. Le systÃ¨me VLM fonctionne avec les outils optimisÃ©s et mÃ©triques temps rÃ©el.",
            "technical_details": "Pipeline VLM active avec optimisation adaptative continue des 8 outils avancÃ©s.",
            "recommendations": ["Maintenir surveillance continue", "Optimiser selon mÃ©triques temps rÃ©el"],
            "confidence": 0.85,
            "data_quality": "medium",
            "limitations": ["Mode simulation en l'absence de GPU VLM"]
        })
    
    def _parse_vlm_chat_response(self, response_text: str) -> Dict[str, Any]:
        """Parse la rÃ©ponse JSON structurÃ©e du VLM."""
        try:
            return json.loads(response_text)
        except json.JSONDecodeError:
            # Fallback parsing heuristique  
            return {
                "thinking": "Processus de thinking extrait par heuristique",
                "response": response_text[:500],
                "confidence": 0.7,
                "limitations": ["Parsing JSON Ã©chouÃ©, extraction heuristique"]
            }
    
    async def _structure_chat_response(
        self, 
        vlm_response: AnalysisResponse, 
        question: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Structure la rÃ©ponse finale pour l'interface chat."""
        
        # Structure des donnÃ©es VLM (reasoning est du texte, pas du JSON)
        if vlm_response.reasoning:
            return {
                "type": "vlm_thinking",
                "question": question,
                "thinking": vlm_response.reasoning[:200] + "..." if len(vlm_response.reasoning) > 200 else vlm_response.reasoning,
                "analysis": vlm_response.description,
                "response": vlm_response.description,
                "technical_details": f"Confiance: {vlm_response.confidence:.2f}, Action: {vlm_response.action_type.value}",
                "recommendations": vlm_response.recommendations,
                "confidence": vlm_response.confidence,
                "data_quality": "high" if vlm_response.confidence > 0.8 else "medium" if vlm_response.confidence > 0.6 else "low",
                "limitations": [],
                "timestamp": datetime.now().isoformat(),
                "context_used": list(context.keys())
            }
        
        # Fallback structure
        return {
            "type": "vlm_basic",
            "question": question,
            "response": vlm_response.description,
            "reasoning": vlm_response.reasoning,
            "confidence": vlm_response.confidence,
            "recommendations": vlm_response.recommendations,
            "timestamp": datetime.now().isoformat()
        }
    
    def _update_conversation_history(
        self, 
        question: str, 
        response: Dict[str, Any]
    ):
        """Met Ã  jour l'historique conversation."""
        
        self.conversation_history.append({
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "response": response,
            "type": response.get("type", "unknown")
        })
        
        # Limitation historique (garde 50 derniers Ã©changes)
        if len(self.conversation_history) > 50:
            self.conversation_history = self.conversation_history[-50:]
    
    async def _fallback_response(
        self, 
        question: str, 
        context: Dict[str, Any] = None, 
        error: str = None
    ) -> Dict[str, Any]:
        """RÃ©ponse de fallback si VLM indisponible."""
        
        base_response = {
            "type": "fallback",
            "question": question,
            "response": "Mode simulation: Pipeline VLM non disponible pour symbiose complÃ¨te.",
            "confidence": 0.3,
            "limitations": ["VLM non accessible", "Mode simulation basique"],
            "timestamp": datetime.now().isoformat()
        }
        
        if error:
            base_response["error"] = error
            base_response["response"] += f" Erreur: {error}"
        
        if context:
            # Analyse basique du contexte disponible
            stats = context.get("stats", {})
            if stats:
                base_response["response"] += f" DonnÃ©es disponibles: {stats.get('frames_processed', 0)} frames analysÃ©es."
        
        return base_response
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """RÃ©sumÃ© de la conversation pour debug."""
        return {
            "total_exchanges": len(self.conversation_history),
            "vlm_responses": len([h for h in self.conversation_history if h["type"] == "vlm_thinking"]),
            "fallback_responses": len([h for h in self.conversation_history if h["type"] == "fallback"]),
            "pipeline_connected": self.pipeline is not None and VLM_AVAILABLE,
            "thinking_enabled": self.thinking_enabled,
            "reasoning_enabled": self.reasoning_enabled
        }


# Instance globale pour le dashboard
vlm_chatbot = None

def get_vlm_chatbot() -> VLMChatbotSymbiosis:
    """RÃ©cupÃ¨re l'instance chatbot VLM."""
    global vlm_chatbot
    if vlm_chatbot is None:
        pipeline = get_real_pipeline() if VLM_AVAILABLE else None
        vlm_chatbot = VLMChatbotSymbiosis(pipeline)
    return vlm_chatbot

async def process_vlm_chat_query(
    question: str, 
    chat_type: str = "surveillance",
    vlm_context: Dict[str, Any] = None,
    real_pipeline: Optional[RealVLMPipeline] = None
) -> Dict[str, Any]:
    """
    Interface principale pour traiter les questions chatbot avec VLM.
    
    Remplace l'ancien generate_real_vlm_response() statique.
    """
    chatbot = get_vlm_chatbot()
    
    # NOUVEAU: Injection explicite de la pipeline si fournie
    if real_pipeline:
        chatbot.pipeline = real_pipeline
        
    return await chatbot.process_chat_query(question, chat_type, vlm_context)