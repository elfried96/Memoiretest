"""
üé• Video Context Integration - M√©tadonn√©es pour VLM
==================================================

Int√©gration des descriptions et m√©tadonn√©es vid√©o dans le contexte VLM
pour une analyse plus pr√©cise et contextualis√©e.
"""

from typing import Dict, List, Any, Optional
from datetime import datetime
import json
from dataclasses import dataclass
from loguru import logger


@dataclass
class VideoContextMetadata:
    """M√©tadonn√©es contextuelles pour analyse vid√©o VLM."""
    title: str
    location_type: str
    time_context: str
    expected_activities: List[str]
    suspicious_focus: List[str]
    camera_angle: str
    detailed_description: str
    analysis_priority: str
    frame_sampling: str
    user_timestamp: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'title': self.title,
            'location_type': self.location_type,
            'time_context': self.time_context,
            'expected_activities': self.expected_activities,
            'suspicious_focus': self.suspicious_focus,
            'camera_angle': self.camera_angle,
            'detailed_description': self.detailed_description,
            'analysis_priority': self.analysis_priority,
            'frame_sampling': self.frame_sampling,
            'user_timestamp': self.user_timestamp
        }


class VideoContextPromptBuilder:
    """Constructeur de prompts enrichis avec contexte vid√©o."""
    
    def __init__(self):
        # Mapping des types de lieux vers caract√©ristiques surveillance
        self.location_characteristics = {
            "Magasin/Commerce": {
                "typical_behaviors": ["shopping", "browsing", "queuing", "payment"],
                "risk_patterns": ["shoplifting", "price_switching", "distraction_theft"],
                "key_zones": ["aisles", "checkout", "entrance", "changing_rooms"],
                "peak_times": ["lunch_hours", "evenings", "weekends"]
            },
            "Entrep√¥t": {
                "typical_behaviors": ["loading", "unloading", "inventory", "forklift_operation"],
                "risk_patterns": ["theft", "unauthorized_access", "safety_violations"],
                "key_zones": ["loading_docks", "storage_areas", "office_areas"],
                "peak_times": ["shift_changes", "delivery_hours"]
            },
            "Bureau": {
                "typical_behaviors": ["working", "meetings", "coffee_breaks", "cleaning"],
                "risk_patterns": ["unauthorized_access", "data_theft", "vandalism"],
                "key_zones": ["entrances", "server_rooms", "executive_areas"],
                "peak_times": ["office_hours", "after_hours"]
            },
            "Parking": {
                "typical_behaviors": ["parking", "walking", "loading_vehicles"],
                "risk_patterns": ["car_theft", "break_ins", "vandalism", "loitering"],
                "key_zones": ["entrances", "payment_areas", "isolated_corners"],
                "peak_times": ["rush_hours", "late_evenings"]
            }
        }
        
        # Mapping contextes temporels vers comportements attendus
        self.temporal_contexts = {
            "Heures ouverture": {
                "activity_level": "normal",
                "staff_presence": "full",
                "customer_flow": "regular",
                "alert_sensitivity": "standard"
            },
            "Heures affluence": {
                "activity_level": "high", 
                "staff_presence": "full",
                "customer_flow": "heavy",
                "alert_sensitivity": "reduced_for_crowding"
            },
            "Heures creuses": {
                "activity_level": "low",
                "staff_presence": "minimal", 
                "customer_flow": "sparse",
                "alert_sensitivity": "increased"
            },
            "Nuit/Fermeture": {
                "activity_level": "minimal",
                "staff_presence": "security_only",
                "customer_flow": "none",
                "alert_sensitivity": "maximum"
            }
        }
    
    def build_context_enhanced_prompt(
        self,
        base_prompt: str,
        video_metadata: VideoContextMetadata,
        frame_context: Dict[str, Any] = None
    ) -> str:
        """Construit un prompt enrichi avec le contexte vid√©o."""
        
        location_info = self.location_characteristics.get(
            video_metadata.location_type, 
            {"typical_behaviors": [], "risk_patterns": [], "key_zones": [], "peak_times": []}
        )
        
        temporal_info = self.temporal_contexts.get(
            video_metadata.time_context,
            {"activity_level": "unknown", "staff_presence": "unknown", "customer_flow": "unknown", "alert_sensitivity": "standard"}
        )
        
        context_enhancement = f"""

üé• CONTEXTE VID√âO SP√âCIFIQUE - INFORMATIONS UTILISATEUR:
=====================================================

üìã IDENTIFICATION:
- Titre: "{video_metadata.title}"
- Type environnement: {video_metadata.location_type}
- Contexte temporel: {video_metadata.time_context}
- Angle cam√©ra: {video_metadata.camera_angle}

üìä CARACT√âRISTIQUES ENVIRONNEMENT {video_metadata.location_type.upper()}:
- Comportements typiques: {', '.join(location_info['typical_behaviors'])}
- Patterns risque connus: {', '.join(location_info['risk_patterns'])}
- Zones critiques: {', '.join(location_info['key_zones'])}
- Pics d'activit√©: {', '.join(location_info['peak_times'])}

‚è∞ CONTEXTE TEMPOREL "{video_metadata.time_context}":
- Niveau activit√© attendu: {temporal_info['activity_level']}
- Pr√©sence personnel: {temporal_info['staff_presence']}
- Flux clients: {temporal_info['customer_flow']}
- Sensibilit√© alertes: {temporal_info['alert_sensitivity']}

‚úÖ ACTIVIT√âS NORMALES ATTENDUES:
{self._format_list_for_prompt(video_metadata.expected_activities)}

üö® FOCUS SURVEILLANCE PRIORITAIRE:
{self._format_list_for_prompt(video_metadata.suspicious_focus)}

üìù DESCRIPTION D√âTAILL√âE UTILISATEUR:
"{video_metadata.detailed_description}"

üéØ PRIORIT√â ANALYSE: {video_metadata.analysis_priority}
üìä √âCHANTILLONNAGE: {video_metadata.frame_sampling}

INSTRUCTIONS CONTEXTUALIS√âES:
=============================

üîç ADAPTATION SELON ENVIRONNEMENT:
- Calibre tes seuils de suspicion selon le type "{video_metadata.location_type}"
- Prends en compte le contexte "{video_metadata.time_context}" pour √©valuer normalit√©
- Perspective cam√©ra "{video_metadata.camera_angle}" influence interpr√©tation spatiale

‚öñÔ∏è √âVALUATION COMPORTEMENTS:
- NORMAUX dans ce contexte: {', '.join(video_metadata.expected_activities)}
- SUSPECTS √† prioriser: {', '.join(video_metadata.suspicious_focus)}
- Distinguer activit√© normale vs comportement inhabituel selon contexte temporel

üéØ OBJECTIFS SP√âCIFIQUES:
- Focus principal: d√©tection patterns list√©s en "Focus surveillance"
- Ignorer ou minimiser activit√©s normales list√©es sauf si vraiment suspectes  
- Adapter confiance selon qualit√© description utilisateur
- Corr√©ler avec description d√©taill√©e fournie

‚ö†Ô∏è CONTRAINTES CONTEXTUELLES:
- Environnement "{video_metadata.location_type}" a des patterns comportementaux sp√©cifiques
- P√©riode "{video_metadata.time_context}" influence normalit√© des activit√©s
- Description utilisateur doit primer sur assumptions g√©n√©rales
- Perspective "{video_metadata.camera_angle}" affecte visibilit√© d√©tails

CALIBRAGE SUSPICION CONTEXTUEL:
- LOW (0.0-0.3): Activit√© list√©e comme normale ET coh√©rente avec environnement/temps
- MEDIUM (0.3-0.6): Activit√© non list√©e mais coh√©rente avec contexte g√©n√©ral
- HIGH (0.6-0.8): Comportement incoh√©rent avec contexte OU focus surveillance d√©tect√©
- CRITICAL (0.8-1.0): Focus surveillance confirm√© ET description utilisateur valid√©e"""

        return base_prompt + context_enhancement
    
    def _format_list_for_prompt(self, items: List[str]) -> str:
        """Formate une liste pour inclusion dans le prompt."""
        if not items:
            return "Aucun √©l√©ment sp√©cifi√©"
        return f"[{', '.join(items)}]"
    
    def extract_analysis_objectives(self, video_metadata: VideoContextMetadata) -> Dict[str, Any]:
        """Extrait les objectifs d'analyse √† partir des m√©tadonn√©es."""
        
        return {
            "primary_focus": video_metadata.suspicious_focus,
            "normal_baseline": video_metadata.expected_activities,
            "environment_adaptation": self.location_characteristics.get(
                video_metadata.location_type, {}
            ),
            "temporal_adaptation": self.temporal_contexts.get(
                video_metadata.time_context, {}
            ),
            "user_priority": video_metadata.analysis_priority,
            "detailed_context": video_metadata.detailed_description
        }
    
    def generate_contextual_questions(self, video_metadata: VideoContextMetadata) -> List[str]:
        """G√©n√®re des questions contextualis√©es pour le chat VLM."""
        
        questions = []
        
        # Questions bas√©es sur le lieu
        if video_metadata.location_type == "Magasin/Commerce":
            questions.extend([
                f"Analyse les comportements de shopping dans '{video_metadata.title}'",
                "D√©tecte-t-on des tentatives de vol √† l'√©talage ?",
                "Comment les clients interagissent avec les produits ?",
                "Y a-t-il des comportements d'√©vitement du personnel ?"
            ])
        elif video_metadata.location_type == "Bureau":
            questions.extend([
                f"Analyse les activit√©s professionnelles dans '{video_metadata.title}'",
                "D√©tecte-t-on des acc√®s non autoris√©s ?",
                "Comment se d√©roulent les interactions professionnelles ?",
                "Y a-t-il des comportements inhabituels pour un environnement bureau ?"
            ])
        
        # Questions bas√©es sur le focus surveillance
        for focus in video_metadata.suspicious_focus:
            if focus == "Vol √† l'√©talage":
                questions.append("Identifie les signes de vol √† l'√©talage dans cette s√©quence")
            elif focus == "Comportements agressifs":
                questions.append("Analyse les interactions pour d√©tecter agressivit√© ou tensions")
            elif focus == "Intrusion":
                questions.append("V√©rifie les acc√®s et d√©tecte d'√©ventuelles intrusions")
        
        # Questions bas√©es sur la description utilisateur
        if video_metadata.detailed_description:
            questions.append(f"Analyse cette vid√©o selon le contexte: '{video_metadata.detailed_description[:100]}...'")
        
        return questions[:8]  # Limite √† 8 questions max


class VideoContextChatIntegration:
    """Int√©gration du contexte vid√©o dans le chat VLM."""
    
    def __init__(self):
        self.prompt_builder = VideoContextPromptBuilder()
    
    def enhance_chat_context(
        self,
        base_chat_context: Dict[str, Any],
        video_metadata: Optional[VideoContextMetadata] = None
    ) -> Dict[str, Any]:
        """Enrichit le contexte chat avec les m√©tadonn√©es vid√©o."""
        
        enhanced_context = base_chat_context.copy()
        
        if video_metadata:
            enhanced_context['video_context'] = {
                'metadata': video_metadata.to_dict(),
                'analysis_objectives': self.prompt_builder.extract_analysis_objectives(video_metadata),
                'contextual_questions': self.prompt_builder.generate_contextual_questions(video_metadata),
                'context_enhanced': True
            }
            
            # Ajout de m√©tadonn√©es pour le chatbot
            enhanced_context['user_intent_context'] = {
                'primary_focus': video_metadata.suspicious_focus,
                'environment': video_metadata.location_type,
                'temporal_context': video_metadata.time_context,
                'user_description': video_metadata.detailed_description
            }
        
        return enhanced_context
    
    def generate_context_aware_response_enhancement(
        self, 
        question: str,
        video_metadata: VideoContextMetadata
    ) -> str:
        """G√©n√®re une am√©lioration de r√©ponse bas√©e sur le contexte vid√©o."""
        
        enhancement = f"""
CONTEXTE VID√âO POUR CETTE QUESTION:
- Analyse de: "{video_metadata.title}"
- Environnement: {video_metadata.location_type} 
- Focus: {', '.join(video_metadata.suspicious_focus)}
- Contexte utilisateur: {video_metadata.detailed_description[:200]}...

Adapte ta r√©ponse selon ce contexte sp√©cifique."""
        
        return enhancement


# Fonctions utilitaires globales
def create_video_metadata_from_form(form_data: Dict[str, Any]) -> VideoContextMetadata:
    """Cr√©e un objet VideoContextMetadata √† partir des donn√©es formulaire."""
    
    return VideoContextMetadata(
        title=form_data.get('title', ''),
        location_type=form_data.get('location_type', 'Autre'),
        time_context=form_data.get('time_context', 'Non sp√©cifi√©'),
        expected_activities=form_data.get('expected_activities', []),
        suspicious_focus=form_data.get('suspicious_focus', []),
        camera_angle=form_data.get('camera_angle', 'Non sp√©cifi√©'),
        detailed_description=form_data.get('detailed_description', ''),
        analysis_priority=form_data.get('analysis_priority', '√âquilibr√©'),
        frame_sampling=form_data.get('frame_sampling', 'Standard'),
        user_timestamp=datetime.now().isoformat()
    )


def integrate_video_context_in_vlm_analysis(
    base_analysis_request,
    video_metadata: VideoContextMetadata
):
    """Int√®gre le contexte vid√©o dans une requ√™te d'analyse VLM."""
    
    if hasattr(base_analysis_request, 'context'):
        if not base_analysis_request.context:
            base_analysis_request.context = {}
        
        base_analysis_request.context.update({
            'video_metadata': video_metadata.to_dict(),
            'context_enhanced': True,
            'user_provided_context': True
        })
    
    return base_analysis_request


# Instance globale
_video_context_integration = None

def get_video_context_integration() -> VideoContextChatIntegration:
    """R√©cup√®re l'instance globale d'int√©gration contexte vid√©o."""
    global _video_context_integration
    if _video_context_integration is None:
        _video_context_integration = VideoContextChatIntegration()
    return _video_context_integration