#!/usr/bin/env python3
"""
Script d'Ã©valuation du systÃ¨me de surveillance intelligent.
Mesure les performances rÃ©elles de votre systÃ¨me.
"""

import sys
import time
import json
import os
from typing import Dict, List, Any
from datetime import datetime
import cv2
import numpy as np

sys.path.append('/home/elfried-kinzoun/PycharmProjects/intelligent-surveillance-system')

from main_Qwen import QwenOnlySurveillanceSystem
from src.core.types import SuspicionLevel, ActionType

class SystemEvaluator:
    def __init__(self):
        self.system = QwenOnlySurveillanceSystem()
        self.metrics = {
            'total_frames': 0,
            'processing_times': [],
            'detections': [],
            'suspicion_levels': [],
            'action_types': [],
            'errors': []
        }
    
    def evaluate_video(self, video_path: str, ground_truth: Dict = None) -> Dict[str, Any]:
        """Ã‰value le systÃ¨me sur une vidÃ©o spÃ©cifique."""
        print(f"ğŸ¥ Ã‰valuation de la vidÃ©o: {video_path}")
        
        if not os.path.exists(video_path):
            print(f"âŒ VidÃ©o non trouvÃ©e: {video_path}")
            return {}
        
        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        results = []
        
        while cap.read()[0]:
            frame_count += 1
        
        cap.release()
        print(f"ğŸ“Š VidÃ©o contient {frame_count} frames")
        
        # Analyser la vidÃ©o avec votre systÃ¨me
        start_time = time.time()
        
        try:
            # Utiliser votre systÃ¨me existant
            analysis_result = self.system.analyze_video(video_path)
            
            processing_time = time.time() - start_time
            
            # Calculer les mÃ©triques
            metrics = self._calculate_metrics(analysis_result, processing_time, frame_count)
            
            return metrics
            
        except Exception as e:
            print(f"âŒ Erreur lors de l'analyse: {e}")
            return {'error': str(e)}
    
    def _calculate_metrics(self, result: Any, processing_time: float, frame_count: int) -> Dict[str, Any]:
        """Calcule les mÃ©triques de performance."""
        
        # MÃ©triques de base
        fps = frame_count / processing_time if processing_time > 0 else 0
        latency_per_frame = processing_time / frame_count if frame_count > 0 else 0
        
        metrics = {
            'performance': {
                'total_processing_time': round(processing_time, 2),
                'fps': round(fps, 2),
                'latency_per_frame': round(latency_per_frame * 1000, 2),  # en ms
                'total_frames': frame_count
            }
        }
        
        # Analyser le rÃ©sultat si c'est un objet AnalysisResponse
        if hasattr(result, 'suspicion_level'):
            metrics['detection'] = {
                'suspicion_level': result.suspicion_level.value,
                'action_type': result.action_type.value,
                'confidence': result.confidence,
                'tools_used': result.tools_used,
                'recommendations_count': len(result.recommendations)
            }
        
        return metrics
    
    def run_benchmark(self, video_directory: str = None) -> Dict[str, Any]:
        """Lance un benchmark complet du systÃ¨me."""
        print("ğŸš€ Lancement du benchmark du systÃ¨me de surveillance")
        
        # VidÃ©os de test (vous pouvez ajouter vos propres vidÃ©os)
        test_videos = []
        
        if video_directory and os.path.exists(video_directory):
            for file in os.listdir(video_directory):
                if file.endswith(('.mp4', '.avi', '.mov')):
                    test_videos.append(os.path.join(video_directory, file))
        
        if not test_videos:
            print("âš ï¸ Aucune vidÃ©o de test trouvÃ©e. CrÃ©ons un test synthÃ©tique...")
            return self._synthetic_benchmark()
        
        # Ã‰valuer chaque vidÃ©o
        all_results = []
        for video_path in test_videos:
            result = self.evaluate_video(video_path)
            if result:
                all_results.append(result)
        
        # Calculer les statistiques globales
        global_metrics = self._aggregate_metrics(all_results)
        
        return global_metrics
    
    def _synthetic_benchmark(self) -> Dict[str, Any]:
        """Benchmark synthÃ©tique avec des donnÃ©es simulÃ©es."""
        print("ğŸ”¬ Test synthÃ©tique du systÃ¨me...")
        
        # Test des composants individuels
        metrics = {}
        
        # 1. Test de l'initialisation
        start_time = time.time()
        try:
            system = QwenOnlySurveillanceSystem()
            init_time = time.time() - start_time
            metrics['initialization'] = {
                'success': True,
                'time': round(init_time, 3)
            }
        except Exception as e:
            metrics['initialization'] = {
                'success': False,
                'error': str(e),
                'time': -1
            }
        
        # 2. Test des types Pydantic
        start_time = time.time()
        try:
            from src.core.types import AnalysisResponse, SuspicionLevel, ActionType
            
            response = AnalysisResponse(
                suspicion_level=SuspicionLevel.MEDIUM,
                action_type=ActionType.SUSPICIOUS_MOVEMENT,
                confidence=0.75,
                description="Test synthÃ©tique",
                reasoning="Test de performance",
                tools_used=["test"],
                recommendations=["test recommendation"],
                timestamp=datetime.now()
            )
            
            # Test de sÃ©rialisation
            serialized = response.model_dump()
            serialization_time = time.time() - start_time
            
            metrics['pydantic_serialization'] = {
                'success': True,
                'time': round(serialization_time * 1000, 3),  # en ms
                'keys_count': len(serialized)
            }
            
        except Exception as e:
            metrics['pydantic_serialization'] = {
                'success': False,
                'error': str(e)
            }
        
        # 3. Test TemporalTransformer
        start_time = time.time()
        try:
            from src.advanced_tools.temporal_transformer import TemporalTransformer
            
            transformer = TemporalTransformer()
            result = transformer.analyze_sequence([0.1, 0.5, 0.8, 0.3, 0.9], "behavior")
            transform_time = time.time() - start_time
            
            metrics['temporal_transformer'] = {
                'success': True,
                'time': round(transform_time * 1000, 3),  # en ms
                'result_keys': list(result.keys()) if isinstance(result, dict) else []
            }
            
        except Exception as e:
            metrics['temporal_transformer'] = {
                'success': False,
                'error': str(e)
            }
        
        return {
            'benchmark_type': 'synthetic',
            'timestamp': datetime.now().isoformat(),
            'components': metrics,
            'overall_health': all(m.get('success', False) for m in metrics.values())
        }
    
    def _aggregate_metrics(self, results: List[Dict]) -> Dict[str, Any]:
        """AgrÃ¨ge les mÃ©triques de plusieurs Ã©valuations."""
        if not results:
            return {}
        
        # Calculer moyennes, min, max
        processing_times = [r.get('performance', {}).get('total_processing_time', 0) for r in results]
        fps_values = [r.get('performance', {}).get('fps', 0) for r in results]
        
        return {
            'summary': {
                'total_videos_tested': len(results),
                'avg_processing_time': np.mean(processing_times),
                'avg_fps': np.mean(fps_values),
                'min_fps': np.min(fps_values),
                'max_fps': np.max(fps_values)
            },
            'detailed_results': results
        }
    
    def save_results(self, results: Dict, filename: str = None):
        """Sauvegarde les rÃ©sultats d'Ã©valuation."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"evaluation_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s dans: {filename}")

def main():
    """Fonction principale d'Ã©valuation."""
    evaluator = SystemEvaluator()
    
    print("ğŸ¯ Ã‰valuation du SystÃ¨me de Surveillance Intelligent")
    print("=" * 50)
    
    # Lancer le benchmark
    results = evaluator.run_benchmark()
    
    # Afficher les rÃ©sultats
    print("\nğŸ“Š RÃ‰SULTATS D'Ã‰VALUATION:")
    print("=" * 30)
    
    if results.get('benchmark_type') == 'synthetic':
        print("Type: Test SynthÃ©tique")
        print(f"SantÃ© Globale: {'âœ…' if results.get('overall_health') else 'âŒ'}")
        
        for component, metrics in results.get('components', {}).items():
            status = "âœ…" if metrics.get('success') else "âŒ"
            time_info = f" ({metrics.get('time', 'N/A')}ms)" if 'time' in metrics else ""
            print(f"  {component}: {status}{time_info}")
            
            if not metrics.get('success') and 'error' in metrics:
                print(f"    Erreur: {metrics['error']}")
    
    # Sauvegarder
    evaluator.save_results(results)
    
    print(f"\nğŸ‰ Ã‰valuation terminÃ©e!")
    return results

if __name__ == "__main__":
    main()