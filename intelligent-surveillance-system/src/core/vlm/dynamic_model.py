"""VLM avec architecture dual-VLM simplifi√©e: Kimi-VL-Thinking + Qwen2-VL fallback."""

import asyncio
from typing import Dict, List, Optional, Any
import torch
from PIL import Image
import base64
from io import BytesIO
from loguru import logger

from ..types import AnalysisRequest, AnalysisResponse, ToolResult
from ...utils.exceptions import ModelError, ProcessingError
from .model_registry import VLMModelRegistry, VLMModelType
from .prompt_builder import PromptBuilder
from .response_parser import ResponseParser
from .tools_integration import AdvancedToolsManager


class DynamicVisionLanguageModel:
    """
    VLM avec architecture dual-VLM simplifi√©e.
    
    Architecture:
    - PRINCIPAL: Kimi-VL-A3B-Thinking pour tous les cas d'usage
    - FALLBACK: Qwen2-VL-7B-Instruct si Kimi indisponible
    - Switch dynamique et fallback automatique
    - Monitoring de performance int√©gr√©
    """
    
    def __init__(
        self,
        default_model: str = "kimi-vl-a3b-thinking",
        device: str = "auto",
        enable_fallback: bool = False  # D√©sactiv√© par d√©faut pour √©conomiser la m√©moire
    ):
        self.device = self._setup_device(device)
        self.enable_fallback = enable_fallback
        
        # Registre des mod√®les
        self.model_registry = VLMModelRegistry()
        
        # √âtat actuel
        self.current_model_id = None
        self.current_config = None
        self.model = None
        self.processor = None
        self.is_loaded = False
        
        # Composants modulaires
        self.prompt_builder = PromptBuilder()
        self.response_parser = ResponseParser()
        self.tools_manager = AdvancedToolsManager()
        
        # Performance stats seulement (pas de cache par d√©faut pour √©conomiser m√©moire)
        self._performance_stats = {}
        
        # Chargement du mod√®le par d√©faut
        self.default_model = default_model
        logger.info(f"VLM Dynamique initialis√© - Mod√®le par d√©faut: {default_model}")
    
    def _setup_device(self, device: str) -> torch.device:
        """Configuration du device."""
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
            elif torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"
        return torch.device(device)
    
    async def load_model(self, model_id: str = None) -> bool:
        """Chargement d'un mod√®le sp√©cifique."""
        return await self._load_model_direct(model_id, enable_fallback=True)
    
    async def _load_model_direct(self, model_id: str = None, enable_fallback: bool = False) -> bool:
        """Chargement direct d'un mod√®le sans fallback automatique."""
        
        model_id = model_id or self.default_model
        
        # V√©rification disponibilit√©
        is_available, message = self.model_registry.validate_model_availability(model_id)
        if not is_available:
            logger.error(f"Mod√®le {model_id} non disponible: {message}")
            if enable_fallback and self.enable_fallback:
                return await self._load_fallback_model(exclude_models=[model_id])
            return False
        
        # R√©cup√©ration config
        config = self.model_registry.get_model_config(model_id)
        if not config:
            logger.error(f"Configuration manquante pour {model_id}")
            if enable_fallback and self.enable_fallback:
                return await self._load_fallback_model(exclude_models=[model_id])
            return False
        
        try:
            logger.info(f"Chargement {config.model_type.value.upper()} : {model_id}")
            
            # Chargement selon le type
            success = await self._load_model_by_type(config)
            
            if success:
                self.current_model_id = model_id
                self.current_config = config
                self.is_loaded = True
                
                logger.info(f"‚úÖ {model_id} charg√© avec succ√®s")
                return True
            else:
                logger.error(f"‚ùå √âchec chargement {model_id}")
                if enable_fallback and self.enable_fallback:
                    return await self._load_fallback_model(exclude_models=[model_id])
                return False
                
        except Exception as e:
            logger.error(f"Erreur chargement {model_id}: {e}")
            if enable_fallback and self.enable_fallback:
                return await self._load_fallback_model(exclude_models=[model_id])
            return False
    
    async def _load_model_by_type(self, config) -> bool:
        """Chargement selon le type de mod√®le."""
        
        try:
            # Configuration commune - FIX: torch.float16 au lieu de torch.auto
            torch_dtype_str = config.default_params.get("torch_dtype", "float16")
            if torch_dtype_str == "auto":
                torch_dtype = torch.float16 if self.device.type == "cuda" else torch.float32
            else:
                torch_dtype = getattr(torch, torch_dtype_str, torch.float16)
                
            model_kwargs = {
                "torch_dtype": torch_dtype,
                "device_map": config.default_params.get("device_map", "auto")
            }
            
            # Quantization si support√©e
            if config.default_params.get("load_in_4bit") and self.device.type == "cuda":
                from transformers import BitsAndBytesConfig
                model_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            elif config.default_params.get("load_in_8bit") and self.device.type == "cuda":
                from transformers import BitsAndBytesConfig
                model_kwargs["quantization_config"] = BitsAndBytesConfig(load_in_8bit=True)
            
            # Trust remote code si n√©cessaire
            if config.default_params.get("trust_remote_code"):
                model_kwargs["trust_remote_code"] = True
            
            # === CHARGEMENT PAR TYPE ===
            
            if config.model_type == VLMModelType.KIMI_VL:
                success = await self._load_kimi_vl_model(config, model_kwargs)
            
            
            elif config.model_type == VLMModelType.QWEN:
                success = await self._load_qwen_model(config, model_kwargs)
            
            else:
                logger.error(f"Type de mod√®le non support√©: {config.model_type}")
                return False
            
            # Configuration finale
            if success and self.model:
                if not model_kwargs.get("device_map"):
                    self.model = self.model.to(self.device)
                self.model.eval()
            
            return success
            
        except Exception as e:
            logger.error(f"Erreur chargement mod√®le: {e}")
            return False
    
    async def _load_kimi_vl_model(self, config, model_kwargs) -> bool:
        """Chargement sp√©cifique Kimi-VL (Moonshot AI)."""
        try:
            # Kimi-VL utilise transformers>=4.48.2 avec trust_remote_code
            from transformers import AutoProcessor, AutoModelForCausalLM
            
            logger.info(f"Chargement Kimi-VL : {config.model_name}")
            
            # Forcer l'usage CPU pour √©viter les probl√®mes SDPA
            model_kwargs_fixed = model_kwargs.copy()
            model_kwargs_fixed["attn_implementation"] = "eager"  # √âvite SDPA
            
            # Chargement du processor 
            self.processor = AutoProcessor.from_pretrained(
                config.model_name, 
                trust_remote_code=True
            )
            
            # Chargement du mod√®le (AutoModelForCausalLM pour Kimi-VL)
            self.model = AutoModelForCausalLM.from_pretrained(
                config.model_name, 
                **model_kwargs_fixed
            )
            
            # Fix pour l'attribut _supports_sdpa manquant
            if not hasattr(self.model, '_supports_sdpa'):
                self.model._supports_sdpa = False
            
            logger.info("‚úÖ Kimi-VL charg√© avec succ√®s")
            return True
            
        except Exception as e:
            logger.error(f"Erreur chargement Kimi-VL: {e}")
            logger.warning("üí° V√©rifiez: transformers>=4.48.2 et trust_remote_code=True")
            return False
    
    
    async def _load_qwen_model(self, config, model_kwargs) -> bool:
        """Chargement sp√©cifique Qwen2-VL et Qwen2.5-VL."""
        try:
            from transformers import AutoProcessor
            
            # Support Qwen2.5-VL et Qwen2-VL
            if "qwen2.5-vl" in config.model_name.lower():
                from transformers import Qwen2_5_VLForConditionalGeneration
                model_class = Qwen2_5_VLForConditionalGeneration
                logger.info(f"Chargement Qwen2.5-VL : {config.model_name}")
            else:
                from transformers import AutoModelForVision2Seq
                model_class = AutoModelForVision2Seq
                logger.info(f"Chargement Qwen2-VL : {config.model_name}")
            
            self.processor = AutoProcessor.from_pretrained(
                config.model_name, 
                trust_remote_code=True
            )
            self.model = model_class.from_pretrained(
                config.model_name, 
                **model_kwargs
            )
            
            logger.info("‚úÖ Qwen VL charg√© avec succ√®s")
            return True
            
        except Exception as e:
            logger.error(f"Erreur chargement Qwen VL: {e}")
            return False
    
    async def _load_fallback_model(self, exclude_models: List[str] = None) -> bool:
        """Chargement d'un mod√®le de fallback."""
        if exclude_models is None:
            exclude_models = []
            
        logger.warning("Tentative de chargement du mod√®le de fallback...")
        
        # Architecture simplifi√©e: Qwen2-VL UNIQUEMENT en fallback
        all_fallback_models = [
            "qwen2-vl-7b-instruct",    # FALLBACK UNIQUE
        ]
        
        # Filtrer les mod√®les d√©j√† tent√©s
        fallback_models = [m for m in all_fallback_models if m not in exclude_models]
        
        if not fallback_models:
            logger.error("‚ùå Aucun mod√®le de fallback disponible")
            return False
        
        for fallback_id in fallback_models:
            try:
                logger.info(f"Tentative fallback: {fallback_id}")
                # Appel direct sans fallback pour √©viter la r√©cursion
                success = await self._load_model_direct(fallback_id)
                if success:
                    logger.warning(f"‚úÖ Fallback r√©ussi avec {fallback_id}")
                    return True
            except Exception as e:
                logger.error(f"Fallback {fallback_id} √©chou√©: {e}")
                continue
        
        logger.error("‚ùå Aucun mod√®le de fallback disponible")
        return False
    
    async def switch_model(self, new_model_id: str) -> bool:
        """Switch vers un nouveau mod√®le √† chaud."""
        
        if new_model_id == self.current_model_id:
            logger.info(f"Mod√®le {new_model_id} d√©j√† charg√©")
            return True
        
        logger.info(f"Switch: {self.current_model_id} ‚Üí {new_model_id}")
        
        # Sauvegarde de l'√©tat actuel
        old_model_id = self.current_model_id
        
        # D√©chargement propre
        self._unload_current_model()
        
        # Chargement du nouveau mod√®le
        success = await self.load_model(new_model_id)
        
        if success:
            logger.success(f"‚úÖ Switch r√©ussi vers {new_model_id}")
            return True
        else:
            logger.error(f"‚ùå Switch √©chou√©, retour √† {old_model_id}")
            # Tentative de retour √† l'ancien mod√®le
            if old_model_id:
                await self.load_model(old_model_id)
            return False
    
    def _unload_current_model(self):
        """D√©chargement propre du mod√®le actuel avec nettoyage m√©moire optimis√©."""
        if self.model is not None:
            # Lib√©ration de la m√©moire
            if hasattr(self.model, 'cpu'):
                self.model.cpu()
            del self.model
            
        if self.processor is not None:
            del self.processor
            
        # Nettoyage m√©moire avanc√©
        import gc
        gc.collect()  # Garbage collection Python
            
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        self.model = None
        self.processor = None
        self.is_loaded = False
        self.current_model_id = None
        self.current_config = None
        
        logger.info("‚úÖ Mod√®le d√©charg√© et m√©moire lib√©r√©e (optimis√©)")
    
    async def analyze_with_tools(
        self, 
        request: AnalysisRequest,
        use_advanced_tools: bool = True
    ) -> AnalysisResponse:
        """Analyse avec le mod√®le actuellement charg√©."""
        
        # V√©rification plus robuste du mod√®le charg√©
        if not self.is_loaded or self.model is None or self.current_model_id != (request.preferred_model or self.default_model):
            logger.info(f"Rechargement n√©cessaire: loaded={self.is_loaded}, model={self.model is not None}, current={self.current_model_id}")
            success = await self.load_model(request.preferred_model or self.default_model)
            if not success:
                raise ModelError("Aucun mod√®le VLM disponible")
        else:
            logger.info(f"‚úÖ Mod√®le {self.current_model_id} d√©j√† charg√© - r√©utilisation du cache")
        
        try:
            # Pr√©paration de l'image
            image = self._prepare_image(request.frame_data)
            image_array = self._pil_to_numpy(image)
            
            # Ex√©cution des outils si demand√©s
            tools_results = {}
            if use_advanced_tools and request.tools_available:
                logger.info(f"Ex√©cution outils avec {self.current_model_id}: {request.tools_available}")
                tools_results = await self.tools_manager.execute_tools(
                    image_array, 
                    request.tools_available,
                    request.context
                )
            
            # Construction du prompt optimis√© pour le mod√®le
            prompt = self._build_model_specific_prompt(request, tools_results)
            
            # G√©n√©ration avec le mod√®le actuel
            response_text = await self._generate_response(image, prompt)
            
            # Parse de la r√©ponse
            analysis_result = self.response_parser.parse_vlm_response(response_text)
            
            # Ajout d'informations sur le mod√®le utilis√©
            analysis_result.description += f" | Mod√®le: {self.current_model_id}"
            
            # Affichage d√©taill√© des d√©cisions VLM
            self._log_vlm_decision(response_text, analysis_result)
            
            logger.success(f"Analyse {self.current_model_id}: {analysis_result.suspicion_level.value}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"Erreur analyse {self.current_model_id}: {e}")
            return self._create_error_response(str(e))
    
    async def analyze_with_custom_prompt(
        self, 
        frame_data: str,
        custom_prompt: str,
        context: Dict[str, Any] = None
    ) -> AnalysisResponse:
        """Analyse avec prompt personnalis√© pour r√©sum√©s cumulatifs."""
        
        # Cr√©er une requ√™te simplifi√©e
        request = AnalysisRequest(
            frame_data=frame_data,
            context=context or {},
            tools_available=[],  # Pas d'outils pour les r√©sum√©s
            preferred_model=self.current_model_id
        )
        
        try:
            # V√©rifier que le mod√®le est charg√©
            if not self.is_loaded:
                success = await self.load_model(request.preferred_model or self.default_model)
                if not success:
                    raise ModelError("Aucun mod√®le VLM disponible")
            
            # Pr√©paration de l'image
            image = self._prepare_image(request.frame_data)
            
            # G√©n√©ration avec prompt personnalis√©
            response_text = await self._generate_response(image, custom_prompt)
            
            # Parse de la r√©ponse (simple car pas d'outils)
            analysis_result = self.response_parser.parse_vlm_response(response_text)
            
            logger.success(f"Analyse custom {self.current_model_id}: {analysis_result.suspicion_level.value}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"Erreur analyse custom {self.current_model_id}: {e}")
            return self._create_error_response(str(e))
    
    def _build_model_specific_prompt(self, request: AnalysisRequest, tools_results: Dict) -> str:
        """Construction de prompt optimis√© selon le mod√®le."""
        
        # Prompt de base
        base_prompt = self.prompt_builder.build_surveillance_prompt(
            request.context,
            request.tools_available,
            tools_results
        )
        
        # Optimisations sp√©cifiques par mod√®le
        if self.current_config and self.current_config.model_type == VLMModelType.KIM:
            # KIM pr√©f√®re des prompts structur√©s avec √©tapes
            base_prompt += "\n\nRaisonne √©tape par √©tape pour cette analyse de surveillance."
        
        elif self.current_config and self.current_config.model_type == VLMModelType.QWEN:
            # Qwen2-VL excelle avec des instructions pr√©cises
            base_prompt += "\n\nSois pr√©cis et factuel dans ton analyse. Justifie chaque conclusion."
        
        
        return base_prompt
    
    async def _generate_response(self, image: Image.Image, prompt: str) -> str:
        """G√©n√©ration optimis√©e selon le mod√®le."""
        
        try:
            logger.debug(f"D√©but g√©n√©ration {self.current_model_id}")
            
            # Pr√©paration des inputs
            logger.debug("Pr√©paration des inputs...")
            inputs = self.processor(
                text=prompt,
                images=image,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            logger.debug(f"Inputs pr√©par√©s, device: {self.device}")
            
            # Param√®tres de g√©n√©ration du mod√®le actuel
            gen_params = {
                "max_new_tokens": self.current_config.default_params.get("max_new_tokens", 512),
                "temperature": self.current_config.default_params.get("temperature", 0.1),
                "do_sample": self.current_config.default_params.get("do_sample", True),
                "pad_token_id": self.processor.tokenizer.eos_token_id if hasattr(self.processor, 'tokenizer') else None
            }
            logger.debug(f"Param√®tres de g√©n√©ration: {gen_params}")
            
            # G√©n√©ration avec gestion d'erreurs am√©lior√©e
            logger.debug("D√©but g√©n√©ration du mod√®le...")
            with torch.no_grad():
                try:
                    generated_ids = self.model.generate(**inputs, **gen_params)
                except Exception as gen_error:
                    # R√©essayer avec des param√®tres plus s√ªrs
                    logger.warning(f"Erreur g√©n√©ration, r√©essai avec param√®tres basiques: {gen_error}")
                    safe_params = {
                        "max_new_tokens": 256,
                        "do_sample": False,  # D√©sactiver sampling
                        "pad_token_id": self.processor.tokenizer.eos_token_id if hasattr(self.processor, 'tokenizer') else None
                    }
                    # Nettoyer les param√®tres None
                    safe_params = {k: v for k, v in safe_params.items() if v is not None}
                    generated_ids = self.model.generate(**inputs, **safe_params)
            logger.debug("G√©n√©ration termin√©e")
            
            # D√©codage
            logger.debug("D√©but d√©codage...")
            generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            logger.debug(f"D√©codage termin√©, longueur: {len(generated_text)}")
            
            # Extraction de la r√©ponse
            if prompt in generated_text:
                response = generated_text.split(prompt)[-1].strip()
            else:
                response = generated_text.strip()
            
            return response
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration {self.current_model_id}: {e}")
            raise ProcessingError(f"G√©n√©ration √©chou√©e: {e}")
    
    def _prepare_image(self, image_data: str) -> Image.Image:
        """Pr√©paration de l'image."""
        try:
            image_bytes = base64.b64decode(image_data)
            image = Image.open(BytesIO(image_bytes)).convert("RGB")
            
            # Redimensionnement optimal selon le mod√®le
            max_size = 768  # Standard pour la plupart des VLM
            if self.current_config and self.current_config.model_type == VLMModelType.QWEN:
                max_size = 1024  # Qwen2-VL supporte des images plus grandes
            
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = tuple(int(dim * ratio) for dim in image.size)
                image = image.resize(new_size, Image.Resampling.LANCZOS)
            
            return image
            
        except Exception as e:
            raise ProcessingError(f"Erreur pr√©paration image: {e}")
    
    def _pil_to_numpy(self, image: Image.Image):
        """Conversion PIL vers numpy pour les outils."""
        import numpy as np
        return np.array(image)
    
    def _create_error_response(self, error: str) -> AnalysisResponse:
        """R√©ponse d'erreur."""
        from ..types import SuspicionLevel, ActionType
        
        return AnalysisResponse(
            suspicion_level=SuspicionLevel.LOW,
            action_type=ActionType.NORMAL_SHOPPING,
            confidence=0.0,
            description=f"Erreur VLM ({self.current_model_id}): {error}",
            tools_used=[],
            recommendations=["V√©rification syst√®me requise"]
        )
    
    def get_system_status(self) -> Dict[str, Any]:
        """√âtat complet du syst√®me multi-mod√®les."""
        
        # Statut des outils
        tools_status = self.tools_manager.get_tools_status()
        available_tools = self.tools_manager.get_available_tools()
        
        # Mod√®les disponibles
        available_models = {}
        for model_id, config in self.model_registry.list_available_models().items():
            is_available, message = self.model_registry.validate_model_availability(model_id)
            available_models[model_id] = {
                "available": is_available,
                "message": message,
                "type": config.model_type.value,
                "description": config.description
            }
        
        return {
            "current_model": {
                "model_id": self.current_model_id,
                "model_type": self.current_config.model_type.value if self.current_config else None,
                "is_loaded": self.is_loaded,
                "device": str(self.device),
                "supports_tool_calling": self.current_config.supports_tool_calling if self.current_config else False
            },
            "available_models": available_models,
            "tools_status": tools_status,
            "available_tools": available_tools,
            "system": {
                "enable_fallback": self.enable_fallback,
                "device": str(self.device),
                "cuda_available": torch.cuda.is_available()
            }
        }
    
    def get_model_recommendations(self) -> Dict[str, str]:
        """Recommandations de mod√®les selon les cas d'usage."""
        return {
            "surveillance_principal": self.model_registry.get_recommended_model("surveillance"),
            "haute_performance": self.model_registry.get_recommended_model("high_performance"),
            "economie_memoire": self.model_registry.get_recommended_model("memory_efficient"),
            "raisonnement_avance": self.model_registry.get_recommended_model("reasoning"),
            "flagship": self.model_registry.get_recommended_model("flagship")
        }
    
    def _log_vlm_decision(self, response_text: str, analysis_result) -> None:
        """Affichage d√©taill√© de la d√©cision VLM."""
        print("\n" + "="*80)
        print("üß† ANALYSE VLM D√âTAILL√âE")
        print("="*80)
        
        # Essayer d'extraire le thinking du JSON
        try:
            import json
            # Chercher le JSON dans la r√©ponse
            if "thinking" in response_text.lower():
                start = response_text.find("{")
                end = response_text.rfind("}")
                if start != -1 and end != -1:
                    json_str = response_text[start:end+1]
                    parsed = json.loads(json_str)
                    
                    if "thinking" in parsed:
                        print("ü§î THINKING PROCESSUS:")
                        print(parsed["thinking"])
                        print()
                    
                    if "observations" in parsed:
                        print("üëÅÔ∏è OBSERVATIONS:")
                        obs = parsed["observations"]
                        if isinstance(obs, dict):
                            for key, value in obs.items():
                                print(f"  ‚Ä¢ {key}: {value}")
                        else:
                            print(f"  {obs}")
                        print()
                    
                    if "decision_final" in parsed:
                        print("‚öñÔ∏è D√âCISION FINALE:")
                        print(parsed["decision_final"])
                        print()
                        
        except Exception:
            # Si parsing JSON √©choue, afficher la r√©ponse brute
            print("üìÑ R√âPONSE COMPL√àTE VLM:")
            print(response_text[:1000] + "..." if len(response_text) > 1000 else response_text)
            print()
        
        # Toujours afficher le r√©sum√© structur√©
        print("üìä R√âSUM√â DE D√âCISION:")
        print(f"  üö® Suspicion: {analysis_result.suspicion_level.value}")
        print(f"  üéØ Action: {analysis_result.action_type}")
        print(f"  üìà Confiance: {analysis_result.confidence:.2f}")
        print(f"  üí≠ Raisonnement: {analysis_result.reasoning}")
        print(f"  üìã Recommandations: {', '.join(analysis_result.recommendations)}")
        print("="*80 + "\n")

    async def shutdown(self):
        """Arr√™t propre."""
        logger.info("Arr√™t du VLM dynamique...")
        self._unload_current_model()
        logger.info("VLM dynamique arr√™t√©")