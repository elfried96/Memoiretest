#!/usr/bin/env python3
"""
ğŸš€ Tests d'IntÃ©gration - ModÃ¨les GPU RÃ©els
==========================================

Tests approfondis des vrais modÃ¨les pour valider leur fonctionnement.
ComplÃ©ment aux tests CPU-only pour validation complÃ¨te du systÃ¨me.
"""

import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
import traceback

# Import optionnel de numpy
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    # Mock simple pour numpy
    class MockNumpy:
        @staticmethod
        def zeros(shape, dtype=None):
            if len(shape) == 3:  # Image
                return [[[0 for _ in range(shape[2])] for _ in range(shape[1])] for _ in range(shape[0])]
            return 0
        @staticmethod
        def array(data, dtype=None):
            return data
        @staticmethod
        def random():
            class MockRandom:
                @staticmethod
                def rand(*args):
                    import random
                    if args:
                        return [random.random() for _ in range(args[0])]
                    return random.random()
            return MockRandom()
        uint8 = int
        int32 = int
    np = MockNumpy()

# Import optionnel de cv2
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    # Mock simple pour cv2 si pas disponible
    class MockCV2:
        @staticmethod
        def rectangle(img, pt1, pt2, color, thickness): pass
        @staticmethod 
        def circle(img, center, radius, color, thickness): pass
        @staticmethod
        def fillPoly(img, pts, color): pass
    cv2 = MockCV2()

# Import optionnel de torch
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    class MockTorch:
        @staticmethod
        def cuda():
            class MockCuda:
                @staticmethod
                def is_available(): return False
                @staticmethod
                def get_device_properties(idx): return MockDevice()
            return MockCuda()
        
        class MockDevice:
            total_memory = 0
            
    torch = MockTorch()

# Configuration du path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

class IntegrationTestRunner:
    """Runner de tests d'intÃ©gration avec GPU"""
    
    def __init__(self):
        self.results = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'errors': [],
            'execution_time': 0,
            'gpu_available': TORCH_AVAILABLE and torch.cuda.is_available(),
            'gpu_memory': torch.cuda.get_device_properties(0).total_memory if (TORCH_AVAILABLE and torch.cuda.is_available()) else 0
        }
        
    def create_test_image(self, width: int = 640, height: int = 480):
        """CrÃ©er une image de test"""
        # Image avec des formes gÃ©omÃ©triques pour tester la dÃ©tection
        img = np.zeros((height, width, 3), dtype=np.uint8)
        
        if CV2_AVAILABLE:
            # Rectangle (simule une personne)
            cv2.rectangle(img, (100, 100), (200, 300), (255, 0, 0), -1)
            
            # Cercle (simule un objet)
            cv2.circle(img, (400, 200), 50, (0, 255, 0), -1)
            
            # Polygone (simule electronics)
            pts = np.array([[300, 50], [350, 50], [375, 100], [325, 120], [275, 100]], np.int32)
            cv2.fillPoly(img, [pts], (0, 0, 255))
        
        return img
    
    def test_yolo_real_model(self) -> Dict[str, Any]:
        """Test du modÃ¨le YOLO rÃ©el"""
        print("\nğŸ¯ TEST YOLO RÃ‰EL")
        print("=" * 40)
        
        test_result = {
            'name': 'YOLO Real Model',
            'passed': False,
            'execution_time': 0,
            'details': {},
            'errors': []
        }
        
        start_time = time.time()
        
        try:
            # Import du modÃ¨le YOLO
            from src.detection.yolo_detector import YOLODetector
            
            # Initialisation
            print("ğŸ“¥ Chargement du modÃ¨le YOLO...")
            detector = YOLODetector()
            
            # Image de test
            test_image = self.create_test_image()
            print(f"ğŸ–¼ï¸ Image de test crÃ©Ã©e: {test_image.shape}")
            
            # Test de dÃ©tection
            print("ğŸ” ExÃ©cution dÃ©tection...")
            detections = detector.detect(test_image)
            
            # Validation des rÃ©sultats
            test_result['details'] = {
                'model_loaded': True,
                'detections_count': len(detections),
                'detections': detections[:3],  # Premiers rÃ©sultats
                'image_processed': True,
                'gpu_used': TORCH_AVAILABLE and hasattr(detector, 'model') and torch.cuda.is_available()
            }
            
            # CritÃ¨res de succÃ¨s
            if len(detections) > 0:
                print(f"âœ… DÃ©tections trouvÃ©es: {len(detections)}")
                for i, det in enumerate(detections[:3]):
                    print(f"   {i+1}. {det.get('class_name', 'unknown')}: {det.get('confidence', 0):.2f}")
                test_result['passed'] = True
            else:
                print("âš ï¸ Aucune dÃ©tection trouvÃ©e")
                test_result['passed'] = False
                
        except ImportError as e:
            error_msg = f"Module YOLO non trouvÃ©: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
        except Exception as e:
            error_msg = f"Erreur YOLO: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
            traceback.print_exc()
        
        test_result['execution_time'] = time.time() - start_time
        print(f"â±ï¸ Temps d'exÃ©cution: {test_result['execution_time']:.2f}s")
        
        return test_result
    
    def test_sam2_real_model(self) -> Dict[str, Any]:
        """Test du modÃ¨le SAM2 rÃ©el"""
        print("\nğŸ¯ TEST SAM2 RÃ‰EL")
        print("=" * 40)
        
        test_result = {
            'name': 'SAM2 Real Model',
            'passed': False,
            'execution_time': 0,
            'details': {},
            'errors': []
        }
        
        start_time = time.time()
        
        try:
            # Import du modÃ¨le SAM2
            from src.advanced_tools.sam2_segmentation import SAM2Segmentation
            
            # Initialisation
            print("ğŸ“¥ Chargement du modÃ¨le SAM2...")
            segmentator = SAM2Segmentation()
            
            # Image de test
            test_image = self.create_test_image()
            print(f"ğŸ–¼ï¸ Image de test crÃ©Ã©e: {test_image.shape}")
            
            # Test de segmentation
            print("ğŸ” ExÃ©cution segmentation...")
            segments = segmentator.segment_everything(test_image)
            
            # Validation des rÃ©sultats
            test_result['details'] = {
                'model_loaded': True,
                'segments_count': len(segments.get('masks', [])),
                'total_segments': segments.get('total_segments', 0),
                'processing_time': segments.get('processing_time', 0),
                'image_processed': True
            }
            
            # CritÃ¨res de succÃ¨s
            if segments.get('total_segments', 0) > 0:
                print(f"âœ… Segments trouvÃ©s: {segments.get('total_segments', 0)}")
                print(f"   Temps processing: {segments.get('processing_time', 0)*1000:.1f}ms")
                test_result['passed'] = True
            else:
                print("âš ï¸ Aucun segment trouvÃ©")
                test_result['passed'] = False
                
        except ImportError as e:
            error_msg = f"Module SAM2 non trouvÃ©: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
        except Exception as e:
            error_msg = f"Erreur SAM2: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
            traceback.print_exc()
        
        test_result['execution_time'] = time.time() - start_time
        print(f"â±ï¸ Temps d'exÃ©cution: {test_result['execution_time']:.2f}s")
        
        return test_result
    
    def test_dino_v2_real_model(self) -> Dict[str, Any]:
        """Test du modÃ¨le DINO v2 rÃ©el"""
        print("\nğŸ¯ TEST DINO V2 RÃ‰EL")
        print("=" * 40)
        
        test_result = {
            'name': 'DINO v2 Real Model',
            'passed': False,
            'execution_time': 0,
            'details': {},
            'errors': []
        }
        
        start_time = time.time()
        
        try:
            # Import du modÃ¨le DINO v2
            from src.advanced_tools.dino_features import DinoFeatures
            
            # Initialisation
            print("ğŸ“¥ Chargement du modÃ¨le DINO v2...")
            extractor = DinoFeatures()
            
            # Image de test
            test_image = self.create_test_image()
            print(f"ğŸ–¼ï¸ Image de test crÃ©Ã©e: {test_image.shape}")
            
            # Test d'extraction de features
            print("ğŸ” ExÃ©cution extraction features...")
            features = extractor.extract_features(test_image)
            
            # Validation des rÃ©sultats
            test_result['details'] = {
                'model_loaded': True,
                'features_extracted': features.get('success', False),
                'feature_dimension': len(features.get('features', [])),
                'confidence': features.get('confidence', 0),
                'processing_time': features.get('processing_time', 0)
            }
            
            # CritÃ¨res de succÃ¨s
            if features.get('success', False) and len(features.get('features', [])) > 0:
                print(f"âœ… Features extraites: {len(features.get('features', []))} dimensions")
                print(f"   Confiance: {features.get('confidence', 0):.2f}")
                print(f"   Temps processing: {features.get('processing_time', 0)*1000:.1f}ms")
                test_result['passed'] = True
            else:
                print("âš ï¸ Ã‰chec extraction features")
                test_result['passed'] = False
                
        except ImportError as e:
            error_msg = f"Module DINO v2 non trouvÃ©: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
        except Exception as e:
            error_msg = f"Erreur DINO v2: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
            traceback.print_exc()
        
        test_result['execution_time'] = time.time() - start_time
        print(f"â±ï¸ Temps d'exÃ©cution: {test_result['execution_time']:.2f}s")
        
        return test_result
    
    def test_kimi_vlm_real_model(self) -> Dict[str, Any]:
        """Test du modÃ¨le Kimi VLM rÃ©el"""
        print("\nğŸ¯ TEST KIMI VLM RÃ‰EL")  
        print("=" * 40)
        
        test_result = {
            'name': 'Kimi VLM Real Model',
            'passed': False,
            'execution_time': 0,
            'details': {},
            'errors': []
        }
        
        start_time = time.time()
        
        try:
            # Import du modÃ¨le Kimi VLM
            from src.core.vlm.model import VLMModel
            
            # Initialisation
            print("ğŸ“¥ Initialisation du modÃ¨le Kimi VLM...")
            analyzer = VLMModel()
            
            # Image de test
            test_image = self.create_test_image()
            print(f"ğŸ–¼ï¸ Image de test crÃ©Ã©e: {test_image.shape}")
            
            # Test d'analyse de scÃ¨ne
            print("ğŸ” ExÃ©cution analyse de scÃ¨ne...")
            analysis = analyzer.analyze_scene(test_image, "person detected with objects")
            
            # Validation des rÃ©sultats
            test_result['details'] = {
                'model_loaded': True,
                'analysis_completed': analysis.get('success', False),
                'suspicion_level': analysis.get('suspicion_level', 0),
                'confidence': analysis.get('confidence', 0),
                'processing_time': analysis.get('processing_time', 0),
                'description': analysis.get('description', '')[:100]  # Premiers 100 caractÃ¨res
            }
            
            # CritÃ¨res de succÃ¨s
            if analysis.get('success', False):
                print(f"âœ… Analyse complÃ©tÃ©e")
                print(f"   Niveau suspicion: {analysis.get('suspicion_level', 0):.2f}")
                print(f"   Confiance: {analysis.get('confidence', 0):.2f}")
                print(f"   Temps processing: {analysis.get('processing_time', 0)*1000:.1f}ms")
                test_result['passed'] = True
            else:
                print("âš ï¸ Ã‰chec analyse de scÃ¨ne")
                test_result['passed'] = False
                
        except ImportError as e:
            error_msg = f"Module Kimi VLM non trouvÃ©: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
        except Exception as e:
            error_msg = f"Erreur Kimi VLM: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
            traceback.print_exc()
        
        test_result['execution_time'] = time.time() - start_time
        print(f"â±ï¸ Temps d'exÃ©cution: {test_result['execution_time']:.2f}s")
        
        return test_result
    
    def test_complete_pipeline(self) -> Dict[str, Any]:
        """Test du pipeline complet d'orchestration"""
        print("\nğŸ¯ TEST PIPELINE COMPLET")
        print("=" * 40)
        
        test_result = {
            'name': 'Complete Pipeline',
            'passed': False,
            'execution_time': 0,
            'details': {},
            'errors': []
        }
        
        start_time = time.time()
        
        try:
            # Import de l'orchestrateur
            from src.core.orchestrator.adaptive_orchestrator import AdaptiveOrchestrator
            
            # Initialisation
            print("ğŸ“¥ Initialisation pipeline complet...")
            orchestrator = AdaptiveOrchestrator()
            
            # Image de test
            test_image = self.create_test_image()
            print(f"ğŸ–¼ï¸ Image de test crÃ©Ã©e: {test_image.shape}")
            
            # Test du pipeline complet
            print("ğŸ” ExÃ©cution pipeline complet...")
            result = orchestrator.analyze_frame(test_image)
            
            # Validation des rÃ©sultats
            test_result['details'] = {
                'pipeline_executed': result.get('success', False),
                'tools_used': len(result.get('tools_results', {})),
                'total_detections': len(result.get('detections', [])),
                'suspicion_score': result.get('suspicion_score', 0),
                'processing_time': result.get('processing_time', 0),
                'mode_used': result.get('mode_used', 'unknown')
            }
            
            # CritÃ¨res de succÃ¨s
            if result.get('success', False):
                print(f"âœ… Pipeline exÃ©cutÃ© avec succÃ¨s")
                print(f"   Outils utilisÃ©s: {len(result.get('tools_results', {}))}")
                print(f"   DÃ©tections: {len(result.get('detections', []))}")
                print(f"   Score suspicion: {result.get('suspicion_score', 0):.2f}")
                print(f"   Mode: {result.get('mode_used', 'unknown')}")
                print(f"   Temps total: {result.get('processing_time', 0)*1000:.1f}ms")
                test_result['passed'] = True
            else:
                print("âš ï¸ Ã‰chec du pipeline complet")
                test_result['passed'] = False
                
        except ImportError as e:
            error_msg = f"Module orchestrateur non trouvÃ©: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
        except Exception as e:
            error_msg = f"Erreur pipeline: {e}"
            print(f"âŒ {error_msg}")
            test_result['errors'].append(error_msg)
            traceback.print_exc()
        
        test_result['execution_time'] = time.time() - start_time
        print(f"â±ï¸ Temps d'exÃ©cution: {test_result['execution_time']:.2f}s")
        
        return test_result
    
    def generate_integration_report(self, test_results: List[Dict[str, Any]]):
        """GÃ©nÃ¨re le rapport d'intÃ©gration"""
        print("\n" + "=" * 60)
        print("ğŸ“Š RAPPORT TESTS D'INTÃ‰GRATION GPU")
        print("=" * 60)
        
        # Statistiques GPU
        print(f"\nğŸ–¥ï¸ ENVIRONNEMENT GPU:")
        print(f"   GPU disponible: {'âœ…' if self.results['gpu_available'] else 'âŒ'}")
        if self.results['gpu_available']:
            print(f"   MÃ©moire GPU: {self.results['gpu_memory'] / (1024**3):.1f} GB")
            if TORCH_AVAILABLE:
                print(f"   Device: {torch.cuda.get_device_name(0)}")
        else:
            print(f"   PyTorch disponible: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
        
        # RÃ©sultats par modÃ¨le
        total_passed = sum(1 for test in test_results if test['passed'])
        total_tests = len(test_results)
        
        print(f"\nğŸ“ˆ RÃ‰SULTATS GLOBAUX:")
        print(f"   Tests exÃ©cutÃ©s: {total_tests}")
        print(f"   Tests rÃ©ussis: {total_passed}")
        print(f"   Tests Ã©chouÃ©s: {total_tests - total_passed}")
        print(f"   Taux de succÃ¨s: {(total_passed/total_tests*100):.1f}%")
        
        print(f"\nğŸ¯ DÃ‰TAILS PAR MODÃˆLE:")
        for test in test_results:
            status = "âœ…" if test['passed'] else "âŒ"
            print(f"   {status} {test['name']}: {test['execution_time']:.2f}s")
            
            if test['details']:
                for key, value in test['details'].items():
                    if isinstance(value, bool):
                        print(f"      â€¢ {key}: {'âœ…' if value else 'âŒ'}")
                    elif isinstance(value, (int, float)):
                        print(f"      â€¢ {key}: {value}")
            
            if test['errors']:
                for error in test['errors']:
                    print(f"      âš ï¸ {error}")
        
        # Recommandations
        print(f"\nğŸ’¡ RECOMMANDATIONS:")
        if not self.results['gpu_available']:
            print("   â€¢ Installer CUDA pour accÃ©lÃ©ration GPU")
            print("   â€¢ Tests exÃ©cutÃ©s en mode CPU uniquement")
        
        failed_tests = [test for test in test_results if not test['passed']]
        if failed_tests:
            print("   â€¢ Modules manquants Ã  installer:")
            for test in failed_tests:
                if test['errors']:
                    print(f"     - {test['name']}")
        else:
            print("   â€¢ âœ… Tous les modÃ¨les fonctionnent correctement")
            print("   â€¢ âœ… Pipeline d'intÃ©gration validÃ©")
            print("   â€¢ âœ… PrÃªt pour dÃ©ploiement production")
    
    def run_all_integration_tests(self):
        """ExÃ©cute tous les tests d'intÃ©gration"""
        overall_start_time = time.time()
        
        print("ğŸš€ TESTS D'INTÃ‰GRATION - MODÃˆLES GPU RÃ‰ELS")
        print("Validation fonctionnement des modÃ¨les d'IA")
        print("=" * 60)
        
        # Liste des tests Ã  exÃ©cuter
        test_results = []
        
        # Test YOLO
        test_results.append(self.test_yolo_real_model())
        
        # Test SAM2
        test_results.append(self.test_sam2_real_model())
        
        # Test DINO v2
        test_results.append(self.test_dino_v2_real_model())
        
        # Test Kimi VLM
        test_results.append(self.test_kimi_vlm_real_model())
        
        # Test pipeline complet
        test_results.append(self.test_complete_pipeline())
        
        # Rapport final
        overall_time = time.time() - overall_start_time
        print(f"\nâ±ï¸ Temps total d'exÃ©cution: {overall_time:.2f} secondes")
        
        self.generate_integration_report(test_results)
        
        # Retourner rÃ©sultat global
        all_passed = all(test['passed'] for test in test_results)
        return all_passed

def main():
    """Point d'entrÃ©e principal"""
    print("ğŸš€ Lancement des Tests d'IntÃ©gration GPU...")
    
    runner = IntegrationTestRunner()
    
    try:
        success = runner.run_all_integration_tests()
        
        if success:
            print(f"\nâœ¨ Tous les tests d'intÃ©gration ont rÃ©ussi!")
            return 0
        else:
            print(f"\nğŸ’¥ Certains tests d'intÃ©gration ont Ã©chouÃ©")
            return 1
            
    except KeyboardInterrupt:
        print(f"\nâš ï¸ Tests interrompus par l'utilisateur")
        return 130
    except Exception as e:
        print(f"\nğŸ’¥ Erreur inattendue: {e}")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)