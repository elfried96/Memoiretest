# üöÄ Guide de D√©marrage Rapide - Multi-VLM

Guide pour commencer rapidement avec le syst√®me de surveillance **KIM + LLaVA + Qwen2-VL**.

## ‚ö° Installation Express

```bash
# 1. Installation des d√©pendances
uv sync
# ou
pip install -r requirements.txt

# 2. Test du syst√®me  
python tests/test_model_switching.py
```

## üéØ Utilisation Basique

### **1. VLM Simple (LLaVA)**

```python
from src.core.vlm.dynamic_model import DynamicVisionLanguageModel
from src.core.types import AnalysisRequest

# Initialisation avec LLaVA (stable)
vlm = DynamicVisionLanguageModel(
    default_model="llava-v1.6-mistral-7b",
    enable_fallback=True
)

# Chargement
await vlm.load_model()

# Analyse basique
request = AnalysisRequest(
    frame_data="<base64_image>",
    context={"location": "Store", "camera": "CAM_01"},
    tools_available=["dino_features", "pose_estimator"]
)

result = await vlm.analyze_with_tools(request)
print(f"Suspicion: {result.suspicion_level.value}")
```

### **2. Switch vers KIM (Recommand√©)**

```python
# Switch vers KIM pour surveillance optimis√©e
success = await vlm.switch_model("kim-7b-instruct")

if success:
    result = await vlm.analyze_with_tools(request)
    print(f"Analyse KIM: {result.description}")
else:
    print("KIM indisponible, LLaVA utilis√© en fallback")
```

### **3. Switch vers Qwen2-VL (Raisonnement)**

```python
# Switch vers Qwen2-VL pour raisonnement avanc√©
await vlm.switch_model("qwen2-vl-7b-instruct")

# Analyse avec plus d'outils pour raisonnement complexe
request.tools_available = [
    "sam2_segmentator", "dino_features", "pose_estimator",
    "trajectory_analyzer", "multimodal_fusion"
]

result = await vlm.analyze_with_tools(request)
```

## üéÆ Orchestrateur Moderne

### **Mode Rapide (FAST)**

```python
from src.core.orchestrator.vlm_orchestrator import (
    ModernVLMOrchestrator, OrchestrationConfig, OrchestrationMode
)

# Configuration rapide
config = OrchestrationConfig(
    mode=OrchestrationMode.FAST,  # 3 outils essentiels
    enable_advanced_tools=True
)

orchestrator = ModernVLMOrchestrator(
    vlm_model_name="llava-v1.6-mistral-7b",  # Stable et rapide
    config=config
)

# Analyse rapide (~1.2s)
result = await orchestrator.analyze_surveillance_frame(
    frame_data=image_base64,
    detections=yolo_detections,
    context={"location": "Aisle 1"}
)
```

### **Mode Complet (THOROUGH)**

```python
# Configuration compl√®te avec KIM
config = OrchestrationConfig(
    mode=OrchestrationMode.THOROUGH,  # 8 outils complets
    enable_advanced_tools=True
)

orchestrator = ModernVLMOrchestrator(
    vlm_model_name="kim-7b-instruct",  # KIM pour performance
    config=config
)

# Analyse compl√®te (~4.8s)
result = await orchestrator.analyze_surveillance_frame(
    frame_data=image_base64,
    detections=yolo_detections,
    context={"location": "High Security Zone"}
)
```

## üìä Comparaison des Mod√®les

### **Quand utiliser chaque mod√®le ?**

| Situation | Mod√®le Recommand√© | Raison |
|-----------|------------------|--------|
| üéØ **Surveillance standard** | `kim-7b-instruct` | Optimis√© pour surveillance |
| üõ°Ô∏è **Syst√®me stable/prod** | `llava-v1.6-mistral-7b` | Tr√®s stable, fallback fiable |
| üß† **Analyse complexe** | `qwen2-vl-7b-instruct` | Excellence en raisonnement |
| üöÄ **Performance max** | `kim-14b-instruct` | Version haute performance |
| üèÜ **Flagship** | `qwen2-vl-72b-instruct` | Performance ultime (GPU++) |

### **Test de tous les mod√®les**

```python
models = [
    "kim-7b-instruct",
    "llava-v1.6-mistral-7b", 
    "qwen2-vl-7b-instruct"
]

for model_id in models:
    print(f"Test {model_id}...")
    success = await vlm.switch_model(model_id)
    
    if success:
        result = await vlm.analyze_with_tools(request)
        print(f"  ‚úÖ Suspicion: {result.suspicion_level.value}")
    else:
        print(f"  ‚ùå Mod√®le indisponible")
```

## üß™ Tests Recommand√©s

### **1. Test Syst√®me Complet**
```bash
python tests/test_integration_complete.py
```
- Orchestration 3 modes
- 8 outils avanc√©s
- Batch processing

### **2. Test Multi-VLM**
```bash
python tests/test_model_switching.py
```
- Switching KIM/LLaVA/Qwen
- Comparaisons performance
- Fallbacks automatiques

### **3. Tests Individuels**
```bash
python tests/test_sam2_segmentator.py
python tests/test_dino_features.py
python tests/test_pose_estimation.py
# etc.
```

## ‚öôÔ∏è Configuration Recommand√©e

### **Production (Stabilit√©)**
```python
config = OrchestrationConfig(
    mode=OrchestrationMode.BALANCED,
    max_concurrent_tools=4,
    timeout_seconds=30,
    confidence_threshold=0.8,
    enable_advanced_tools=True
)

vlm = DynamicVisionLanguageModel(
    default_model="llava-v1.6-mistral-7b",  # Stable
    enable_fallback=True
)
```

### **Recherche (Performance)**
```python
config = OrchestrationConfig(
    mode=OrchestrationMode.THOROUGH,
    max_concurrent_tools=6,
    timeout_seconds=60,
    confidence_threshold=0.7,
    enable_advanced_tools=True
)

vlm = DynamicVisionLanguageModel(
    default_model="kim-7b-instruct",  # Principal
    enable_fallback=True
)
```

### **Edge/Mobile (√âconomique)**
```python
config = OrchestrationConfig(
    mode=OrchestrationMode.FAST,
    max_concurrent_tools=2,
    timeout_seconds=15,
    confidence_threshold=0.6,
    enable_advanced_tools=True
)

vlm = DynamicVisionLanguageModel(
    default_model="llava-v1.6-mistral-7b",
    enable_fallback=True
)
```

## üîß D√©pannage Rapide

### **KIM Indisponible**
```python
# Le syst√®me utilise automatiquement LLaVA en fallback
# KIM n√©cessite possiblement une version sp√©cialis√©e
print("KIM non disponible, utilisation de LLaVA")
```

### **GPU Insuffisant**
```python
# Pour Qwen2-VL-72B ou mod√®les volumineux
# Utiliser les versions 7B √† la place
await vlm.switch_model("qwen2-vl-7b-instruct")  # Au lieu de 72B
```

### **Erreurs D√©pendances**
```bash
# R√©installation compl√®te
uv sync
pip install torch torchvision transformers
```

## üéØ Prochaines √âtapes

1. **Explorez** les diff√©rents mod√®les avec vos donn√©es
2. **Testez** les 3 modes d'orchestration
3. **Optimisez** la configuration selon vos besoins
4. **Int√©grez** dans votre pipeline de production

## üí° Conseils Pro

- **Commencez** avec LLaVA pour la stabilit√©
- **Utilisez** KIM pour la surveillance optimis√©e  
- **Testez** Qwen2-VL pour des analyses complexes
- **Activez** les fallbacks automatiques
- **Surveillez** les performances avec `get_system_status()`

---

**üöÄ Votre syst√®me multi-VLM est pr√™t ! Commencez par un test simple avec LLaVA puis explorez KIM et Qwen2-VL !**