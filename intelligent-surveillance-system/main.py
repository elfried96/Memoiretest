#!/usr/bin/env python3
"""
üéØ MAIN.PY - Pipeline Complet de Surveillance Intelligente
==========================================================

WORKFLOW COMPLET :
1. Capture vid√©o ‚Üí 2. D√©tection YOLO ‚Üí 3. Tracking ‚Üí 4. VLM Kimi-VL ‚Üí 5. Orchestration ‚Üí 6. D√©cisions

Architecture :
- Traitement temps r√©el des flux vid√©o
- D√©tection/tracking des objets et personnes
- Analyse VLM avec Kimi-VL (principal) + fallbacks LLaVA/Qwen
- Orchestration intelligente des 8 outils avanc√©s
- Prise de d√©cisions automatis√©e
"""

import asyncio
import cv2
import time
import base64
import json
import argparse
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum
import logging
from io import BytesIO
import numpy as np

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Imports du syst√®me de surveillance
from src.core.vlm.dynamic_model import DynamicVisionLanguageModel
from src.core.orchestrator.vlm_orchestrator import (
    ModernVLMOrchestrator, 
    OrchestrationConfig, 
    OrchestrationMode
)
from src.core.types import AnalysisRequest, AnalysisResponse, Detection, BoundingBox
from src.detection.yolo_detector import YOLODetector
from src.detection.tracking.byte_tracker import BYTETracker


class AlertLevel(Enum):
    """Niveaux d'alerte du syst√®me."""
    NORMAL = "normal"
    ATTENTION = "attention"
    ALERTE = "alerte"
    CRITIQUE = "critique"


@dataclass
class SurveillanceFrame:
    """Frame de surveillance enrichi."""
    frame_id: int
    timestamp: float
    frame: np.ndarray
    detections: List[Detection]
    tracked_objects: List[Dict]
    vlm_analysis: Optional[AnalysisResponse] = None
    alert_level: AlertLevel = AlertLevel.NORMAL
    actions_taken: List[str] = None


class SurveillanceDecisionEngine:
    """Moteur de d√©cision pour les actions de surveillance."""
    
    def __init__(self):
        self.alert_history = []
        self.action_log = []
    
    def process_analysis(self, analysis: AnalysisResponse, frame_context: Dict) -> Dict[str, Any]:
        """Traite l'analyse VLM et prend des d√©cisions."""
        
        decisions = {
            "alert_level": AlertLevel.NORMAL,
            "actions": [],
            "notifications": [],
            "recording_required": False,
            "human_review": False
        }
        
        # === LOGIQUE DE D√âCISION ===
        
        # 1. Niveau de suspicion √©lev√©
        if analysis.suspicion_level.value in ["HIGH", "CRITICAL"]:
            decisions["alert_level"] = AlertLevel.ALERTE
            decisions["recording_required"] = True
            decisions["actions"].append("start_recording")
            decisions["notifications"].append(f"Suspicion {analysis.suspicion_level.value} d√©tect√©e")
        
        # 2. Confiance faible n√©cessite r√©vision humaine
        if analysis.confidence < 0.6:
            decisions["human_review"] = True
            decisions["actions"].append("request_human_review")
        
        # 3. Actions sp√©cifiques selon le type d'action d√©tect√©
        action_responses = {
            "SUSPICIOUS_ACTIVITY": [
                "increase_monitoring", 
                "alert_security", 
                "track_individual"
            ],
            "THEFT_ATTEMPT": [
                "immediate_alert", 
                "contact_security", 
                "start_recording",
                "track_suspect"
            ],
            "NORMAL_SHOPPING": [
                "continue_monitoring"
            ],
            "LOITERING": [
                "extended_observation",
                "gentle_staff_intervention"
            ]
        }
        
        if analysis.action_type.value in action_responses:
            decisions["actions"].extend(action_responses[analysis.action_type.value])
        
        # 4. Outils utilis√©s influencent les d√©cisions
        if "adversarial_detector" in analysis.tools_used:
            decisions["actions"].append("security_protocol_enhanced")
        
        if "trajectory_analyzer" in analysis.tools_used:
            decisions["actions"].append("movement_pattern_logged")
        
        # 5. Contexte temporel
        current_time = time.strftime("%H:%M")
        if current_time < "08:00" or current_time > "22:00":
            # Hors heures d'ouverture = plus strict
            if decisions["alert_level"] == AlertLevel.NORMAL:
                decisions["alert_level"] = AlertLevel.ATTENTION
            decisions["actions"].append("after_hours_protocol")
        
        return decisions


class IntelligentSurveillanceSystem:
    """Syst√®me de surveillance intelligente complet."""
    
    def __init__(
        self,
        video_source: str = 0,  # 0 pour webcam, ou chemin vers fichier
        vlm_model: str = "kimi-vl-a3b-thinking",
        orchestration_mode: OrchestrationMode = OrchestrationMode.BALANCED
    ):
        self.video_source = video_source
        self.vlm_model = vlm_model
        self.orchestration_mode = orchestration_mode
        
        # === COMPOSANTS PRINCIPAUX ===
        
        # 1. D√©tection et tracking
        self.yolo_detector = YOLODetector(
            model_path="yolov8n.pt",  # Mod√®le l√©ger pour d√©mo
            device="auto"
        )
        self.tracker = BYTETracker()
        
        # 2. VLM et orchestration
        self.vlm = DynamicVisionLanguageModel(
            default_model=vlm_model,
            enable_fallback=True
        )
        
        config = OrchestrationConfig(
            mode=orchestration_mode,
            enable_advanced_tools=True,
            max_concurrent_tools=4,
            confidence_threshold=0.7
        )
        
        self.orchestrator = ModernVLMOrchestrator(
            vlm_model_name=vlm_model,
            config=config
        )
        
        # 3. Moteur de d√©cision
        self.decision_engine = SurveillanceDecisionEngine()
        
        # === √âTAT DU SYST√àME ===
        self.frame_count = 0
        self.processing_stats = {
            "total_frames": 0,
            "detected_objects": 0,
            "vlm_analyses": 0,
            "alerts_triggered": 0,
            "average_fps": 0.0
        }
        
        logger.info(f"üéØ Syst√®me initialis√© - VLM: {vlm_model}, Mode: {orchestration_mode.value}")
    
    async def initialize(self):
        """Initialisation asynchrone des composants."""
        logger.info("üöÄ Initialisation du syst√®me...")
        
        # Chargement du VLM
        logger.info(f"‚è≥ Chargement {self.vlm_model}...")
        vlm_loaded = await self.vlm.load_model()
        
        if vlm_loaded:
            logger.info(f"‚úÖ {self.vlm_model} charg√© avec succ√®s")
        else:
            logger.warning("‚ö†Ô∏è VLM principal √©chec, fallback activ√©")
        
        # Test des composants
        logger.info("üîç Tests des composants...")
        
        # Test YOLO
        test_frame = np.zeros((480, 640, 3), dtype=np.uint8)
        test_detections = self.yolo_detector.detect(test_frame)
        logger.info(f"‚úÖ YOLO op√©rationnel - {len(test_detections)} d√©tections test")
        
        # Statut complet
        status = await self.orchestrator.health_check()
        logger.info(f"üìä Health check: {status}")
        
        logger.info("üéØ Syst√®me pr√™t pour surveillance!")
    
    def encode_frame_to_base64(self, frame: np.ndarray) -> str:
        """Encode un frame OpenCV en base64 pour le VLM."""
        # Conversion BGR (OpenCV) ‚Üí RGB (PIL)
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Encodage en JPEG puis base64
        _, buffer = cv2.imencode('.jpg', frame_rgb, [cv2.IMWRITE_JPEG_QUALITY, 85])
        frame_b64 = base64.b64encode(buffer).decode('utf-8')
        
        return frame_b64
    
    def create_detections_list(self, yolo_results) -> List[Detection]:
        """Convertit les r√©sultats YOLO en liste de Detection."""
        detections = []
        
        for result in yolo_results:
            if hasattr(result, 'boxes') and result.boxes is not None:
                for box in result.boxes:
                    # Extraction des coordonn√©es
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = box.conf[0].cpu().numpy()
                    cls = int(box.cls[0].cpu().numpy())
                    
                    # Nom de classe (COCO classes)
                    class_names = result.names
                    class_name = class_names.get(cls, f"class_{cls}")
                    
                    # Cr√©ation de la d√©tection
                    detection = Detection(
                        bbox=BoundingBox(x1=float(x1), y1=float(y1), x2=float(x2), y2=float(y2)),
                        confidence=float(conf),
                        class_name=class_name,
                        track_id=None  # Sera ajout√© par le tracker
                    )
                    detections.append(detection)
        
        return detections
    
    async def process_frame(self, frame: np.ndarray) -> SurveillanceFrame:
        """Traite un frame complet : d√©tection ‚Üí tracking ‚Üí VLM ‚Üí d√©cisions."""
        
        start_time = time.time()
        self.frame_count += 1
        
        # === √âTAPE 1: D√âTECTION YOLO ===
        logger.debug(f"üîç Frame {self.frame_count} - D√©tection YOLO...")
        yolo_results = self.yolo_detector.detect(frame)
        detections = self.create_detections_list(yolo_results)
        
        self.processing_stats["detected_objects"] += len(detections)
        
        # === √âTAPE 2: TRACKING ===
        logger.debug(f"üéØ Frame {self.frame_count} - Tracking...")
        # Note: Impl√©mentation simplifi√©e du tracking
        tracked_objects = []
        for i, detection in enumerate(detections):
            tracked_objects.append({
                "track_id": f"track_{i}_{self.frame_count}",
                "detection": detection,
                "age": 1,
                "status": "active"
            })
        
        # === √âTAPE 3: ANALYSE VLM (conditionnelle) ===
        vlm_analysis = None
        
        # Analyse VLM si personnes d√©tect√©es ou tous les N frames
        should_analyze = (
            len([d for d in detections if d.class_name == "person"]) > 0 or
            self.frame_count % 30 == 0  # Analyse p√©riodique
        )
        
        if should_analyze:
            logger.debug(f"üß† Frame {self.frame_count} - Analyse VLM...")
            
            # Encodage pour VLM
            frame_b64 = self.encode_frame_to_base64(frame)
            
            # Contexte enrichi
            context = {
                "frame_id": self.frame_count,
                "timestamp": time.time(),
                "location": "Store Main Area",
                "camera": "CAM_01",
                "person_count": len([d for d in detections if d.class_name == "person"]),
                "total_objects": len(detections),
                "time_of_day": time.strftime("%H:%M:%S")
            }
            
            # Analyse orchestr√©e
            vlm_analysis = await self.orchestrator.analyze_surveillance_frame(
                frame_data=frame_b64,
                detections=detections,
                context=context
            )
            
            self.processing_stats["vlm_analyses"] += 1
        
        # === √âTAPE 4: PRISE DE D√âCISIONS ===
        alert_level = AlertLevel.NORMAL
        actions_taken = []
        
        if vlm_analysis:
            decisions = self.decision_engine.process_analysis(vlm_analysis, {
                "frame_count": self.frame_count,
                "detections_count": len(detections)
            })
            
            alert_level = decisions["alert_level"]
            actions_taken = decisions["actions"]
            
            # Logging des d√©cisions importantes
            if alert_level != AlertLevel.NORMAL:
                logger.warning(f"‚ö†Ô∏è Frame {self.frame_count}: {alert_level.value} - Actions: {actions_taken}")
                self.processing_stats["alerts_triggered"] += 1
        
        # === CR√âATION DU FRAME DE SURVEILLANCE ===
        surveillance_frame = SurveillanceFrame(
            frame_id=self.frame_count,
            timestamp=time.time(),
            frame=frame,
            detections=detections,
            tracked_objects=tracked_objects,
            vlm_analysis=vlm_analysis,
            alert_level=alert_level,
            actions_taken=actions_taken
        )
        
        # === MISE √Ä JOUR DES STATISTIQUES ===
        processing_time = time.time() - start_time
        self.processing_stats["total_frames"] += 1
        
        # Calcul FPS moyen
        if self.processing_stats["total_frames"] > 0:
            self.processing_stats["average_fps"] = 1.0 / processing_time
        
        logger.debug(f"‚úÖ Frame {self.frame_count} trait√© en {processing_time:.2f}s")
        
        return surveillance_frame
    
    def draw_surveillance_overlay(self, frame: np.ndarray, surveillance_frame: SurveillanceFrame) -> np.ndarray:
        """Dessine les overlays de surveillance sur le frame."""
        overlay_frame = frame.copy()
        
        # === D√âTECTIONS ET TRACKING ===
        for detection in surveillance_frame.detections:
            bbox = detection.bbox
            
            # Couleur selon la classe
            color = (0, 255, 0) if detection.class_name == "person" else (255, 0, 0)
            
            # Rectangle de d√©tection
            cv2.rectangle(
                overlay_frame,
                (int(bbox.x1), int(bbox.y1)),
                (int(bbox.x2), int(bbox.y2)),
                color, 2
            )
            
            # Label avec confiance
            label = f"{detection.class_name}: {detection.confidence:.2f}"
            cv2.putText(
                overlay_frame, label,
                (int(bbox.x1), int(bbox.y1) - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2
            )
        
        # === INFORMATIONS SYST√àME ===
        info_y = 30
        
        # Frame ID et timestamp
        cv2.putText(overlay_frame, f"Frame: {surveillance_frame.frame_id}", 
                   (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        info_y += 25
        
        # Alert level
        alert_color = {
            AlertLevel.NORMAL: (0, 255, 0),
            AlertLevel.ATTENTION: (0, 255, 255),
            AlertLevel.ALERTE: (0, 165, 255),
            AlertLevel.CRITIQUE: (0, 0, 255)
        }
        
        cv2.putText(overlay_frame, f"Alert: {surveillance_frame.alert_level.value.upper()}", 
                   (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, 
                   alert_color[surveillance_frame.alert_level], 2)
        info_y += 25
        
        # Analyse VLM si disponible
        if surveillance_frame.vlm_analysis:
            analysis = surveillance_frame.vlm_analysis
            cv2.putText(overlay_frame, f"VLM: {analysis.suspicion_level.value} ({analysis.confidence:.2f})", 
                       (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            info_y += 20
            
            cv2.putText(overlay_frame, f"Action: {analysis.action_type.value}", 
                       (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # === STATISTIQUES ===
        stats_y = overlay_frame.shape[0] - 100
        cv2.putText(overlay_frame, f"FPS: {self.processing_stats['average_fps']:.1f}", 
                   (10, stats_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        stats_y += 20
        
        cv2.putText(overlay_frame, f"Detections: {len(surveillance_frame.detections)}", 
                   (10, stats_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        stats_y += 20
        
        cv2.putText(overlay_frame, f"Alerts: {self.processing_stats['alerts_triggered']}", 
                   (10, stats_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return overlay_frame
    
    async def run_surveillance(self, max_frames: int = None, display: bool = True):
        """Lance la surveillance en temps r√©el."""
        logger.info("üé¨ D√©marrage de la surveillance...")
        
        # Ouverture de la source vid√©o
        cap = cv2.VideoCapture(self.video_source)
        
        if not cap.isOpened():
            logger.error(f"‚ùå Impossible d'ouvrir la source vid√©o: {self.video_source}")
            return
        
        logger.info(f"üìπ Source vid√©o ouverte: {self.video_source}")
        
        try:
            frame_processed = 0
            
            while True:
                # Lecture du frame
                ret, frame = cap.read()
                if not ret:
                    logger.warning("üìπ Fin de vid√©o ou erreur lecture")
                    break
                
                # Traitement complet du frame
                surveillance_frame = await self.process_frame(frame)
                
                # Affichage si demand√©
                if display:
                    overlay_frame = self.draw_surveillance_overlay(frame, surveillance_frame)
                    cv2.imshow("üéØ Surveillance Intelligente - Multi-VLM", overlay_frame)
                    
                    # ESC pour quitter
                    if cv2.waitKey(1) & 0xFF == 27:
                        logger.info("üõë Arr√™t demand√© par utilisateur")
                        break
                
                frame_processed += 1
                
                # Limite de frames si sp√©cifi√©e
                if max_frames and frame_processed >= max_frames:
                    logger.info(f"üèÅ Limite de {max_frames} frames atteinte")
                    break
                
                # Logging p√©riodique
                if frame_processed % 60 == 0:
                    logger.info(f"üìä {frame_processed} frames trait√©s - "
                              f"FPS moyen: {self.processing_stats['average_fps']:.1f} - "
                              f"Alertes: {self.processing_stats['alerts_triggered']}")
        
        except KeyboardInterrupt:
            logger.info("üõë Arr√™t par interruption clavier")
        
        finally:
            # Nettoyage
            cap.release()
            if display:
                cv2.destroyAllWindows()
            
            # Statistiques finales
            logger.info("üìà Statistiques finales:")
            for key, value in self.processing_stats.items():
                logger.info(f"  ‚Ä¢ {key}: {value}")
    
    async def shutdown(self):
        """Arr√™t propre du syst√®me."""
        logger.info("üõë Arr√™t du syst√®me de surveillance...")
        await self.orchestrator.shutdown()
        await self.vlm.shutdown()
        logger.info("‚úÖ Syst√®me arr√™t√© proprement")


def parse_arguments():
    """Parse les arguments de la ligne de commande."""
    parser = argparse.ArgumentParser(
        description="üéØ Syst√®me de Surveillance Intelligente Multi-VLM",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python main.py                                    # Webcam (d√©faut)
  python main.py --video webcam                     # Webcam explicite
  python main.py --video /path/to/video.mp4         # Fichier vid√©o
  python main.py --video rtsp://camera_ip:554/stream # Flux RTSP
  python main.py --model llava-v1.6-mistral-7b      # Mod√®le diff√©rent
  python main.py --mode FAST                        # Mode rapide
  python main.py --mode THOROUGH                    # Mode complet
        """
    )
    
    parser.add_argument(
        "--video", "-v",
        default="webcam",
        help="Source vid√©o: 'webcam', chemin vers fichier, ou URL RTSP (d√©faut: webcam)"
    )
    
    parser.add_argument(
        "--model", "-m",
        default="kimi-vl-a3b-thinking",
        choices=["kimi-vl-a3b-thinking", "kimi-vl-a3b-instruct", "llava-v1.6-mistral-7b", "qwen2-vl-7b-instruct"],
        help="Mod√®le VLM principal (d√©faut: kimi-vl-a3b-thinking)"
    )
    
    parser.add_argument(
        "--mode",
        default="BALANCED",
        choices=["FAST", "BALANCED", "THOROUGH"],
        help="Mode d'orchestration (d√©faut: BALANCED)"
    )
    
    parser.add_argument(
        "--max-frames",
        type=int,
        default=None,
        help="Nombre maximum de frames √† traiter (d√©faut: infini)"
    )
    
    parser.add_argument(
        "--no-display",
        action="store_true",
        help="D√©sactiver l'affichage vid√©o (mode headless)"
    )
    
    return parser.parse_args()


def validate_video_source(video_arg: str) -> Any:
    """Valide et convertit la source vid√©o."""
    
    if video_arg.lower() == "webcam":
        logger.info("üìπ Source vid√©o: Webcam (device 0)")
        return 0
    
    # V√©rifier si c'est un fichier local
    video_path = Path(video_arg)
    if video_path.exists():
        if video_path.suffix.lower() in ['.mp4', '.avi', '.mov', '.mkv', '.webm']:
            logger.info(f"üìπ Source vid√©o: Fichier local {video_path}")
            return str(video_path)
        else:
            logger.warning(f"‚ö†Ô∏è Extension de fichier non recommand√©e: {video_path.suffix}")
            return str(video_path)
    
    # V√©rifier si c'est une URL (RTSP, HTTP, etc.)
    if video_arg.startswith(('rtsp://', 'http://', 'https://')):
        logger.info(f"üìπ Source vid√©o: Flux r√©seau {video_arg}")
        return video_arg
    
    # Sinon, consid√©rer comme chemin de fichier (m√™me si inexistant)
    logger.warning(f"‚ö†Ô∏è Fichier non trouv√©, tentative d'ouverture: {video_arg}")
    return video_arg


async def main():
    """Point d'entr√©e principal du syst√®me."""
    
    # Parsing des arguments
    args = parse_arguments()
    
    print(f"""
üéØ SYST√àME DE SURVEILLANCE INTELLIGENTE MULTI-VLM
==================================================

Configuration:
üìπ Source vid√©o  : {args.video}
ü§ñ Mod√®le VLM    : {args.model}
‚öôÔ∏è Mode          : {args.mode}
üñ•Ô∏è Affichage     : {'D√©sactiv√©' if args.no_display else 'Activ√©'}
üìä Max frames    : {args.max_frames or 'Illimit√©'}

WORKFLOW COMPLET:
1. üìπ Capture vid√©o (webcam/fichier)
2. üîç D√©tection YOLO (objets/personnes)  
3. üéØ Tracking multi-objets
4. üß† Analyse VLM {args.model} + 8 outils avanc√©s
5. ‚öôÔ∏è Orchestration intelligente
6. üö® Prise de d√©cisions automatis√©e
7. üìä Affichage temps r√©el + alertes

Appuyez sur ESC pour quitter (si affichage activ√©)
""")
    
    # Validation de la source vid√©o
    try:
        video_source = validate_video_source(args.video)
    except Exception as e:
        logger.error(f"‚ùå Erreur validation source vid√©o: {e}")
        sys.exit(1)
    
    # Configuration du mode d'orchestration
    mode_mapping = {
        "FAST": OrchestrationMode.FAST,
        "BALANCED": OrchestrationMode.BALANCED,
        "THOROUGH": OrchestrationMode.THOROUGH
    }
    orchestration_mode = mode_mapping[args.mode]
    
    # Initialisation du syst√®me
    system = IntelligentSurveillanceSystem(
        video_source=video_source,
        vlm_model=args.model,
        orchestration_mode=orchestration_mode
    )
    
    try:
        # Initialisation asynchrone
        await system.initialize()
        
        # D√©marrage de la surveillance
        await system.run_surveillance(
            max_frames=args.max_frames,
            display=not args.no_display
        )
    
    except Exception as e:
        logger.error(f"‚ùå Erreur syst√®me: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Arr√™t propre
        await system.shutdown()


if __name__ == "__main__":
    asyncio.run(main())