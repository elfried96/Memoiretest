#!/usr/bin/env python3
"""
Script de libÃ©ration mÃ©moire GPU pour Kimi-VL
===========================================

RÃ©sout le problÃ¨me "parameters offloaded to cpu"
"""

import torch
import gc
import subprocess
import sys
import os

def check_gpu_memory():
    """VÃ©rifier Ã©tat mÃ©moire GPU."""
    if torch.cuda.is_available():
        print(f"ğŸ” GPU disponible: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ MÃ©moire totale: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        print(f"ğŸ’¾ MÃ©moire allouÃ©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB")
        print(f"ğŸ’¾ MÃ©moire rÃ©servÃ©e: {torch.cuda.memory_reserved() / 1e9:.1f} GB")
        print(f"ğŸ’¾ MÃ©moire libre: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1e9:.1f} GB")
    else:
        print("âŒ Aucun GPU disponible")

def clear_gpu_cache():
    """Nettoyer cache GPU."""
    print("\nğŸ§¹ Nettoyage cache GPU...")
    
    if torch.cuda.is_available():
        # Vider cache PyTorch
        torch.cuda.empty_cache()
        
        # Garbage collection Python
        gc.collect()
        
        print("âœ… Cache GPU vidÃ©")
    else:
        print("âŒ Pas de GPU Ã  nettoyer")

def kill_gpu_processes():
    """Tuer processus GPU qui consomment."""
    print("\nğŸ”§ VÃ©rification processus GPU...")
    
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("ğŸ“Š Processus GPU actifs:")
            print(result.stdout)
            
            # Optionnel: tuer processus Python gourmands
            try:
                subprocess.run(['pkill', '-f', 'python.*kimi'], check=False)
                print("âœ… Processus Kimi-VL terminÃ©s")
            except:
                pass
        else:
            print("âŒ nvidia-smi non disponible")
    except FileNotFoundError:
        print("âŒ nvidia-smi non trouvÃ©")

def optimize_environment_variables():
    """Optimiser variables environnement pour GPU."""
    print("\nâš™ï¸ Optimisation variables environnement...")
    
    # Variables pour Ã©conomiser mÃ©moire
    env_vars = {
        "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:512",  # Limite fragmentation
        "CUDA_LAUNCH_BLOCKING": "0",  # Async pour vitesse
        "TORCH_USE_CUDA_DSA": "1",    # Debug allocation
        "TOKENIZERS_PARALLELISM": "false",  # Ã‰conomie mÃ©moire
        "OMP_NUM_THREADS": "4",       # Limite threads CPU
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"âœ… {key}={value}")

def suggest_optimizations():
    """Suggestions d'optimisation."""
    print("\nğŸ’¡ SUGGESTIONS OPTIMISATION:")
    print("1. ğŸš€ Utiliser modÃ¨le plus petit:")
    print("   python main_Qwen.py (7B au lieu de 27B)")
    
    print("\n2. âš¡ RÃ©duire paramÃ¨tres:")
    print("   --max-frames 3 --frame-skip 5")
    
    print("\n3. ğŸ’¾ Mode Ã©conomique:")
    print("   --vlm-mode smart --summary-interval 60")
    
    print("\n4. ğŸ”§ Si problÃ¨me persiste:")
    print("   RedÃ©marrer: sudo reboot")

def main():
    """Diagnostic complet et nettoyage."""
    print("ğŸ” DIAGNOSTIC MÃ‰MOIRE GPU KIMI-VL")
    print("=" * 40)
    
    # Ã‰tape 1: VÃ©rifier Ã©tat
    check_gpu_memory()
    
    # Ã‰tape 2: Nettoyer
    clear_gpu_cache()
    
    # Ã‰tape 3: Tuer processus
    kill_gpu_processes()
    
    # Ã‰tape 4: Optimiser environnement
    optimize_environment_variables()
    
    # Ã‰tape 5: VÃ©rifier aprÃ¨s nettoyage
    print("\nğŸ“Š Ã‰TAT APRÃˆS NETTOYAGE:")
    check_gpu_memory()
    
    # Ã‰tape 6: Suggestions
    suggest_optimizations()
    
    print("\nğŸ¯ COMMANDES RECOMMANDÃ‰ES:")
    print("# Test rapide Kimi-VL optimisÃ©")
    print("python main_headless.py --model kimi-vl-a3b-thinking-2506 --video videos/surveillance01.mp4 --max-frames 3 --frame-skip 5")
    
    print("\n# Alternative stable Qwen (plus lÃ©ger)")  
    print("python main_Qwen.py --video videos/surveillance01.mp4 --max-frames 5 --vlm-mode smart")

if __name__ == "__main__":
    main()