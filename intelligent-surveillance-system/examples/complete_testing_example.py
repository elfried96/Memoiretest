#!/usr/bin/env python3
"""
Exemple complet d'utilisation du framework de test avanc√©.
D√©montre l'utilisation de tous les outils et configurations disponibles.
"""

import asyncio
import time
import numpy as np
from pathlib import Path
from typing import List, Dict, Any

# Import des modules de test
from src.testing import (
    ABTestFramework, 
    BenchmarkSuite, 
    PerformanceCollector, 
    MetricsVisualizer,
    TestVariant,
    BenchmarkConfig
)

# Import des outils avanc√©s pour configuration
from src.advanced_tools import *
from src.core.orchestrator.vlm_orchestrator import VLMOrchestrator

class CompleteTester:
    """D√©monstrateur complet du framework de test."""
    
    def __init__(self, output_dir: str = "test_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # Initialisation des composants
        self.ab_framework = ABTestFramework(str(self.output_dir / "ab_tests"))
        self.benchmark_suite = BenchmarkSuite(str(self.output_dir / "benchmarks"))
        self.visualizer = MetricsVisualizer(str(self.output_dir / "visualizations"))
        self.performance_collector = PerformanceCollector()
        
        # Configuration
        self.test_datasets = self._setup_test_datasets()
        
    def _setup_test_datasets(self) -> Dict[str, List[str]]:
        """Configure les datasets de test."""
        datasets = {
            "small": self._create_mock_dataset(50),
            "medium": self._create_mock_dataset(200), 
            "large": self._create_mock_dataset(500),
            "stress": self._create_mock_dataset(1000)
        }
        return datasets
    
    def _create_mock_dataset(self, size: int) -> List[str]:
        """Cr√©e un dataset mock pour les tests."""
        return [f"mock_frame_{i:04d}.jpg" for i in range(size)]
    
    async def demonstrate_basic_ab_test(self):
        """D√©montre un test A/B basique."""
        print("üß™ === TEST A/B BASIQUE ===")
        
        # Configuration des variants par d√©faut
        self.ab_framework.setup_default_variants()
        
        # Lancement du test
        print("Lancement du test A/B avec dataset moyen...")
        
        ab_results = self.ab_framework.run_ab_test(
            test_dataset=self.test_datasets["medium"],
            test_duration=60,  # 1 minute pour la d√©mo
            concurrent_variants=2,
            metrics_to_collect=[
                "execution_time", "memory_usage", "accuracy",
                "false_positive_rate", "throughput", "latency"
            ]
        )
        
        # Affichage des r√©sultats
        print(f"‚úÖ Test termin√©. Test ID: {ab_results['test_id']}")
        print(f"üèÜ Meilleur variant: {ab_results['winner']}")
        
        # G√©n√©ration des visualisations
        print("üìä G√©n√©ration des visualisations...")
        self.ab_framework.visualize_results(ab_results['variant_results'])
        
        # Rapport d√©taill√©
        report = self.ab_framework.generate_report(
            ab_results['variant_results'],
            ab_results['comparison_report']
        )
        
        # Sauvegarde du rapport
        report_file = self.output_dir / "ab_test_report.md"
        report_file.write_text(report)
        print(f"üìÑ Rapport sauvegard√©: {report_file}")
        
        return ab_results
    
    async def demonstrate_advanced_variants(self):
        """D√©montre l'utilisation des variants avanc√©s."""
        print("\nüöÄ === VARIANTS AVANC√âS ===")
        
        # Configuration des variants avanc√©s
        self.benchmark_suite.setup_test_variants()
        
        # Cr√©er un variant personnalis√©
        custom_variant = TestVariant(
            name="custom_optimized",
            pipeline_type="custom",
            components=[
                "yolo_detector", "sam2_segmentation", "dino_features", 
                "pose_estimation", "multimodal_fusion", "vlm_analyzer"
            ],
            parameters={
                "sam2_confidence": 0.85,
                "dino_attention": True,
                "pose_threshold": 0.6,
                "fusion_method": "attention",
                "vlm_temperature": 0.5
            },
            enabled_tools=[
                "object_detector", "segmentation_tool", "feature_extractor",
                "pose_analyzer", "behavior_classifier"
            ],
            description="Variant personnalis√© optimis√© pour pr√©cision maximale"
        )
        
        # Enregistrer le variant personnalis√©
        self.ab_framework.register_variant(custom_variant)
        
        print("‚úÖ Variants avanc√©s configur√©s")
        print(f"Nombre total de variants: {len(self.ab_framework.test_variants)}")
        
        # Test avec variants avanc√©s
        print("Lancement du test avec variants avanc√©s...")
        
        ab_results = self.ab_framework.run_ab_test(
            test_dataset=self.test_datasets["small"],  # Dataset plus petit pour d√©mo
            test_duration=30,  # Test rapide
            concurrent_variants=3,
            metrics_to_collect=[
                "execution_time", "memory_usage", "accuracy", 
                "false_positive_rate", "throughput"
            ]
        )
        
        print(f"üèÜ Meilleur variant avanc√©: {ab_results['winner']}")
        
        return ab_results
    
    async def demonstrate_performance_monitoring(self):
        """D√©montre le monitoring de performance en temps r√©el."""
        print("\nüìà === MONITORING DE PERFORMANCE ===")
        
        # D√©marrer la collecte de m√©triques
        self.performance_collector.start_collection()
        
        # Simuler une charge de travail
        print("Simulation d'une charge de travail...")
        
        def simulate_workload_callback(snapshot):
            # Simuler des m√©triques personnalis√©es
            self.performance_collector.record_custom_metric(
                "custom_accuracy", np.random.uniform(0.8, 0.95)
            )
            self.performance_collector.record_custom_metric(
                "queue_length", np.random.randint(0, 20)
            )
        
        self.performance_collector.add_callback(simulate_workload_callback)
        
        # Simuler du traitement
        for i in range(50):
            self.performance_collector.record_frame_processed()
            self.performance_collector.record_latency(np.random.uniform(50, 200))
            
            if np.random.random() < 0.05:  # 5% d'erreurs
                self.performance_collector.record_error()
            
            await asyncio.sleep(0.1)  # 100ms par frame
        
        # Arr√™ter la collecte et obtenir le rapport
        performance_report = self.performance_collector.stop_collection()
        
        print("üìä Rapport de performance:")
        print(f"- Frames trait√©es: {performance_report.total_frames}")
        print(f"- FPS moyen: {performance_report.average_fps:.2f}")
        print(f"- Latence moyenne: {performance_report.average_latency:.2f}ms")
        print(f"- Pic m√©moire: {performance_report.peak_memory_mb:.1f}MB")
        print(f"- Score d'efficacit√©: {performance_report.resource_efficiency:.1f}/100")
        
        # G√©n√©ration des visualisations de performance
        self.visualizer.plot_time_series(self.performance_collector)
        
        return performance_report
    
    async def demonstrate_comprehensive_benchmark(self):
        """D√©montre l'utilisation du benchmark complet."""
        print("\nüèãÔ∏è === BENCHMARK COMPLET ===")
        
        # Configuration d'un benchmark personnalis√©
        benchmark_config = BenchmarkConfig(
            test_name="demo_benchmark",
            dataset_path=str(self.output_dir / "demo_data"),
            test_duration=45,  # 45 secondes pour la d√©mo
            concurrent_streams=2,
            repeat_count=2,  # R√©duit pour la d√©mo
            hardware_profiling=True,
            memory_profiling=True
        )
        
        self.benchmark_suite.add_benchmark_config(benchmark_config)
        
        # Lancement du benchmark
        print("Lancement du benchmark complet...")
        
        benchmark_result = self.benchmark_suite.run_comprehensive_benchmark(
            benchmark_name="demo_benchmark"
        )
        
        print("üìä R√©sultats du benchmark:")
        print(f"- Temps d'ex√©cution: {benchmark_result.execution_time:.2f}s")
        print(f"- Variants test√©s: {len(benchmark_result.variant_results)}")
        print(f"- Meilleur variant: {benchmark_result.comparison_report.get('best_overall_variant')}")
        
        # Affichage des m√©triques de performance
        print("\nüìà M√©triques de performance:")
        for metric, value in benchmark_result.performance_summary.items():
            if isinstance(value, float):
                print(f"- {metric}: {value:.4f}")
            else:
                print(f"- {metric}: {value}")
        
        # Utilisation des ressources
        print("\nüíª Utilisation des ressources:")
        for resource, stats in benchmark_result.hardware_utilization.items():
            print(f"- {resource.upper()}:")
            for stat_name, stat_value in stats.items():
                if isinstance(stat_value, float):
                    print(f"  - {stat_name}: {stat_value:.2f}")
        
        return benchmark_result
    
    async def demonstrate_custom_tools_integration(self):
        """D√©montre l'int√©gration des outils personnalis√©s."""
        print("\nüõ†Ô∏è === INT√âGRATION OUTILS PERSONNALIS√âS ===")
        
        # Initialiser les outils avanc√©s
        sam2_segmentator = SAM2Segmentator()
        dino_extractor = DinoV2FeatureExtractor()
        pose_estimator = OpenPoseEstimator()
        trajectory_analyzer = TrajectoryAnalyzer()
        
        print("‚úÖ Outils avanc√©s initialis√©s")
        
        # Tester chaque outil individuellement
        print("üîç Test des outils individuels...")
        
        # Mock frame pour test
        mock_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        mock_bboxes = [[100, 100, 200, 200], [300, 150, 400, 250]]
        
        # Test SAM2
        try:
            seg_result = sam2_segmentator.segment_objects(mock_frame, mock_bboxes)
            print(f"‚úÖ SAM2: {len(seg_result.masks)} masques g√©n√©r√©s en {seg_result.processing_time:.3f}s")
        except Exception as e:
            print(f"‚ö†Ô∏è SAM2: {e}")
        
        # Test DINO
        try:
            dino_features = dino_extractor.extract_features(mock_frame)
            print(f"‚úÖ DINO: Features extraites ({dino_features.features.shape}) en {dino_features.processing_time:.3f}s")
        except Exception as e:
            print(f"‚ö†Ô∏è DINO: {e}")
        
        # Test Pose Estimation
        try:
            pose_result = pose_estimator.estimate_poses(mock_frame, [(100, 100, 200, 300)])
            print(f"‚úÖ Pose: {pose_result.keypoints.shape[0]} personnes d√©tect√©es en {pose_result.processing_time:.3f}s")
        except Exception as e:
            print(f"‚ö†Ô∏è Pose: {e}")
        
        # Test Trajectory Analysis
        try:
            # Simuler des donn√©es de trajectoire
            trajectory_data = [
                {"person_id": "p1", "x": 100 + i*5, "y": 200 + i*3, "timestamp": time.time() + i}
                for i in range(10)
            ]
            motion_result = trajectory_analyzer.analyze_motion(trajectory_data)
            print(f"‚úÖ Trajectory: Pattern '{motion_result['pattern']}' avec anomaly_score {motion_result['anomaly_score']:.3f}")
        except Exception as e:
            print(f"‚ö†Ô∏è Trajectory: {e}")
    
    async def demonstrate_vlm_orchestration(self):
        """D√©montre l'orchestration VLM avec tool-calling."""
        print("\nüß† === ORCHESTRATION VLM ===")
        
        try:
            # Initialiser l'orchestrateur VLM
            vlm_orchestrator = VLMOrchestrator()
            
            print("‚úÖ VLM Orchestrator initialis√©")
            print(f"Outils disponibles: {list(vlm_orchestrator.tool_registry.get_available_tools().keys())}")
            
            # Cr√©er un contexte de test
            from src.core.types import Detection
            from src.core.orchestrator.vlm_orchestrator import VLMContext
            
            # Mock detections
            mock_detections = [
                # Simuler des d√©tections
            ]
            
            context = VLMContext(
                frame_id="test_001",
                timestamp=time.time(),
                detections=mock_detections,
                previous_context=None,
                scene_metadata={"camera_id": "cam_01", "location": "entrance"}
            )
            
            # Mock frame
            mock_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            
            # Analyse de sc√®ne avec tool-calling
            print("üîç Analyse de sc√®ne avec tool-calling...")
            
            analysis_result = await vlm_orchestrator.analyze_scene(mock_frame, context)
            
            print("üìä R√©sultat de l'analyse:")
            print(f"- Niveau de suspicion: {analysis_result.suspicion_level:.3f}")
            print(f"- Confiance: {analysis_result.confidence:.3f}")
            print(f"- Temps de traitement: {analysis_result.processing_time:.3f}s")
            print(f"- Outils utilis√©s: {len(analysis_result.tool_results)}")
            print(f"- Raisonnement: {analysis_result.reasoning[:100]}...")
            
            # Statistiques de performance VLM
            stats = vlm_orchestrator.get_performance_stats()
            print(f"\nüìà Stats VLM:")
            print(f"- Appels totaux: {stats['total_calls']}")
            print(f"- Taux de succ√®s: {stats['success_rate']:.2%}")
            print(f"- Temps de r√©ponse moyen: {stats['average_response_time']:.3f}s")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur VLM: {e}")
    
    async def generate_final_report(self, results: Dict[str, Any]):
        """G√©n√®re un rapport final complet."""
        print("\nüìã === G√âN√âRATION DU RAPPORT FINAL ===")
        
        # Rapport du benchmark suite
        summary_report = self.benchmark_suite.generate_summary_report()
        
        # Cr√©er un rapport personnalis√©
        report_lines = [
            "# Rapport de Test Complet - Syst√®me de Surveillance Intelligent",
            f"G√©n√©r√© le: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## R√©sum√© Ex√©cutif",
            ""
        ]
        
        # Ajouter les r√©sultats des diff√©rents tests
        for test_name, test_results in results.items():
            report_lines.extend([
                f"### {test_name}",
                ""
            ])
            
            if hasattr(test_results, '__dict__'):
                for key, value in test_results.__dict__.items():
                    if isinstance(value, (int, float)):
                        report_lines.append(f"- {key}: {value:.4f}")
                    elif isinstance(value, str) and len(value) < 100:
                        report_lines.append(f"- {key}: {value}")
            
            report_lines.append("")
        
        # Ajouter le rapport du benchmark
        report_lines.extend([
            "## D√©tails du Benchmark",
            "",
            summary_report
        ])
        
        # Recommandations
        report_lines.extend([
            "## Recommandations",
            "",
            "### Configuration Optimale",
            "Bas√© sur les r√©sultats des tests, voici nos recommandations:",
            "",
            "1. **Pour performance temps r√©el**: Utiliser `parallel_fusion` ou `baseline_yolo`",
            "2. **Pour pr√©cision maximale**: Utiliser `complete_advanced` ou `sam2_enhanced`", 
            "3. **Pour ressources limit√©es**: Utiliser `baseline_yolo`",
            "4. **Pour environnements variables**: Utiliser `domain_adaptive`",
            "",
            "### Optimisations Sugg√©r√©es",
            "- Monitoring continu des performances en production",
            "- Tests A/B r√©guliers avec nouvelles donn√©es",
            "- Ajustement des seuils selon l'environnement",
            "- Formation continue des mod√®les avec nouveaux cas",
            ""
        ])
        
        # Sauvegarde du rapport final
        final_report = "\n".join(report_lines)
        report_file = self.output_dir / "rapport_final_complet.md"
        report_file.write_text(final_report)
        
        print(f"üìÑ Rapport final sauvegard√©: {report_file}")
        
        # Export CSV des donn√©es
        csv_file = self.benchmark_suite.export_results_csv()
        print(f"üìä Donn√©es export√©es en CSV: {csv_file}")
        
        return final_report

async def main():
    """Fonction principale de d√©monstration."""
    print("üéØ D√âMONSTRATION COMPL√àTE DU FRAMEWORK DE TEST AVANC√â")
    print("=" * 60)
    
    # Initialisation
    tester = CompleteTester("demo_results")
    results = {}
    
    try:
        # 1. Test A/B basique
        ab_results = await tester.demonstrate_basic_ab_test()
        results["ab_test_basique"] = ab_results
        
        # 2. Variants avanc√©s
        advanced_results = await tester.demonstrate_advanced_variants()
        results["variants_avanc√©s"] = advanced_results
        
        # 3. Monitoring de performance
        perf_results = await tester.demonstrate_performance_monitoring()
        results["monitoring_performance"] = perf_results
        
        # 4. Int√©gration outils personnalis√©s
        await tester.demonstrate_custom_tools_integration()
        
        # 5. Orchestration VLM
        await tester.demonstrate_vlm_orchestration()
        
        # 6. Benchmark complet (optionnel)
        run_full_benchmark = input("\n‚ùì Ex√©cuter le benchmark complet ? (y/N): ").lower() == 'y'
        
        if run_full_benchmark:
            benchmark_results = await tester.demonstrate_comprehensive_benchmark()
            results["benchmark_complet"] = benchmark_results
        
        # 7. G√©n√©ration du rapport final
        final_report = await tester.generate_final_report(results)
        
        print("\nüéâ D√âMONSTRATION TERMIN√âE AVEC SUCC√àS!")
        print(f"üìÅ Tous les r√©sultats sont disponibles dans: {tester.output_dir}")
        
        # Affichage du r√©sum√© final
        print("\nüìä R√âSUM√â DES TESTS:")
        print(f"- Tests A/B ex√©cut√©s: {len([k for k in results.keys() if 'ab_test' in k or 'variants' in k])}")
        print(f"- Monitoring de performance: ‚úÖ")
        print(f"- Outils avanc√©s test√©s: ‚úÖ")
        print(f"- Orchestration VLM: ‚úÖ")
        if run_full_benchmark:
            print(f"- Benchmark complet: ‚úÖ")
        print(f"- Rapport final g√©n√©r√©: ‚úÖ")
        
    except Exception as e:
        print(f"‚ùå Erreur pendant la d√©monstration: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        print(f"\nüìÅ Consultez {tester.output_dir} pour tous les fichiers g√©n√©r√©s")

if __name__ == "__main__":
    # Ex√©cuter la d√©monstration
    asyncio.run(main())