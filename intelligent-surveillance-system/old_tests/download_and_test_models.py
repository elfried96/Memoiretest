#!/usr/bin/env python3
"""
Script de t√©l√©chargement et test des vrais mod√®les VLM.
ATTENTION: Ce script t√©l√©charge de gros mod√®les (plusieurs GB).
"""

import asyncio
import sys
import os
import subprocess
import logging
from pathlib import Path

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def install_dependencies():
    """Installe les d√©pendances n√©cessaires."""
    logger.info("üîß Installation des d√©pendances...")
    
    dependencies = [
        "torch>=2.1.0",
        "torchvision>=0.16.0", 
        "transformers>=4.35.0",
        "accelerate>=0.21.0",
        "opencv-python>=4.8.0",
        "pillow>=10.0.0",
        "numpy>=1.24.0",
        "huggingface-hub>=0.17.0",
        "safetensors>=0.4.0"
    ]
    
    for dep in dependencies:
        try:
            logger.info(f"Installation de {dep}...")
            subprocess.run([sys.executable, "-m", "pip", "install", dep], check=True, capture_output=True)
            logger.info(f"‚úÖ {dep} install√©")
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå Erreur installation {dep}: {e}")
            return False
    
    return True


def check_gpu_availability():
    """V√©rifie la disponibilit√© du GPU."""
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"‚úÖ GPU disponible: {gpu_name} ({gpu_memory:.1f} GB)")
            return True, gpu_memory
        else:
            logger.warning("‚ö†Ô∏è Aucun GPU CUDA d√©tect√© - utilisation CPU")
            return False, 0
    except ImportError:
        logger.error("‚ùå PyTorch non install√©")
        return False, 0


def download_model(model_name: str, model_path: str) -> bool:
    """T√©l√©charge un mod√®le sp√©cifique."""
    logger.info(f"üì• T√©l√©chargement de {model_name}...")
    
    try:
        from transformers import AutoTokenizer, AutoProcessor
        
        # Tentative de t√©l√©chargement du tokenizer/processor
        if "llava" in model_name.lower():
            processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
            logger.info(f"‚úÖ Processor LLaVA t√©l√©charg√©: {model_name}")
        elif "qwen" in model_name.lower():
            processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
            logger.info(f"‚úÖ Processor Qwen2-VL t√©l√©charg√©: {model_name}")
        elif "kimi" in model_name.lower():
            # Kimi-VL n√©cessite des credentials Moonshot AI
            logger.warning(f"‚ö†Ô∏è Kimi-VL n√©cessite une API key Moonshot AI")
            return False
        else:
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            logger.info(f"‚úÖ Tokenizer t√©l√©charg√©: {model_name}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur t√©l√©chargement {model_name}: {e}")
        return False


def test_model_loading(model_name: str, model_path: str) -> bool:
    """Test le chargement d'un mod√®le."""
    logger.info(f"üß™ Test de chargement {model_name}...")
    
    try:
        import torch
        from transformers import AutoProcessor, AutoModelForVision2Seq, AutoModelForCausalLM
        
        # Configuration selon le mod√®le
        device = "cuda" if torch.cuda.is_available() else "cpu"
        torch_dtype = torch.float16 if device == "cuda" else torch.float32
        
        logger.info(f"Device: {device}, dtype: {torch_dtype}")
        
        if "llava" in model_name.lower():
            processor = AutoProcessor.from_pretrained(model_path)
            model = AutoModelForVision2Seq.from_pretrained(
                model_path, 
                torch_dtype=torch_dtype,
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True
            )
            
        elif "qwen" in model_name.lower():
            processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
            model = AutoModelForVision2Seq.from_pretrained(
                model_path,
                torch_dtype=torch_dtype,
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True
            )
            
        else:
            logger.error(f"Type de mod√®le non support√©: {model_name}")
            return False
        
        # Test basique
        if device != "auto":
            model = model.to(device)
        model.eval()
        
        logger.info(f"‚úÖ Mod√®le {model_name} charg√© avec succ√®s")
        
        # Nettoyage m√©moire
        del model
        del processor
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur test {model_name}: {e}")
        return False


async def test_surveillance_analysis(model_name: str, model_path: str) -> bool:
    """Test d'analyse de surveillance avec un vrai mod√®le."""
    logger.info(f"üéØ Test analyse surveillance avec {model_name}...")
    
    try:
        import torch
        import base64
        from PIL import Image
        from io import BytesIO
        from transformers import AutoProcessor, AutoModelForVision2Seq
        
        # Chargement du mod√®le
        processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForVision2Seq.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        if not hasattr(model, 'device_map'):
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model = model.to(device)
        
        model.eval()
        
        # Cr√©er une image de test
        test_image = Image.new('RGB', (640, 480), color='white')
        # Ajouter du texte simulant une sc√®ne de magasin
        from PIL import ImageDraw
        draw = ImageDraw.Draw(test_image)
        draw.rectangle([100, 100, 200, 400], fill='blue', outline='black')
        draw.text((110, 120), "PERSON", fill='white')
        
        # Prompt de surveillance
        surveillance_prompt = """
        Analyse cette image de surveillance de magasin. 
        
        D√©termine:
        1. Le niveau de suspicion (LOW/MEDIUM/HIGH/CRITICAL)
        2. Le type d'action (NORMAL_SHOPPING/SUSPICIOUS_MOVEMENT/POTENTIAL_THEFT)
        3. Ta confiance dans l'analyse (0.0 √† 1.0)
        4. Une description br√®ve
        5. Des recommandations d'action
        
        Contexte: Magasin, camera de surveillance, d√©tection de personne pr√©sente.
        """
        
        # Pr√©paration des inputs
        inputs = processor(
            text=surveillance_prompt,
            images=test_image,
            return_tensors="pt",
            padding=True
        )
        
        if hasattr(model, 'device_map'):
            # Le mod√®le g√®re automatiquement les devices
            pass
        else:
            device = next(model.parameters()).device
            inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}
        
        # G√©n√©ration
        logger.info("G√©n√©ration en cours...")
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True
            )
        
        # D√©codage
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        # Extraction de la r√©ponse
        if surveillance_prompt in generated_text:
            response = generated_text.split(surveillance_prompt)[-1].strip()
        else:
            response = generated_text.strip()
        
        logger.info(f"üìä R√©ponse du mod√®le {model_name}:")
        logger.info(f"{'='*50}")
        logger.info(response)
        logger.info(f"{'='*50}")
        
        # Nettoyage
        del model
        del processor
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info(f"‚úÖ Test surveillance {model_name} r√©ussi")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur test surveillance {model_name}: {e}")
        return False


async def main():
    """Fonction principale."""
    print("""
üéØ T√âL√âCHARGEMENT ET TEST DES MOD√àLES VLM R√âELS
==============================================

‚ö†Ô∏è  ATTENTION: Ce script t√©l√©charge de gros mod√®les (2-7 GB chacun)
‚ö†Ô∏è  Assurez-vous d'avoir suffisamment d'espace disque et de bande passante

Mod√®les support√©s:
- LLaVA-NeXT (llava-hf/llava-v1.6-mistral-7b-hf) ~7GB
- Qwen2-VL (Qwen/Qwen2-VL-7B-Instruct) ~7GB  
- Kimi-VL (moonshot-ai/kimi-vl-a3b-instruct) - N√©cessite API key

""")
    
    # V√©rifications pr√©liminaires
    logger.info("üîç V√©rifications syst√®me...")
    
    # 1. Installation des d√©pendances
    if not install_dependencies():
        logger.error("‚ùå √âchec installation d√©pendances")
        return
    
    # 2. V√©rification GPU
    has_gpu, gpu_memory = check_gpu_availability()
    if has_gpu and gpu_memory < 8:
        logger.warning(f"‚ö†Ô∏è GPU avec {gpu_memory:.1f}GB peut √™tre insuffisant pour les mod√®les 7B")
    
    # 3. Espace disque
    free_space = os.statvfs('.').f_frsize * os.statvfs('.').f_availls // (1024**3)
    logger.info(f"üíæ Espace disque disponible: {free_space} GB")
    if free_space < 20:
        logger.warning("‚ö†Ô∏è Espace disque faible - au moins 20GB recommand√©s")
    
    # Mod√®les √† tester
    models_to_test = {
        "LLaVA-NeXT-7B": "llava-hf/llava-v1.6-mistral-7b-hf",
        "Qwen2-VL-7B": "Qwen/Qwen2-VL-7B-Instruct"
    }
    
    # Demander confirmation
    response = input("\nüöÄ Continuer avec le t√©l√©chargement? (y/N): ")
    if response.lower() != 'y':
        logger.info("Op√©ration annul√©e")
        return
    
    # Tests s√©quentiels des mod√®les
    results = {}
    
    for model_name, model_path in models_to_test.items():
        logger.info(f"\n{'='*60}")
        logger.info(f"üì• TRAITEMENT DE {model_name}")
        logger.info(f"{'='*60}")
        
        try:
            # 1. T√©l√©chargement
            download_success = download_model(model_name, model_path)
            if not download_success:
                results[model_name] = "‚ùå √âchec t√©l√©chargement"
                continue
            
            # 2. Test de chargement
            loading_success = test_model_loading(model_name, model_path)
            if not loading_success:
                results[model_name] = "‚ùå √âchec chargement"
                continue
            
            # 3. Test d'analyse de surveillance
            analysis_success = await test_surveillance_analysis(model_name, model_path)
            if analysis_success:
                results[model_name] = "‚úÖ Complet"
            else:
                results[model_name] = "‚ö†Ô∏è Partiel"
                
        except KeyboardInterrupt:
            logger.info("Interruption utilisateur")
            break
        except Exception as e:
            logger.error(f"Erreur inattendue pour {model_name}: {e}")
            results[model_name] = f"‚ùå Erreur: {e}"
    
    # R√©sum√© final
    logger.info(f"\n{'='*60}")
    logger.info("üìä R√âSUM√â DES TESTS")
    logger.info(f"{'='*60}")
    
    for model_name, status in results.items():
        logger.info(f"{model_name}: {status}")
    
    logger.info("\nüéØ Tests termin√©s!")
    
    # Recommandations
    logger.info("\nüí° RECOMMANDATIONS:")
    logger.info("- Pour la production: Utilisez LLaVA-NeXT (plus stable)")
    logger.info("- Pour la recherche: Qwen2-VL (meilleur raisonnement)")
    logger.info("- Pour l'edge computing: Utilisez les versions quantifi√©es")
    logger.info("- Surveillez l'utilisation m√©moire GPU en production")


if __name__ == "__main__":
    asyncio.run(main())