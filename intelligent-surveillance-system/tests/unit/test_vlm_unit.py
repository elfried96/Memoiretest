"""
üß† Tests Unitaires VLM - √âvaluation isol√©e des mod√®les de langage visuel
=======================================================================

Tests sp√©cialis√©s pour valider individuellement :
- Chargement et switch des mod√®les (Kimi-VL, Qwen2-VL)
- Performance d'inf√©rence avec m√©triques pr√©cises
- Robustesse et fallback automatique
- Utilisation GPU optimis√©e
"""

import pytest
import torch
import time
import numpy as np
from typing import Dict, Any, List
import asyncio
from pathlib import Path
import base64
from PIL import Image
import io

from src.core.vlm.dynamic_model import DynamicVisionLanguageModel
from src.core.vlm.model_registry import VLMModelRegistry, VLMModelType
from src.core.types import AnalysisRequest, SuspicionLevel, ActionType
from src.utils.performance import measure_time, get_current_performance


class TestVLMUnitIsolated:
    """Tests unitaires isol√©s pour les mod√®les VLM."""
    
    @pytest.fixture
    def sample_image_b64(self) -> str:
        """Image d'exemple encod√©e en base64."""
        # Cr√©ation d'une image test 224x224
        img = Image.new('RGB', (224, 224), color='red')
        buffer = io.BytesIO()
        img.save(buffer, format='JPEG')
        img_bytes = buffer.getvalue()
        return base64.b64encode(img_bytes).decode('utf-8')
    
    @pytest.fixture
    def vlm_model(self) -> DynamicVisionLanguageModel:
        """Instance VLM pour tests."""
        return DynamicVisionLanguageModel(
            default_model="kimi-vl-a3b-thinking",
            device="auto",
            enable_fallback=True
        )
    
    @pytest.mark.asyncio
    async def test_model_loading_performance(self, vlm_model):
        """Test 1.1: Performance de chargement des mod√®les."""
        start_time = time.time()
        
        # Test chargement Kimi-VL
        success_kimi = await vlm_model.load_model("kimi-vl-a3b-thinking")
        kimi_load_time = time.time() - start_time
        
        assert success_kimi, "√âchec chargement Kimi-VL-A3B-Thinking"
        assert kimi_load_time < 30.0, f"Chargement Kimi trop lent: {kimi_load_time:.2f}s"
        
        # V√©rification √©tat du mod√®le
        assert vlm_model.is_loaded
        assert vlm_model.current_model_id == "kimi-vl-a3b-thinking"
        assert vlm_model.model is not None
        assert vlm_model.processor is not None
        
        print(f"‚úÖ Kimi-VL charg√© en {kimi_load_time:.2f}s")
    
    @pytest.mark.asyncio
    async def test_model_switching_performance(self, vlm_model):
        """Test 1.2: Performance de switch entre mod√®les."""
        # Chargement initial
        await vlm_model.load_model("kimi-vl-a3b-thinking")
        
        # Switch vers Qwen2-VL
        start_time = time.time()
        success_switch = await vlm_model.switch_model("qwen2-vl-7b-instruct")
        switch_time = time.time() - start_time
        
        if success_switch:
            assert vlm_model.current_model_id == "qwen2-vl-7b-instruct"
            assert switch_time < 45.0, f"Switch trop lent: {switch_time:.2f}s"
            print(f"‚úÖ Switch Kimi‚ÜíQwen r√©ussi en {switch_time:.2f}s")
        else:
            print("‚ö†Ô∏è Switch √©chou√© - Fallback test√©")
    
    @pytest.mark.asyncio
    async def test_inference_performance_isolated(self, vlm_model, sample_image_b64):
        """Test 1.3: Performance d'inf√©rence isol√©e."""
        await vlm_model.load_model("kimi-vl-a3b-thinking")
        
        request = AnalysisRequest(
            frame_data=sample_image_b64,
            context={"location": "test_lab", "mode": "unit_test"},
            tools_available=[]  # Pas d'outils = test VLM pur
        )
        
        # Test inf√©rence multiple pour moyenner
        inference_times = []
        for i in range(5):
            start_time = time.time()
            result = await vlm_model.analyze_with_tools(request, use_advanced_tools=False)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            
            # Validations
            assert result is not None
            assert result.confidence >= 0.0
            assert result.suspicion_level in SuspicionLevel
            assert result.action_type in ActionType
        
        avg_inference_time = np.mean(inference_times)
        std_inference_time = np.std(inference_times)
        
        assert avg_inference_time < 3.0, f"Inf√©rence trop lente: {avg_inference_time:.2f}s"
        
        print(f"‚úÖ Inf√©rence VLM: {avg_inference_time:.2f}¬±{std_inference_time:.2f}s")
        
        return {
            "avg_inference_time": avg_inference_time,
            "std_inference_time": std_inference_time,
            "model_used": vlm_model.current_model_id,
            "gpu_used": torch.cuda.is_available()
        }
    
    @pytest.mark.asyncio
    async def test_fallback_mechanism(self, vlm_model):
        """Test 1.4: M√©canisme de fallback."""
        # Test avec mod√®le inexistant pour d√©clencher fallback
        success = await vlm_model.load_model("modele-inexistant-test")
        
        if vlm_model.enable_fallback:
            assert vlm_model.is_loaded, "Fallback devrait charger un mod√®le"
            assert vlm_model.current_model_id in ["qwen2-vl-7b-instruct"], "Mauvais mod√®le fallback"
            print(f"‚úÖ Fallback activ√©: {vlm_model.current_model_id}")
        else:
            assert not success, "Sans fallback, le chargement devrait √©chouer"
            print("‚úÖ Fallback d√©sactiv√© fonctionne correctement")
    
    @pytest.mark.asyncio
    async def test_memory_management(self, vlm_model):
        """Test 1.5: Gestion m√©moire GPU/CPU."""
        initial_memory = get_current_performance()
        
        # Chargement mod√®le
        await vlm_model.load_model("kimi-vl-a3b-thinking")
        post_load_memory = get_current_performance()
        
        # Multiple inf√©rences
        request = AnalysisRequest(
            frame_data="iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg==",  # pixel rouge 1x1
            context={},
            tools_available=[]
        )
        
        for _ in range(10):
            await vlm_model.analyze_with_tools(request, use_advanced_tools=False)
        
        # D√©chargement
        vlm_model._unload_current_model()
        final_memory = get_current_performance()
        
        # V√©rifications m√©moire
        memory_growth = post_load_memory.memory_used_mb - initial_memory.memory_used_mb
        memory_cleaned = final_memory.memory_used_mb < post_load_memory.memory_used_mb
        
        print(f"üìä M√©moire: Initial={initial_memory.memory_used_mb:.1f}MB, "
              f"Apr√®s chargement={post_load_memory.memory_used_mb:.1f}MB, "
              f"Final={final_memory.memory_used_mb:.1f}MB")
        
        assert memory_growth < 8000, f"Croissance m√©moire excessive: {memory_growth:.1f}MB"
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    @pytest.mark.asyncio
    async def test_model_registry_validation(self):
        """Test 1.6: Validation du registre de mod√®les."""
        registry = VLMModelRegistry()
        
        # Test mod√®les disponibles
        available_models = registry.list_available_models()
        assert len(available_models) >= 2, "Registry doit contenir au moins 2 mod√®les"
        
        # Test validation Kimi-VL
        is_available, message = registry.validate_model_availability("kimi-vl-a3b-thinking")
        print(f"Kimi-VL disponible: {is_available} - {message}")
        
        # Test validation Qwen2-VL
        is_available_qwen, message_qwen = registry.validate_model_availability("qwen2-vl-7b-instruct")
        print(f"Qwen2-VL disponible: {is_available_qwen} - {message_qwen}")
        
        # Test mod√®le recommand√©
        recommended = registry.get_recommended_model("surveillance")
        assert recommended == "kimi-vl-a3b-thinking", f"Mod√®le recommand√© incorrect: {recommended}"
        
        # Test comparaison de mod√®les
        comparison = registry.get_model_comparison()
        assert "models" in comparison
        assert "summary" in comparison
        print(f"üìä {comparison['summary']['total_models']} mod√®les dans le registry")


@pytest.mark.gpu
class TestVLMGPUOptimized:
    """Tests sp√©cialement optimis√©s pour environnement GPU."""
    
    @pytest.mark.asyncio
    async def test_gpu_acceleration(self):
        """Test GPU: Acc√©l√©ration mat√©rielle."""
        if not torch.cuda.is_available():
            pytest.skip("GPU non disponible")
        
        vlm_cpu = DynamicVisionLanguageModel(device="cpu")
        vlm_gpu = DynamicVisionLanguageModel(device="cuda")
        
        # Test chargement
        await vlm_cpu.load_model("kimi-vl-a3b-thinking")
        await vlm_gpu.load_model("kimi-vl-a3b-thinking")
        
        # Comparaison performance CPU vs GPU
        request = AnalysisRequest(
            frame_data="iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg==",
            context={},
            tools_available=[]
        )
        
        # CPU timing
        start_cpu = time.time()
        await vlm_cpu.analyze_with_tools(request, use_advanced_tools=False)
        cpu_time = time.time() - start_cpu
        
        # GPU timing
        start_gpu = time.time()
        await vlm_gpu.analyze_with_tools(request, use_advanced_tools=False)
        gpu_time = time.time() - start_gpu
        
        print(f"üöÄ Performance: CPU={cpu_time:.2f}s, GPU={gpu_time:.2f}s")
        print(f"üìà Acc√©l√©ration GPU: {cpu_time/gpu_time:.2f}x")
        
        # Nettoyage
        vlm_cpu._unload_current_model()
        vlm_gpu._unload_current_model()


if __name__ == "__main__":
    # Ex√©cution directe pour d√©veloppement
    pytest.main([__file__, "-v", "--tb=short"])