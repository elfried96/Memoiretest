#!/bin/bash
# ğŸš€ Script de DÃ©marrage Rapide - VLM Chatbot Symbiosis
# =====================================================

echo "ğŸ§  VLM Chatbot Symbiosis - DÃ©marrage Intelligent"
echo "================================================"

# Configuration environnement
export PYTHONPATH="${PWD}/src:${PWD}/dashboard:${PYTHONPATH}"
export TOKENIZERS_PARALLELISM=false
export AUTO_INIT_VLM=true
export FORCE_REAL_PIPELINE=true

# VÃ©rification GPU
echo "ğŸ” VÃ©rification GPU..."
if command -v nvidia-smi &> /dev/null; then
    GPU_INFO=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | head -1)
    echo "âœ… GPU dÃ©tectÃ©: $GPU_INFO"
    export CUDA_VISIBLE_DEVICES=0
    export VLM_MODE="gpu"
else
    echo "âš ï¸ Aucun GPU dÃ©tectÃ© - Mode CPU/Simulation"
    export VLM_MODE="cpu"
fi

# VÃ©rification dÃ©pendances Python
echo "ğŸ” VÃ©rification dÃ©pendances..."
python3 -c "
try:
    import streamlit, torch, transformers, PIL, matplotlib, loguru
    print('âœ… DÃ©pendances principales OK')
except ImportError as e:
    print(f'âŒ DÃ©pendance manquante: {e}')
    exit(1)
"

if [ $? -ne 0 ]; then
    echo "ğŸ’¡ Installation dÃ©pendances manquantes..."
    pip install streamlit torch torchvision transformers pillow matplotlib loguru numpy pandas plotly
fi

# Test chatbot VLM
echo "ğŸ§ª Test rapide VLM Chatbot..."
timeout 30s python3 test_vlm_chatbot.py || echo "âš ï¸ Test timeout - Continuing..."

# Choix mode lancement
echo ""
echo "ğŸ¯ Choisissez le mode de lancement:"
echo "1) ğŸ® Dashboard complet avec VLM Chatbot"
echo "2) ğŸ§  Test interactif chatbot VLM"
echo "3) âš¡ Qwen2.5-VL-32B optimisÃ© (GPU requis)"
echo "4) ğŸ”§ Mode dÃ©veloppement/debug"

read -p "Votre choix (1-4): " choice

case $choice in
    1)
        echo "ğŸ® Lancement Dashboard VLM Chatbot..."
        cd dashboard/
        echo "ğŸŒ Dashboard sera disponible sur: http://localhost:8501"
        echo "ğŸ’¬ Chat VLM disponible dans onglets 'Surveillance' et 'Upload VidÃ©o'"
        streamlit run production_dashboard.py
        ;;
    2)
        echo "ğŸ§  Mode test interactif chatbot..."
        python3 -c "
import asyncio
import sys
sys.path.append('dashboard')
from vlm_chatbot_symbiosis import get_vlm_chatbot, process_vlm_chat_query

async def interactive_chat():
    print('ğŸ¤– VLM Chatbot Interactif - Tapez \"quit\" pour sortir')
    chatbot = get_vlm_chatbot()
    
    mock_context = {
        'stats': {'frames_processed': 42, 'current_performance_score': 0.89},
        'detections': [], 'optimizations': []
    }
    
    while True:
        question = input('\\nğŸ‘¤ Vous: ')
        if question.lower() in ['quit', 'exit', 'q']:
            break
            
        try:
            response = await process_vlm_chat_query(
                question=question,
                chat_type='surveillance', 
                vlm_context=mock_context
            )
            print(f'\\nğŸ§  VLM: {response.get(\"response\", \"Erreur rÃ©ponse\")}')
            
            if response.get('thinking'):
                print(f'ğŸ’­ Thinking: {response[\"thinking\"][:200]}...')
                
        except Exception as e:
            print(f'âŒ Erreur: {e}')

asyncio.run(interactive_chat())
        "
        ;;
    3)
        echo "âš¡ Lancement Qwen2.5-VL-32B optimisÃ©..."
        if [ "$VLM_MODE" = "cpu" ]; then
            echo "âŒ GPU requis pour Qwen2.5-VL-32B"
            exit 1
        fi
        
        echo "ğŸ”§ Configuration GPU optimale..."
        export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
        
        # Test vidÃ©o avec Qwen2.5-VL-32B
        if [ -f "videos/test.mp4" ]; then
            python3 launch_qwen_32b.py --video videos/test.mp4 --max-frames 10
        else
            echo "âš ï¸ Aucune vidÃ©o test trouvÃ©e dans videos/"
            echo "ğŸ’¡ CrÃ©ez un dossier 'videos/' avec des fichiers MP4 pour tester"
        fi
        ;;
    4)
        echo "ğŸ”§ Mode dÃ©veloppement/debug..."
        echo "ğŸ“Š Variables environnement:"
        echo "   - VLM_MODE: $VLM_MODE"
        echo "   - CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-non_dÃ©fini}"
        echo "   - PYTHONPATH: $PYTHONPATH"
        
        echo "ğŸ§ª Tests disponibles:"
        echo "   python3 test_vlm_chatbot.py        # Test complet chatbot"
        echo "   python3 launch_qwen_32b.py --help  # Aide Qwen2.5-VL-32B"
        
        echo "ğŸ® Dashboard:"
        echo "   cd dashboard/ && streamlit run production_dashboard.py"
        
        echo "ğŸ Shell interactif Python:"
        python3 -c "
import sys
sys.path.append('dashboard')
sys.path.append('src')

print('ğŸ”¬ Modules disponibles:')
try:
    from vlm_chatbot_symbiosis import get_vlm_chatbot
    print('  âœ… vlm_chatbot_symbiosis')
except:
    print('  âŒ vlm_chatbot_symbiosis')

try:
    from real_pipeline_integration import get_real_pipeline
    print('  âœ… real_pipeline_integration')
except:
    print('  âŒ real_pipeline_integration')

print('\\nğŸ’¡ Commandes utiles:')
print('  chatbot = get_vlm_chatbot()')
print('  pipeline = get_real_pipeline()')
"
        bash
        ;;
    *)
        echo "âŒ Choix invalide"
        exit 1
        ;;
esac