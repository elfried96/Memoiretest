# üìö Guide d'Utilisation Complet - Syst√®me de Surveillance Intelligente

## üéØ Aper√ßu du Code Principal

### **main.py** - Architecture Compl√®te

Le fichier `main.py` est le **c≈ìur du syst√®me** qui orchestre tous les composants :

```python
# Structure principale du main.py :

1. üì¶ IMPORTS
   - asyncio, cv2, argparse (syst√®me)
   - DynamicVisionLanguageModel (VLM multi-mod√®les)
   - ModernVLMOrchestrator (coordination 8 outils)
   - YOLODetector, BYTETracker (d√©tection/tracking)

2. üèóÔ∏è CLASSES PRINCIPALES
   - AlertLevel : Enum des niveaux d'alerte
   - SurveillanceFrame : Structure d'un frame enrichi
   - SurveillanceDecisionEngine : Moteur de d√©cision
   - IntelligentSurveillanceSystem : Syst√®me complet

3. üîß FONCTIONS UTILITAIRES
   - parse_arguments() : Arguments ligne de commande
   - validate_video_source() : Validation sources vid√©o
   - main() : Point d'entr√©e principal
```

## üöÄ Utilisation avec Arguments

### **Webcam (D√©faut)**
```bash
python main.py
# ou explicitement
python main.py --video webcam
```

### **Fichier Vid√©o Local**
```bash
# Fichiers support√©s : .mp4, .avi, .mov, .mkv, .webm
python main.py --video /chemin/vers/video.mp4
python main.py -v ./surveillance_footage.avi
python main.py --video "C:/Videos/ma video avec espaces.mp4"
```

### **Flux R√©seau (RTSP/HTTP)**
```bash
# Cam√©ra IP RTSP
python main.py --video rtsp://192.168.1.100:554/stream
python main.py --video rtsp://admin:password@camera_ip/live

# Flux HTTP
python main.py --video http://example.com/livestream.mjpg
```

### **Configuration Avanc√©e**
```bash
# Mode rapide avec LLaVA
python main.py --video video.mp4 --model llava-v1.6-mistral-7b --mode FAST

# Mode complet avec Kimi-VL
python main.py --video webcam --model kimi-vl-a3b-thinking --mode THOROUGH

# Mode headless (sans affichage)
python main.py --video video.mp4 --no-display --max-frames 1000

# Qwen pour analyse complexe
python main.py --video surveillance.mp4 --model qwen2-vl-7b-instruct --mode BALANCED
```

## üß© Explication D√©taill√©e des Classes

### **1. AlertLevel (Enum)**
```python
class AlertLevel(Enum):
    NORMAL = "normal"      # Situation normale
    ATTENTION = "attention" # Surveillance renforc√©e
    ALERTE = "alerte"      # Intervention requise
    CRITIQUE = "critique"   # Action imm√©diate
```

### **2. SurveillanceFrame (@dataclass)**
```python
@dataclass
class SurveillanceFrame:
    frame_id: int                              # ID unique du frame
    timestamp: float                           # Timestamp Unix
    frame: np.ndarray                          # Image OpenCV (BGR)
    detections: List[Detection]                # D√©tections YOLO
    tracked_objects: List[Dict]                # Objets suivis avec IDs
    vlm_analysis: Optional[AnalysisResponse]   # Analyse VLM (si effectu√©e)
    alert_level: AlertLevel                    # Niveau d'alerte calcul√©
    actions_taken: List[str]                   # Actions automatis√©es prises
```

### **3. SurveillanceDecisionEngine**

**R√¥le** : Convertit les analyses VLM en d√©cisions concr√®tes et actions automatis√©es.

```python
def process_analysis(self, analysis: AnalysisResponse, frame_context: Dict) -> Dict:
    """
    LOGIQUE DE D√âCISION :
    
    1. Niveau de suspicion √©lev√© ‚Üí ALERTE + enregistrement
    2. Confiance faible (< 0.6) ‚Üí R√©vision humaine requise
    3. Actions sp√©cifiques par type :
       - SUSPICIOUS_ACTIVITY ‚Üí Surveillance renforc√©e + alerte s√©curit√©
       - THEFT_ATTEMPT ‚Üí Alerte imm√©diate + contact s√©curit√© + tracking suspect
       - LOITERING ‚Üí Observation prolong√©e + intervention staff
    4. Contexte temporel ‚Üí Hors heures = plus strict
    5. Outils utilis√©s ‚Üí Actions sp√©cialis√©es
    """
```

**Exemples de d√©cisions** :
```python
# Suspicion √©lev√©e d√©tect√©e
{
    "alert_level": AlertLevel.ALERTE,
    "actions": ["start_recording", "alert_security", "track_individual"],
    "notifications": ["Suspicion HIGH d√©tect√©e"],
    "recording_required": True,
    "human_review": False
}

# Confiance faible
{
    "alert_level": AlertLevel.NORMAL,
    "actions": ["request_human_review"],
    "human_review": True
}
```

### **4. IntelligentSurveillanceSystem** - Classe Principale

#### **__init__()** - Initialisation
```python
def __init__(self, video_source, vlm_model, orchestration_mode):
    # 1. Configuration source vid√©o
    self.video_source = video_source
    
    # 2. Composants de d√©tection
    self.yolo_detector = YOLODetector(model_path="yolov11n.pt")
    self.tracker = BYTETracker()
    
    # 3. VLM multi-mod√®les
    self.vlm = DynamicVisionLanguageModel(
        default_model=vlm_model,
        enable_fallback=True  # Auto-switch vers LLaVA si Kimi-VL indispo
    )
    
    # 4. Orchestrateur 8 outils
    self.orchestrator = ModernVLMOrchestrator(config=OrchestrationConfig())
    
    # 5. Moteur de d√©cision
    self.decision_engine = SurveillanceDecisionEngine()
    
    # 6. Statistiques de performance
    self.processing_stats = {
        "total_frames": 0,
        "detected_objects": 0,
        "vlm_analyses": 0,
        "alerts_triggered": 0,
        "average_fps": 0.0
    }
```

#### **process_frame()** - C≈ìur du Pipeline
```python
async def process_frame(self, frame: np.ndarray) -> SurveillanceFrame:
    """
    PIPELINE COMPLET D'UN FRAME :
    
    1. üîç D√âTECTION YOLO
       - yolo_results = self.yolo_detector.detect(frame)
       - D√©tecte personnes, objets avec bbox + confiance
    
    2. üéØ TRACKING MULTI-OBJETS
       - tracked_objects = self.tracker.update(detections)
       - Assigne IDs persistants, suit mouvements
    
    3. üß† ANALYSE VLM (CONDITIONNELLE)
       D√©clenche si :
       - Personnes d√©tect√©es OU
       - Tous les 30 frames (analyse p√©riodique)
       
       Pipeline VLM :
       - frame_b64 = encode_frame_to_base64(frame)
       - context = {frame_id, timestamp, location, person_count, ...}
       - vlm_analysis = orchestrator.analyze_surveillance_frame()
    
    4. üö® PRISE DE D√âCISIONS
       - decisions = decision_engine.process_analysis(vlm_analysis)
       - alert_level, actions_taken = decisions
    
    5. üìä RETOUR SURVEILLANCE_FRAME
       - Agr√®ge toutes les donn√©es du frame
    """
```

#### **run_surveillance()** - Boucle Principale
```python
async def run_surveillance(self, max_frames=None, display=True):
    """
    BOUCLE INFINIE DE SURVEILLANCE :
    
    1. cap = cv2.VideoCapture(self.video_source)
    2. while True:
         - ret, frame = cap.read()
         - surveillance_frame = await self.process_frame(frame)
         - if display: cv2.imshow() avec overlays
         - ESC pour quitter
         - Logging p√©riodique (60 frames)
    3. Nettoyage : cap.release(), cv2.destroyAllWindows()
    """
```

#### **draw_surveillance_overlay()** - Interface Visuelle
```python
def draw_surveillance_overlay(self, frame, surveillance_frame):
    """
    OVERLAYS AFFICH√âS :
    
    üîç D√âTECTIONS :
    - Rectangles color√©s : vert=personne, rouge=objet
    - Labels avec confiance : "person: 0.85"
    
    üìä INFORMATIONS SYST√àME :
    - Frame ID actuel
    - Niveau d'alerte (couleur selon gravit√©)
    - R√©sultats VLM : suspicion + confiance
    - Action type d√©tect√©
    
    üìà STATISTIQUES TEMPS R√âEL :
    - FPS moyen
    - Nombre d√©tections frame actuel
    - Total alertes d√©clench√©es
    """
```

## üîß Fonctions Utilitaires

### **parse_arguments()** - Arguments CLI
```python
def parse_arguments():
    """
    ARGUMENTS SUPPORT√âS :
    
    --video/-v     : Source vid√©o (webcam/fichier/RTSP)
    --model/-m     : Mod√®le VLM (kimi-vl-a3b-thinking par d√©faut)
    --mode         : Mode orchestration (FAST/BALANCED/THOROUGH)
    --max-frames   : Limite nombre frames (None = infini)
    --no-display   : Mode headless sans affichage
    
    EXEMPLES AUTO-G√âN√âR√âS dans --help
    """
```

### **validate_video_source()** - Validation Sources
```python
def validate_video_source(video_arg: str):
    """
    LOGIQUE DE VALIDATION :
    
    1. "webcam" ‚Üí return 0 (device webcam)
    2. Fichier local existant ‚Üí return str(path)
       - Extensions recommand√©es : .mp4, .avi, .mov, .mkv, .webm
       - Warning si extension non standard
    3. URL r√©seau (rtsp://, http://) ‚Üí return url
    4. Fichier inexistant ‚Üí Warning + tentative ouverture
    """
```

## üìä Workflow D√©taill√© Frame par Frame

### **Frame N** - Exemple de Traitement
```python
# ENTR√âE : frame OpenCV (640x480, BGR)
frame = np.ndarray(shape=(480, 640, 3))

# 1. D√âTECTION YOLO (10-20ms)
yolo_results = yolo_detector.detect(frame)
detections = [
    Detection(bbox=BoundingBox(100, 50, 200, 300), confidence=0.85, class_name="person"),
    Detection(bbox=BoundingBox(300, 150, 400, 250), confidence=0.72, class_name="backpack")
]

# 2. TRACKING (5ms)
tracked_objects = tracker.update(detections)
# ‚Üí Assigne track_1, track_2, met √† jour trajectoires

# 3. ANALYSE VLM CONDITIONNELLE (2000-5000ms selon mode)
if len(persons) > 0 or frame_count % 30 == 0:
    frame_b64 = encode_frame_to_base64(frame)
    context = {
        "frame_id": 1234,
        "person_count": 1,
        "location": "Store Main Area",
        "time_of_day": "14:30:15"
    }
    
    # Orchestration selon mode :
    # FAST: 3 outils (dino, pose, fusion) ‚Üí ~1.2s
    # BALANCED: 6 outils + sam2, trajectory, adversarial ‚Üí ~2.5s  
    # THOROUGH: 8 outils complets ‚Üí ~4.8s
    
    vlm_analysis = AnalysisResponse(
        suspicion_level=SuspicionLevel.MEDIUM,
        action_type=ActionType.SUSPICIOUS_ACTIVITY,
        confidence=0.78,
        description="Personne near high-value items, extended observation",
        tools_used=["sam2_segmentator", "pose_estimator", "trajectory_analyzer"],
        recommendations=["Surveillance renforc√©e", "Intervention discr√®te"]
    )

# 4. D√âCISIONS AUTOMATIS√âES (1ms)
decisions = decision_engine.process_analysis(vlm_analysis, context)
{
    "alert_level": AlertLevel.ATTENTION,
    "actions": ["increase_monitoring", "alert_security", "track_individual"],
    "recording_required": False,
    "human_review": False
}

# 5. FRAME DE SURVEILLANCE ENRICHI
surveillance_frame = SurveillanceFrame(
    frame_id=1234,
    timestamp=1704886800.123,
    frame=frame,
    detections=detections,
    tracked_objects=tracked_objects,
    vlm_analysis=vlm_analysis,
    alert_level=AlertLevel.ATTENTION,
    actions_taken=["increase_monitoring", "alert_security"]
)

# 6. AFFICHAGE AVEC OVERLAYS
overlay_frame = draw_surveillance_overlay(frame, surveillance_frame)
cv2.imshow("Surveillance", overlay_frame)
```

## üéõÔ∏è Modes d'Orchestration D√©taill√©s

### **FAST Mode** (‚ö° Performance)
```python
tools_used = ["dino_features", "pose_estimator", "multimodal_fusion"]

# Utilisation :
# - Edge computing, embedded systems
# - Surveillance temps r√©el haute fr√©quence
# - Ressources limit√©es
# - Temps de r√©ponse critique (< 1.5s)

# Trade-offs :
# + Tr√®s rapide, faible consommation
# - Analyse moins approfondie
# - Peut manquer des nuances comportementales
```

### **BALANCED Mode** (‚öñÔ∏è Production)
```python
tools_used = [
    "sam2_segmentator",      # Segmentation pr√©cise
    "dino_features",         # Features robustes  
    "pose_estimator",        # Analyse posturale
    "trajectory_analyzer",   # Patterns mouvement
    "multimodal_fusion",     # Fusion intelligente
    "adversarial_detector"   # Protection attaques
]

# Utilisation :
# - Production standard (RECOMMAND√â)
# - √âquilibre performance/pr√©cision
# - Surveillance commerciale/industrielle
# - Bon compromis toutes situations

# Trade-offs :
# + Tr√®s bon √©quilibre
# + D√©tection robuste
# - Temps traitement mod√©r√© (2-3s)
```

### **THOROUGH Mode** (üî¨ Analyse)
```python
tools_used = [
    "sam2_segmentator", "dino_features", "pose_estimator",
    "trajectory_analyzer", "multimodal_fusion", 
    "temporal_transformer",  # Analyse s√©quentielle
    "adversarial_detector",  # S√©curit√© avanc√©e
    "domain_adapter"         # Multi-environnements
]

# Utilisation :
# - Analyse forensique post-incident
# - Recherche et d√©veloppement
# - Environnements haute s√©curit√©
# - Quand pr√©cision > vitesse

# Trade-offs :
# + Analyse la plus compl√®te
# + D√©tection sophistiqu√©e
# - Plus lent (4-5s/frame)
# - Consommation ressources √©lev√©e
```

## üéØ Exemples d'Utilisation R√©els

### **Surveillance Magasin**
```bash
# Webcam temps r√©el, mode √©quilibr√©
python main.py --video webcam --mode BALANCED --model kimi-vl-a3b-thinking

# Analyse vid√©o s√©curit√© existante
python main.py --video surveillance_20240115.mp4 --mode THOROUGH --model qwen2-vl-7b-instruct
```

### **Surveillance Entrep√¥t**
```bash
# Cam√©ra IP fixe, mode rapide pour monitoring continu
python main.py --video rtsp://192.168.1.100:554/stream --mode FAST --model llava-v1.6-mistral-7b

# Mode headless pour serveur
python main.py --video rtsp://warehouse_cam/live --mode BALANCED --no-display
```

### **Analyse Forensique**
```bash
# Analyse approfondie incident
python main.py --video incident_footage.mp4 --mode THOROUGH --model kimi-vl-a3b-thinking --max-frames 500

# Export sans affichage
python main.py --video evidence.avi --mode THOROUGH --no-display --model qwen2-vl-7b-instruct
```

## üìà Performance et Optimisation

### **Temps de Traitement Typiques** (par frame)
```
Mode FAST    : 1.0-1.5s  (3 outils)
Mode BALANCED: 2.0-3.0s  (6 outils) 
Mode THOROUGH: 4.0-6.0s  (8 outils)

Facteurs d'influence :
- R√©solution vid√©o (plus haute = plus lent)
- Nombre de d√©tections (plus d'objets = plus lent)
- Mod√®le VLM choisi (Kimi-VL vs LLaVA vs Qwen)
- Hardware GPU (CUDA vs CPU)
```

### **Ressources Syst√®me**
```
RAM Minimum   : 8 GB (16 GB recommand√©)
GPU           : 6 GB VRAM (RTX 3060+) ou CPU puissant
Stockage      : 5 GB pour mod√®les + espace vid√©os
R√©seau        : Stable pour flux RTSP/mod√®les distants
```

## üõü D√©pannage et Logs

### **Logs Utiles**
```python
# Niveaux de log configurables
logging.basicConfig(level=logging.DEBUG)  # Plus de d√©tails

# Types de messages :
logger.info("üéØ Syst√®me initialis√©")        # Informations g√©n√©rales
logger.warning("‚ö†Ô∏è VLM √©chec, fallback")    # Avertissements
logger.error("‚ùå Erreur critique")           # Erreurs
logger.debug("üîç Frame 1234 - Analyse VLM") # Debug d√©taill√©
```

### **Erreurs Communes**
```bash
# Source vid√©o indisponible
‚ùå Impossible d'ouvrir la source vid√©o: /path/video.mp4
‚Üí V√©rifier chemin, permissions, format support√©

# Mod√®le VLM indisponible  
‚ö†Ô∏è VLM principal √©chec, fallback activ√©
‚Üí Normal, utilisation automatique de LLaVA

# CUDA out of memory
‚ùå CUDA out of memory
‚Üí R√©duire r√©solution, utiliser mode FAST, ou --no-display
```

Le syst√®me est maintenant **compl√®tement configur√©** pour traiter vos vid√©os avec le chemin de fichier sp√©cifi√© ! üöÄ