"""Service GPT intelligent pour le chat de surveillance VLM."""

import os
import openai
from typing import Dict, List, Any, Optional
from datetime import datetime
import json
import asyncio
from pathlib import Path

# Configuration de l'API OpenAI
def load_openai_key():
    """Charge la cl√© API OpenAI depuis le fichier .env."""
    env_file = Path(__file__).parent.parent / ".env"
    
    if env_file.exists():
        with open(env_file, 'r') as f:
            for line in f:
                if line.startswith('OPENAI_API_KEY='):
                    api_key = line.split('=', 1)[1].strip()
                    return api_key
    
    # Fallback vers variable d'environnement
    return os.getenv('OPENAI_API_KEY')

# Initialisation
OPENAI_API_KEY = load_openai_key()
if OPENAI_API_KEY:
    openai.api_key = OPENAI_API_KEY

class GPTSurveillanceChat:
    """Service de chat intelligent bas√© sur GPT pour l'analyse de surveillance."""
    
    def __init__(self):
        self.api_available = bool(OPENAI_API_KEY)
        self.client = None
        
        if self.api_available:
            try:
                from openai import AsyncOpenAI
                self.client = AsyncOpenAI(api_key=OPENAI_API_KEY)
            except ImportError:
                # Fallback vers ancienne version
                try:
                    import openai
                    openai.api_key = OPENAI_API_KEY
                    self.client = openai
                    self.use_legacy_api = True
                except ImportError:
                    self.api_available = False
        
        self.stats = {
            'queries_processed': 0,
            'successful_queries': 0,
            'error_count': 0,
            'average_response_time': 0.0
        }
    
    async def analyze_surveillance_question(
        self, 
        question: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Analyse intelligente d'une question de surveillance avec GPT.
        
        Args:
            question: Question de l'utilisateur
            context: Contexte complet (analyses vid√©o, descriptions, alertes)
            
        Returns:
            R√©ponse structur√©e avec contenu et m√©tadonn√©es
        """
        
        if not self.api_available:
            return self._fallback_response(question, context)
        
        start_time = datetime.now()
        
        try:
            # Construction du prompt contextualis√©
            system_prompt = self._build_surveillance_system_prompt()
            user_prompt = self._build_contextual_user_prompt(question, context)
            
            # Appel GPT avec contexte enrichi
            if hasattr(self, 'use_legacy_api') and self.use_legacy_api:
                response_content = await self._call_legacy_gpt(system_prompt, user_prompt)
            else:
                response_content = await self._call_modern_gpt(system_prompt, user_prompt)
            
            # Calcul du temps de traitement
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Mise √† jour des stats
            self.stats['queries_processed'] += 1
            self.stats['successful_queries'] += 1
            self.stats['average_response_time'] = (
                (self.stats['average_response_time'] * (self.stats['successful_queries'] - 1) + processing_time)
                / self.stats['successful_queries']
            )
            
            return {
                'content': response_content,
                'metadata': {
                    'provider': 'gpt-4',
                    'analysis_time': processing_time,
                    'confidence': 0.92,  # GPT-4 est tr√®s fiable pour l'analyse contextuelle
                    'tools_used': ['gpt-4', 'context_analysis', 'video_analysis'],
                    'context_items': len(context),
                    'api_available': True
                }
            }
            
        except Exception as e:
            self.stats['error_count'] += 1
            print(f"Erreur GPT Chat: {e}")
            return self._fallback_response(question, context)
    
    def _build_surveillance_system_prompt(self) -> str:
        """Construit le prompt syst√®me pour GPT sp√©cialis√© en surveillance."""
        
        return """Tu es un expert en surveillance retail et s√©curit√© avec 15 ans d'exp√©rience dans l'analyse comportementale et la pr√©vention du vol.

R√îLE ET EXPERTISE:
- Expert en surveillance de magasins/supermarch√©s
- Sp√©cialiste en d√©tection de comportements suspects et vol √† l'√©talage
- Analyste en s√©curit√© retail avec compr√©hension des techniques de vol modernes
- Conseiller en s√©curit√© pour les d√©cisions d'intervention

CAPACIT√âS SP√âCIALIS√âES:
- Analyse contextuelle des comportements suspects
- √âvaluation des niveaux de risque et recommandations d'actions
- Interpr√©tation des donn√©es de surveillance (vid√©o, d√©tections, timelines)
- Raisonnement bas√© sur le contexte fourni par l'op√©rateur

INSTRUCTIONS CRITIQUES:
1. **PRIORIT√â AU CONTEXTE UTILISATEUR**: Si l'op√©rateur mentionne "sortie sans payer", "vol", "vient de sortir sans passer √† la caisse", etc., c'est une INFORMATION FACTUELLE qui pr√©vaut sur l'analyse automatique.

2. **RAISONNEMENT CONTEXTUEL**: Analyse toujours en tenant compte de:
   - La description d√©taill√©e fournie par l'op√©rateur
   - Les r√©sultats d'analyse vid√©o automatique  
   - Le niveau de suspicion d√©tect√©
   - La timeline des √©v√©nements
   - Les objets et comportements d√©tect√©s

3. **R√âPONSES PROFESSIONNELLES**: 
   - Fournir des analyses claires et actionnables
   - Proposer des recommandations sp√©cifiques
   - √âvaluer les risques de fa√ßon √©quilibr√©e
   - √âviter les faux positifs mais ne pas ignorer les signaux d'alarme

4. **STYLE DE COMMUNICATION**:
   - Professionnel mais accessible
   - Structur√© avec sections claires
   - Bas√© sur les faits observ√©s
   - Recommandations pratiques et proportionn√©es

Tu dois analyser chaque situation avec l'expertise d'un professionnel de la s√©curit√© retail et fournir des conseils pertinents pour l'aide √† la d√©cision."""

    def _build_contextual_user_prompt(self, question: str, context: Dict[str, Any]) -> str:
        """Construit le prompt utilisateur avec tout le contexte."""
        
        prompt = f"**QUESTION DE L'OP√âRATEUR**: {question}\n\n"
        
        # Contexte vid√©o si disponible
        video_analyses = context.get('video_analyses', {})
        if video_analyses:
            prompt += "**CONTEXTE VID√âO ANALYS√â**:\n"
            
            for video_name, analysis in video_analyses.items():
                prompt += f"\nüìπ **{video_name}**:\n"
                
                # Description utilisateur si disponible
                if 'options_used' in analysis and analysis['options_used'].get('description_detaillee'):
                    user_description = analysis['options_used']['description_detaillee']
                    prompt += f"üî¥ **DESCRIPTION OP√âRATEUR**: {user_description}\n"
                
                # R√©sultats analyse automatique
                prompt += f"üìä **ANALYSE AUTOMATIQUE**:\n"
                prompt += f"   ‚Ä¢ Niveau de suspicion: {analysis.get('suspicion_level', 'Non d√©termin√©')}\n"
                prompt += f"   ‚Ä¢ Confiance: {analysis.get('confidence', 0):.0%}\n"
                
                # Timeline des √©v√©nements
                timeline = analysis.get('timeline', [])
                if timeline:
                    prompt += f"‚è±Ô∏è **CHRONOLOGIE**:\n"
                    for event in timeline[:3]:  # Limiter √† 3 √©v√©nements
                        time_marker = event.get('time', 'N/A')
                        event_desc = event.get('event', '√âv√©nement non sp√©cifi√©')
                        prompt += f"   ‚Ä¢ {time_marker}: {event_desc}\n"
                
                # Objets d√©tect√©s
                detected_objects = analysis.get('detected_objects', [])
                if detected_objects:
                    prompt += f"üîç **D√âTECTIONS**:\n"
                    for obj in detected_objects[:3]:
                        obj_type = obj.get('type', 'objet')
                        obj_count = obj.get('count', 1)
                        obj_confidence = obj.get('confidence', 0.0)
                        prompt += f"   ‚Ä¢ {obj_count}x {obj_type} (confiance: {obj_confidence:.0%})\n"
                
                prompt += "\n"
        
        # √âtat du syst√®me
        cameras_state = context.get('cameras_state', {})
        active_cameras = sum(1 for cam in cameras_state.values() if cam.get('enabled', False))
        prompt += f"üì∑ **√âTAT SURVEILLANCE**: {active_cameras}/{len(cameras_state)} cam√©ras actives\n"
        
        # Alertes actives
        active_alerts = context.get('active_alerts', [])
        if active_alerts:
            prompt += f"üö® **ALERTES ACTIVES**: {len(active_alerts)} alerte(s)\n"
            for alert in active_alerts[:2]:
                level = alert.get('level', 'INFO')
                message = alert.get('message', 'Alerte non sp√©cifi√©e')
                prompt += f"   ‚Ä¢ [{level}] {message}\n"
        else:
            prompt += "‚úÖ **ALERTES**: Aucune alerte active\n"
        
        prompt += "\n**ANALYSE DEMAND√âE**: "
        prompt += "En tant qu'expert en surveillance retail, analyse cette situation et r√©pond √† la question de l'op√©rateur avec ton expertise professionnelle. "
        prompt += "Prends en compte TOUS les √©l√©ments contextuels, particuli√®rement la description de l'op√©rateur qui a la priorit√© absolue."
        
        return prompt
    
    async def _call_modern_gpt(self, system_prompt: str, user_prompt: str) -> str:
        """Appel API GPT moderne (OpenAI >= 1.0)."""
        
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=800,
                temperature=0.3,  # Faible pour des r√©ponses plus coh√©rentes
                top_p=0.9
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"Erreur appel GPT moderne: {e}")
            # Fallback vers GPT-3.5 si GPT-4 √©choue
            try:
                response = await self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=600,
                    temperature=0.3
                )
                return response.choices[0].message.content.strip()
            except Exception as e2:
                raise e2
    
    async def _call_legacy_gpt(self, system_prompt: str, user_prompt: str) -> str:
        """Appel API GPT legacy (OpenAI < 1.0)."""
        
        try:
            response = await openai.ChatCompletion.acreate(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=800,
                temperature=0.3,
                top_p=0.9
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception:
            # Fallback vers GPT-3.5
            response = await openai.ChatCompletion.acreate(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=600,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
    
    def _fallback_response(self, question: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """R√©ponse de secours si GPT n'est pas disponible."""
        
        video_analyses = context.get('video_analyses', {})
        fallback_content = "ü§ñ **ANALYSE DE SURVEILLANCE** (Mode Fallback)\n\n"
        
        if video_analyses:
            latest_analysis = list(video_analyses.values())[-1]
            suspicion = latest_analysis.get('suspicion_level', 'LOW')
            
            fallback_content += f"**Votre question**: {question}\n\n"
            fallback_content += f"**Analyse disponible**:\n"
            fallback_content += f"‚Ä¢ Niveau de suspicion d√©tect√©: {suspicion}\n"
            fallback_content += f"‚Ä¢ Confiance: {latest_analysis.get('confidence', 0):.0%}\n\n"
            
            if suspicion in ['HIGH', 'CRITICAL']:
                fallback_content += "‚ö†Ô∏è **RECOMMANDATION**: Niveau de suspicion √©lev√© d√©tect√© - v√©rification manuelle recommand√©e.\n"
            elif suspicion == 'MEDIUM':
                fallback_content += "üëÅÔ∏è **RECOMMANDATION**: Surveillance continue - situation √† surveiller.\n"
            else:
                fallback_content += "‚úÖ **RECOMMANDATION**: Comportement normal d√©tect√© - surveillance de routine.\n"
        else:
            fallback_content += f"**Votre question**: {question}\n\n"
            fallback_content += "Aucune analyse vid√©o r√©cente disponible. Pour une analyse approfondie, veuillez uploader une vid√©o.\n"
        
        fallback_content += "\nüí° **Note**: Service GPT temporairement indisponible - analyse basique utilis√©e."
        
        return {
            'content': fallback_content,
            'metadata': {
                'provider': 'fallback',
                'analysis_time': 0.1,
                'confidence': 0.6,
                'tools_used': ['fallback_analysis'],
                'context_items': len(context),
                'api_available': False
            }
        }
    
    def get_service_status(self) -> Dict[str, Any]:
        """√âtat du service GPT."""
        
        return {
            'api_available': self.api_available,
            'api_key_configured': bool(OPENAI_API_KEY),
            'stats': self.stats.copy(),
            'models_available': ['gpt-4', 'gpt-3.5-turbo'] if self.api_available else [],
            'service_ready': self.api_available and bool(OPENAI_API_KEY)
        }

# Instance globale
_gpt_chat_service: Optional[GPTSurveillanceChat] = None

def get_gpt_chat_service() -> GPTSurveillanceChat:
    """R√©cup√®re le service GPT (singleton)."""
    global _gpt_chat_service
    
    if _gpt_chat_service is None:
        _gpt_chat_service = GPTSurveillanceChat()
    
    return _gpt_chat_service