#!/usr/bin/env python3
"""
ğŸ§ª Script de Test Simple pour l'Optimisation des Outils
======================================================

Test rapide pour vÃ©rifier que le systÃ¨me d'optimisation fonctionne.
"""

import asyncio
import sys
import os
from pathlib import Path

# Ajout du chemin pour les imports
sys.path.insert(0, str(Path(__file__).parent))

try:
    from src.testing.tool_optimization_benchmark import (
        ToolOptimizationBenchmark,
        quick_tool_evaluation
    )
    from src.core.orchestrator.adaptive_orchestrator import create_adaptive_orchestrator
    from src.core.orchestrator.tool_calling_vlm import create_tool_calling_vlm
    print("âœ… Tous les modules importÃ©s avec succÃ¨s")
except ImportError as e:
    print(f"âŒ Erreur d'import: {e}")
    print("VÃ©rifiez que tous les fichiers sont prÃ©sents")
    sys.exit(1)


async def test_basic_functionality():
    """Test de fonctionnalitÃ© de base."""
    
    print("\nğŸ§ª Test 1: Initialisation du Benchmark")
    try:
        benchmark = ToolOptimizationBenchmark()
        print("âœ… Benchmark initialisÃ©")
    except Exception as e:
        print(f"âŒ Erreur benchmark: {e}")
        return False
    
    print("\nğŸ§ª Test 2: GÃ©nÃ©ration des cas de test")
    try:
        await benchmark.load_test_cases()
        print(f"âœ… {len(benchmark.test_cases)} cas de test gÃ©nÃ©rÃ©s")
    except Exception as e:
        print(f"âŒ Erreur cas de test: {e}")
        return False
    
    print("\nğŸ§ª Test 3: GÃ©nÃ©ration des combinaisons d'outils")
    try:
        combinations = benchmark.generate_tool_combinations()
        print(f"âœ… {len(combinations)} combinaisons gÃ©nÃ©rÃ©es")
        print(f"ğŸ“ Exemples: {combinations[:3]}")
    except Exception as e:
        print(f"âŒ Erreur combinaisons: {e}")
        return False
    
    print("\nğŸ§ª Test 4: Orchestrateur adaptatif")
    try:
        orchestrator = create_adaptive_orchestrator()
        status = orchestrator.get_adaptive_status()
        print("âœ… Orchestrateur adaptatif crÃ©Ã©")
        print(f"ğŸ“Š Outils optimaux: {status['current_optimal_tools']}")
    except Exception as e:
        print(f"âŒ Erreur orchestrateur: {e}")
        return False
    
    print("\nğŸ§ª Test 5: VLM avec Tool Calling")
    try:
        vlm = create_tool_calling_vlm()
        stats = vlm.get_tool_calling_stats()
        print("âœ… VLM avec tool calling crÃ©Ã©")
        print(f"ğŸ”§ Outils enregistrÃ©s: {stats['registered_tools']}")
    except Exception as e:
        print(f"âŒ Erreur VLM tool calling: {e}")
        return False
    
    return True


async def test_quick_evaluation():
    """Test d'Ã©valuation rapide d'outils."""
    
    print("\nğŸš€ Test d'Ã‰valuation Rapide")
    
    try:
        # Test avec une petite combinaison d'outils
        test_tools = ["dino_features", "pose_estimator", "multimodal_fusion"]
        
        print(f"ğŸ”§ Test des outils: {test_tools}")
        
        # Note: Ce test utilisera des donnÃ©es synthÃ©tiques
        result = await quick_tool_evaluation(test_tools)
        
        print("âœ… Ã‰valuation rapide terminÃ©e:")
        print(f"   ğŸ“Š Quality Score: {result['quality_score']:.3f}")
        print(f"   ğŸ¯ F1 Score: {result['f1_score']:.3f}")
        print(f"   â±ï¸  Temps: {result['response_time']:.2f}s")
        print(f"   ğŸ’° CoÃ»t: {result['total_cost']:.1f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur Ã©valuation rapide: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_adaptive_learning():
    """Test d'apprentissage adaptatif simple."""
    
    print("\nğŸ§  Test d'Apprentissage Adaptatif")
    
    try:
        orchestrator = create_adaptive_orchestrator(enable_learning=True)
        
        # Simulation de quelques analyses
        test_scenarios = [
            {
                "context": {"location": "entrance", "time": "morning"},
                "detections": [{"class_name": "person"}]
            },
            {
                "context": {"location": "electronics", "time": "evening"}, 
                "detections": [{"class_name": "person"}, {"class_name": "backpack"}]
            }
        ]
        
        print("ğŸ“š Simulation d'apprentissage...")
        
        for i, scenario in enumerate(test_scenarios):
            print(f"   ğŸ”„ ScÃ©nario {i+1}: {scenario['context']['location']}")
            
            # Note: Utilise des donnÃ©es factices pour test
            analysis = await orchestrator.analyze_surveillance_frame(
                frame_data="test_frame_data",
                detections=scenario["detections"],
                context=scenario["context"]
            )
            
            print(f"   âœ… Outils utilisÃ©s: {len(analysis.tools_used)}")
        
        # Statut aprÃ¨s apprentissage
        status = orchestrator.get_adaptive_status()
        print(f"\nğŸ“ˆ Apprentissage effectuÃ©:")
        print(f"   ğŸ¯ Patterns appris: {status['learning_stats']['context_patterns_learned']}")
        print(f"   ğŸ“Š ExÃ©cutions: {status['learning_stats']['executions_in_history']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur apprentissage adaptatif: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Test principal."""
    
    print("ğŸ¯ TESTS DU SYSTÃˆME D'OPTIMISATION DES OUTILS")
    print("=" * 60)
    
    # VÃ©rification de l'environnement
    print(f"ğŸ Python: {sys.version.split()[0]}")
    print(f"ğŸ“ RÃ©pertoire: {Path.cwd()}")
    
    # Tests de base
    basic_ok = await test_basic_functionality()
    
    if not basic_ok:
        print("\nâŒ Tests de base Ã©chouÃ©s - ArrÃªt")
        return False
    
    print("\n" + "="*60)
    
    # Test d'Ã©valuation rapide
    eval_ok = await test_quick_evaluation()
    
    # Test d'apprentissage adaptatif
    learning_ok = await test_adaptive_learning()
    
    # RÃ©sumÃ© final
    print("\n" + "="*60)
    print("ğŸ“‹ RÃ‰SUMÃ‰ DES TESTS")
    print("="*60)
    
    results = {
        "Tests de base": "âœ…" if basic_ok else "âŒ",
        "Ã‰valuation rapide": "âœ…" if eval_ok else "âŒ", 
        "Apprentissage adaptatif": "âœ…" if learning_ok else "âŒ"
    }
    
    for test_name, status in results.items():
        print(f"{status} {test_name}")
    
    all_passed = all([basic_ok, eval_ok, learning_ok])
    
    if all_passed:
        print("\nğŸ‰ TOUS LES TESTS PASSÃ‰S !")
        print("ğŸš€ Le systÃ¨me d'optimisation est prÃªt Ã  Ãªtre utilisÃ©")
        print("\nCommandes disponibles:")
        print("  python examples/tool_optimization_demo.py --mode benchmark")
        print("  python examples/tool_optimization_demo.py --mode adaptive") 
        print("  python examples/tool_optimization_demo.py --mode full")
    else:
        print("\nâš ï¸  Certains tests ont Ã©chouÃ©")
        print("ğŸ“ VÃ©rifiez les erreurs ci-dessus avant d'utiliser le systÃ¨me complet")
    
    return all_passed


if __name__ == "__main__":
    asyncio.run(main())