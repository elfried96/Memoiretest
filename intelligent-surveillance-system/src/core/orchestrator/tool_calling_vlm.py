"""
üîß VLM avec Tool Calling Optimis√©
=================================

Interface VLM sp√©cialis√©e pour le tool calling intelligent avec optimisation automatique.
Ce module impl√©mente la logique de tool calling que le VLM utilise pour orchestrer les outils.

Features:
- Tool calling natif int√©gr√© au VLM
- S√©lection intelligente bas√©e sur le contexte
- Optimisation automatique des appels d'outils
- Gestion des erreurs et fallbacks
- Cache intelligent des r√©sultats
"""

import asyncio
import json
import time
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import numpy as np

from loguru import logger

from ..types import AnalysisRequest, AnalysisResponse, SuspicionLevel, ActionType
from ..vlm.dynamic_model import DynamicVisionLanguageModel


@dataclass
class ToolCall:
    """Repr√©sentation d'un appel d'outil."""
    tool_name: str
    parameters: Dict[str, Any]
    reasoning: str  # Raison de l'appel de cet outil
    priority: int = 1  # 1=haute, 2=moyenne, 3=basse
    expected_output_type: str = "dict"
    timeout_seconds: float = 5.0
    
    
@dataclass
class ToolCallResult:
    """R√©sultat d'un appel d'outil."""
    tool_call: ToolCall
    success: bool
    result: Any = None
    error_message: str = ""
    execution_time: float = 0.0
    confidence: float = 0.0


@dataclass  
class ToolCallPlan:
    """Plan d'ex√©cution des appels d'outils."""
    tool_calls: List[ToolCall]
    execution_strategy: str  # "sequential", "parallel", "conditional"
    estimated_total_time: float
    confidence_threshold: float = 0.6
    max_retries: int = 2


class ToolCallingVLM(DynamicVisionLanguageModel):
    """
    VLM avec capacit√©s de tool calling optimis√©.
    
    Extends DynamicVisionLanguageModel avec:
    - Tool calling intelligent
    - Optimisation bas√©e sur l'historique
    - Gestion des d√©pendances entre outils
    - Cache des r√©sultats d'outils
    """
    
    def __init__(
        self,
        default_model: str = "kimi-vl-a3b-thinking",
        enable_fallback: bool = True,
        tool_cache_enabled: bool = True
    ):
        super().__init__(default_model, enable_fallback)
        
        self.tool_cache_enabled = tool_cache_enabled
        self.tool_registry: Dict[str, Callable] = {}
        self.tool_call_history: List[ToolCallResult] = []
        self.tool_performance_cache: Dict[str, Dict] = {}
        
        # Cache des r√©sultats d'outils (cl√©: hash des param√®tres, valeur: r√©sultat)
        self.tool_results_cache: Dict[str, Tuple[Any, datetime]] = {}
        self.cache_ttl = timedelta(minutes=15)
        
        # Configuration du tool calling
        self.max_parallel_tools = 4
        self.tool_call_timeout = 30.0
        self.confidence_threshold = 0.6
        
        # Initialisation des outils par d√©faut
        self._register_default_tools()
        
        logger.info("ToolCallingVLM initialis√© avec capacit√©s de tool calling optimis√©")
    
    def _register_default_tools(self) -> None:
        """Enregistrement des outils par d√©faut."""
        
        # Outils de base pour surveillance
        self.register_tool("object_detection", self._tool_object_detection)
        self.register_tool("pose_analysis", self._tool_pose_analysis) 
        self.register_tool("behavior_assessment", self._tool_behavior_assessment)
        self.register_tool("context_analysis", self._tool_context_analysis)
        self.register_tool("risk_evaluation", self._tool_risk_evaluation)
        self.register_tool("temporal_analysis", self._tool_temporal_analysis)
        self.register_tool("spatial_analysis", self._tool_spatial_analysis)
        self.register_tool("confidence_validation", self._tool_confidence_validation)
    
    def register_tool(self, tool_name: str, tool_function: Callable) -> None:
        """Enregistrement d'un outil."""
        self.tool_registry[tool_name] = tool_function
        
        # Initialisation des m√©triques de performance
        if tool_name not in self.tool_performance_cache:
            self.tool_performance_cache[tool_name] = {
                "usage_count": 0,
                "success_rate": 1.0,
                "avg_execution_time": 1.0,
                "confidence_correlation": 0.5,
                "last_used": datetime.now()
            }
        
        logger.info(f"Outil enregistr√©: {tool_name}")
    
    async def analyze_with_tool_calling(
        self,
        request: AnalysisRequest,
        optimize_tool_selection: bool = True
    ) -> AnalysisResponse:
        """
        Analyse avec tool calling intelligent.
        
        Args:
            request: Requ√™te d'analyse
            optimize_tool_selection: Active l'optimisation de s√©lection d'outils
            
        Returns:
            R√©ponse d'analyse enrichie par les outils
        """
        
        start_time = time.time()
        
        try:
            # 1. Analyse initiale pour d√©terminer les outils n√©cessaires
            initial_analysis = await self._initial_context_analysis(request)
            
            # 2. G√©n√©ration du plan de tool calling
            tool_plan = await self._generate_tool_call_plan(
                request, initial_analysis, optimize_tool_selection
            )
            
            # 3. Ex√©cution des appels d'outils
            tool_results = await self._execute_tool_call_plan(tool_plan)
            
            # 4. Synth√®se finale avec int√©gration des r√©sultats d'outils
            final_analysis = await self._synthesize_with_tool_results(
                request, initial_analysis, tool_results
            )
            
            # 5. Mise √† jour des m√©triques de performance
            self._update_tool_performance_metrics(tool_results)
            
            processing_time = time.time() - start_time
            
            logger.info(
                f"Analyse avec tool calling termin√©e: {len(tool_results)} outils utilis√©s "
                f"en {processing_time:.2f}s - Confiance: {final_analysis.confidence:.2f}"
            )
            
            return final_analysis
            
        except Exception as e:
            logger.error(f"Erreur tool calling: {e}")
            
            # Fallback vers analyse simple
            return await self.analyze_frame(request)
    
    async def _initial_context_analysis(self, request: AnalysisRequest) -> Dict[str, Any]:
        """Analyse initiale du contexte pour d√©terminer les outils n√©cessaires."""
        
        context = request.context or {}
        
        # Analyse des √©l√©ments pr√©sents
        elements_detected = {
            "persons": context.get("person_count", 0),
            "objects": context.get("detections_count", 0),
            "location_type": context.get("location", "unknown"),
            "time_period": self._determine_time_period(),
            "complexity_level": self._assess_complexity(context)
        }
        
        # D√©termination du niveau de risque initial
        risk_indicators = []
        
        if elements_detected["persons"] > 3:
            risk_indicators.append("crowded_scene")
        
        if elements_detected["location_type"] in ["electronics", "jewelry", "cash_register"]:
            risk_indicators.append("high_value_area")
        
        if elements_detected["time_period"] in ["night", "late_evening"]:
            risk_indicators.append("off_hours")
        
        # Score de priorit√© pour s√©lection d'outils
        priority_score = len(risk_indicators) + elements_detected["complexity_level"]
        
        return {
            "elements_detected": elements_detected,
            "risk_indicators": risk_indicators,
            "priority_score": priority_score,
            "recommended_analysis_depth": "thorough" if priority_score >= 3 else "standard"
        }
    
    def _determine_time_period(self) -> str:
        """D√©termination de la p√©riode temporelle."""
        hour = datetime.now().hour
        
        if 6 <= hour < 9:
            return "morning"
        elif 9 <= hour < 12:
            return "late_morning"
        elif 12 <= hour < 17:
            return "afternoon"
        elif 17 <= hour < 20:
            return "evening"
        elif 20 <= hour < 23:
            return "late_evening"
        else:
            return "night"
    
    def _assess_complexity(self, context: Dict[str, Any]) -> int:
        """√âvaluation de la complexit√© de la sc√®ne (1-5)."""
        
        complexity = 1
        
        # Nombre d'√©l√©ments d√©tect√©s
        detections = context.get("detections_count", 0)
        if detections > 5:
            complexity += 1
        if detections > 10:
            complexity += 1
        
        # Diversit√© des classes d'objets
        classes = context.get("detection_classes", [])
        if len(set(classes)) > 3:
            complexity += 1
        
        # Pr√©sence de personnes multiples
        if context.get("person_count", 0) > 1:
            complexity += 1
        
        return min(5, complexity)
    
    async def _generate_tool_call_plan(
        self,
        request: AnalysisRequest,
        initial_analysis: Dict[str, Any],
        optimize_selection: bool
    ) -> ToolCallPlan:
        """G√©n√©ration du plan d'appels d'outils."""
        
        available_tools = list(self.tool_registry.keys())
        selected_tools = []
        
        if optimize_selection:
            selected_tools = self._optimize_tool_selection(
                initial_analysis, available_tools
            )
        else:
            # S√©lection bas√©e sur les r√®gles
            selected_tools = self._rule_based_tool_selection(initial_analysis)
        
        # Cr√©ation des appels d'outils
        tool_calls = []
        
        for tool_name in selected_tools:
            tool_call = ToolCall(
                tool_name=tool_name,
                parameters=self._generate_tool_parameters(tool_name, request, initial_analysis),
                reasoning=self._generate_tool_reasoning(tool_name, initial_analysis),
                priority=self._calculate_tool_priority(tool_name, initial_analysis),
                timeout_seconds=self._get_tool_timeout(tool_name)
            )
            tool_calls.append(tool_call)
        
        # Tri par priorit√©
        tool_calls.sort(key=lambda tc: tc.priority)
        
        # Strat√©gie d'ex√©cution
        execution_strategy = self._determine_execution_strategy(tool_calls, initial_analysis)
        
        # Estimation du temps total
        if execution_strategy == "parallel":
            estimated_time = max(tc.timeout_seconds for tc in tool_calls)
        else:
            estimated_time = sum(tc.timeout_seconds for tc in tool_calls)
        
        return ToolCallPlan(
            tool_calls=tool_calls,
            execution_strategy=execution_strategy,
            estimated_total_time=estimated_time,
            confidence_threshold=self.confidence_threshold
        )
    
    def _optimize_tool_selection(
        self,
        initial_analysis: Dict[str, Any],
        available_tools: List[str]
    ) -> List[str]:
        """S√©lection optimis√©e des outils bas√©e sur l'historique de performance."""
        
        priority_score = initial_analysis["priority_score"]
        recommended_depth = initial_analysis["recommended_analysis_depth"]
        
        # Score des outils bas√© sur performance historique et contexte
        tool_scores = {}
        
        for tool_name in available_tools:
            if tool_name in self.tool_performance_cache:
                metrics = self.tool_performance_cache[tool_name]
                
                # Score de base (performance historique)
                base_score = (
                    metrics["success_rate"] * 0.4 +
                    (1.0 - min(1.0, metrics["avg_execution_time"] / 5.0)) * 0.3 +
                    metrics["confidence_correlation"] * 0.3
                )
                
                # Bonus contextuel
                context_bonus = self._calculate_context_bonus(tool_name, initial_analysis)
                
                # Score de fra√Æcheur (favorise outils r√©cemment utilis√©s avec succ√®s)
                freshness_bonus = self._calculate_freshness_bonus(tool_name)
                
                tool_scores[tool_name] = base_score + context_bonus + freshness_bonus
            else:
                # Nouveau outil - score neutre
                tool_scores[tool_name] = 0.5
        
        # S√©lection selon le niveau recommand√©
        sorted_tools = sorted(tool_scores.items(), key=lambda x: x[1], reverse=True)
        
        if recommended_depth == "thorough":
            selected = [tool for tool, score in sorted_tools[:6] if score > 0.4]
        else:
            selected = [tool for tool, score in sorted_tools[:4] if score > 0.5]
        
        # Assurer les outils essentiels
        essential_tools = ["object_detection", "context_analysis"]
        for tool in essential_tools:
            if tool in available_tools and tool not in selected:
                selected.append(tool)
        
        return selected[:self.max_parallel_tools]
    
    def _rule_based_tool_selection(self, initial_analysis: Dict[str, Any]) -> List[str]:
        """S√©lection d'outils bas√©e sur des r√®gles."""
        
        selected_tools = ["object_detection", "context_analysis"]  # Toujours inclus
        
        elements = initial_analysis["elements_detected"]
        risk_indicators = initial_analysis["risk_indicators"]
        
        # Ajout selon les √©l√©ments d√©tect√©s
        if elements["persons"] > 0:
            selected_tools.append("pose_analysis")
            selected_tools.append("behavior_assessment")
        
        # Ajout selon les indicateurs de risque
        if "high_value_area" in risk_indicators:
            selected_tools.append("risk_evaluation")
        
        if "crowded_scene" in risk_indicators:
            selected_tools.append("spatial_analysis")
        
        if "off_hours" in risk_indicators:
            selected_tools.append("temporal_analysis")
        
        # Validation de confiance si situation complexe
        if initial_analysis["priority_score"] >= 3:
            selected_tools.append("confidence_validation")
        
        return list(set(selected_tools))  # D√©duplication
    
    def _calculate_context_bonus(self, tool_name: str, initial_analysis: Dict[str, Any]) -> float:
        """Calcul du bonus contextuel pour un outil."""
        
        bonus = 0.0
        
        elements = initial_analysis["elements_detected"]
        risk_indicators = initial_analysis["risk_indicators"]
        
        # Bonus sp√©cifiques par outil
        if tool_name == "pose_analysis" and elements["persons"] > 0:
            bonus += 0.2
        
        if tool_name == "risk_evaluation" and "high_value_area" in risk_indicators:
            bonus += 0.3
        
        if tool_name == "spatial_analysis" and "crowded_scene" in risk_indicators:
            bonus += 0.2
        
        if tool_name == "temporal_analysis" and "off_hours" in risk_indicators:
            bonus += 0.25
        
        return min(0.3, bonus)  # Bonus max de 0.3
    
    def _calculate_freshness_bonus(self, tool_name: str) -> float:
        """Calcul du bonus de fra√Æcheur."""
        
        if tool_name not in self.tool_performance_cache:
            return 0.0
        
        last_used = self.tool_performance_cache[tool_name]["last_used"]
        time_since_use = datetime.now() - last_used
        
        # Bonus d√©gressif selon le temps √©coul√©
        if time_since_use < timedelta(hours=1):
            return 0.1
        elif time_since_use < timedelta(hours=6):
            return 0.05
        else:
            return 0.0
    
    def _generate_tool_parameters(
        self,
        tool_name: str,
        request: AnalysisRequest,
        initial_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """G√©n√©ration des param√®tres pour un outil."""
        
        base_params = {
            "frame_data": request.frame_data,
            "context": request.context or {},
            "initial_analysis": initial_analysis
        }
        
        # Param√®tres sp√©cifiques par outil
        tool_specific_params = {
            "object_detection": {
                "confidence_threshold": 0.5,
                "filter_classes": ["person", "handbag", "backpack"]
            },
            "pose_analysis": {
                "focus_on_hands": True,
                "detect_suspicious_poses": True
            },
            "behavior_assessment": {
                "analysis_window": 30,
                "behavior_patterns": ["loitering", "erratic_movement", "concealment"]
            },
            "risk_evaluation": {
                "location_context": initial_analysis["elements_detected"]["location_type"],
                "time_context": initial_analysis["elements_detected"]["time_period"]
            },
            "temporal_analysis": {
                "look_back_frames": 10,
                "pattern_detection": True
            },
            "spatial_analysis": {
                "zone_analysis": True,
                "crowd_density": True
            }
        }
        
        if tool_name in tool_specific_params:
            base_params.update(tool_specific_params[tool_name])
        
        return base_params
    
    def _generate_tool_reasoning(self, tool_name: str, initial_analysis: Dict[str, Any]) -> str:
        """G√©n√©ration du raisonnement pour l'appel d'outil."""
        
        reasoning_templates = {
            "object_detection": "D√©tection d'objets n√©cessaire pour identifier les √©l√©ments pr√©sents",
            "pose_analysis": "Analyse des poses corporelles pour d√©tecter comportements suspects",
            "behavior_assessment": "√âvaluation comportementale pour identifier activit√©s anormales",
            "context_analysis": "Analyse contextuelle pour comprendre l'environnement",
            "risk_evaluation": "√âvaluation des risques selon le contexte spatio-temporel",
            "temporal_analysis": "Analyse temporelle pour d√©tecter patterns suspects",
            "spatial_analysis": "Analyse spatiale pour √©valuer densit√© et mouvement",
            "confidence_validation": "Validation de confiance pour situation complexe"
        }
        
        base_reasoning = reasoning_templates.get(
            tool_name, 
            f"Analyse sp√©cialis√©e avec {tool_name}"
        )
        
        # Enrichissement contextuel
        risk_indicators = initial_analysis["risk_indicators"]
        if risk_indicators:
            base_reasoning += f" (Indicateurs: {', '.join(risk_indicators)})"
        
        return base_reasoning
    
    def _calculate_tool_priority(self, tool_name: str, initial_analysis: Dict[str, Any]) -> int:
        """Calcul de la priorit√© d'un outil (1=haute, 3=basse)."""
        
        # Priorit√©s par d√©faut
        default_priorities = {
            "object_detection": 1,  # Toujours haute priorit√©
            "context_analysis": 1,
            "pose_analysis": 2,
            "behavior_assessment": 2,
            "risk_evaluation": 2,
            "temporal_analysis": 3,
            "spatial_analysis": 3,
            "confidence_validation": 3
        }
        
        base_priority = default_priorities.get(tool_name, 2)
        
        # Ajustement selon le contexte
        risk_indicators = initial_analysis["risk_indicators"]
        priority_score = initial_analysis["priority_score"]
        
        # Augmentation de priorit√© si situation critique
        if priority_score >= 4:
            base_priority = max(1, base_priority - 1)
        
        # Priorit√©s sp√©cifiques selon indicateurs de risque
        if "high_value_area" in risk_indicators and tool_name == "risk_evaluation":
            base_priority = 1
        
        if "crowded_scene" in risk_indicators and tool_name == "spatial_analysis":
            base_priority = 2
        
        return base_priority
    
    def _get_tool_timeout(self, tool_name: str) -> float:
        """R√©cup√©ration du timeout pour un outil."""
        
        # Timeouts par d√©faut selon la complexit√© de l'outil
        default_timeouts = {
            "object_detection": 2.0,
            "context_analysis": 1.5,
            "pose_analysis": 3.0,
            "behavior_assessment": 4.0,
            "risk_evaluation": 2.5,
            "temporal_analysis": 3.5,
            "spatial_analysis": 3.0,
            "confidence_validation": 2.0
        }
        
        return default_timeouts.get(tool_name, 3.0)
    
    def _determine_execution_strategy(
        self,
        tool_calls: List[ToolCall],
        initial_analysis: Dict[str, Any]
    ) -> str:
        """D√©termination de la strat√©gie d'ex√©cution."""
        
        priority_score = initial_analysis["priority_score"]
        
        # Parall√®le si situation critique et outils ind√©pendants
        if priority_score >= 4 and len(tool_calls) <= self.max_parallel_tools:
            return "parallel"
        
        # S√©quentiel si d√©pendances ou situation standard
        if self._has_tool_dependencies(tool_calls):
            return "sequential"
        
        return "parallel" if len(tool_calls) <= 3 else "sequential"
    
    def _has_tool_dependencies(self, tool_calls: List[ToolCall]) -> bool:
        """V√©rification des d√©pendances entre outils."""
        
        # D√©pendances simples (√† √©tendre selon besoins)
        dependencies = {
            "behavior_assessment": ["object_detection", "pose_analysis"],
            "confidence_validation": ["risk_evaluation"]
        }
        
        tool_names = [tc.tool_name for tc in tool_calls]
        
        for tool_name in tool_names:
            if tool_name in dependencies:
                for dependency in dependencies[tool_name]:
                    if dependency in tool_names:
                        return True
        
        return False
    
    async def _execute_tool_call_plan(self, plan: ToolCallPlan) -> List[ToolCallResult]:
        """Ex√©cution du plan d'appels d'outils."""
        
        results = []
        
        if plan.execution_strategy == "parallel":
            # Ex√©cution parall√®le
            tasks = []
            for tool_call in plan.tool_calls:
                task = self._execute_single_tool_call(tool_call)
                tasks.append(task)
            
            parallel_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(parallel_results):
                if isinstance(result, Exception):
                    # Gestion d'erreur
                    error_result = ToolCallResult(
                        tool_call=plan.tool_calls[i],
                        success=False,
                        error_message=str(result)
                    )
                    results.append(error_result)
                else:
                    results.append(result)
        
        else:
            # Ex√©cution s√©quentielle
            for tool_call in plan.tool_calls:
                result = await self._execute_single_tool_call(tool_call)
                results.append(result)
                
                # Arr√™t si outil critique √©choue
                if not result.success and tool_call.priority == 1:
                    logger.warning(f"Outil critique {tool_call.tool_name} √©chou√© - Arr√™t du plan")
                    break
        
        return results
    
    async def _execute_single_tool_call(self, tool_call: ToolCall) -> ToolCallResult:
        """Ex√©cution d'un appel d'outil unique."""
        
        start_time = time.time()
        
        try:
            # V√©rification du cache
            if self.tool_cache_enabled:
                cache_key = self._generate_cache_key(tool_call)
                cached_result = self._get_cached_result(cache_key)
                
                if cached_result is not None:
                    logger.debug(f"R√©sultat depuis cache pour {tool_call.tool_name}")
                    return ToolCallResult(
                        tool_call=tool_call,
                        success=True,
                        result=cached_result,
                        execution_time=0.001,  # Cache hit
                        confidence=0.9  # Confiance √©lev√©e pour cache
                    )
            
            # Ex√©cution de l'outil
            if tool_call.tool_name not in self.tool_registry:
                raise ValueError(f"Outil non enregistr√©: {tool_call.tool_name}")
            
            tool_function = self.tool_registry[tool_call.tool_name]
            
            # Ex√©cution avec timeout
            result = await asyncio.wait_for(
                tool_function(**tool_call.parameters),
                timeout=tool_call.timeout_seconds
            )
            
            execution_time = time.time() - start_time
            
            # √âvaluation de la confiance du r√©sultat
            confidence = self._evaluate_result_confidence(tool_call, result, execution_time)
            
            # Mise en cache si r√©ussite
            if self.tool_cache_enabled and confidence > 0.7:
                cache_key = self._generate_cache_key(tool_call)
                self._cache_result(cache_key, result)
            
            return ToolCallResult(
                tool_call=tool_call,
                success=True,
                result=result,
                execution_time=execution_time,
                confidence=confidence
            )
        
        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            logger.warning(f"Timeout pour {tool_call.tool_name} apr√®s {tool_call.timeout_seconds}s")
            
            return ToolCallResult(
                tool_call=tool_call,
                success=False,
                error_message=f"Timeout apr√®s {tool_call.timeout_seconds}s",
                execution_time=execution_time
            )
        
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Erreur ex√©cution {tool_call.tool_name}: {e}")
            
            return ToolCallResult(
                tool_call=tool_call,
                success=False,
                error_message=str(e),
                execution_time=execution_time
            )
    
    def _generate_cache_key(self, tool_call: ToolCall) -> str:
        """G√©n√©ration d'une cl√© de cache pour un appel d'outil."""
        
        # S√©rialisation des param√®tres (exclut frame_data pour √©viter cl√©s trop longues)
        cache_params = tool_call.parameters.copy()
        if "frame_data" in cache_params:
            # Hash du frame_data au lieu de la valeur compl√®te
            frame_hash = hash(cache_params["frame_data"][:100]) if cache_params["frame_data"] else 0
            cache_params["frame_data_hash"] = frame_hash
            del cache_params["frame_data"]
        
        params_str = json.dumps(cache_params, sort_keys=True, default=str)
        cache_key = f"{tool_call.tool_name}:{hash(params_str)}"
        
        return cache_key
    
    def _get_cached_result(self, cache_key: str) -> Optional[Any]:
        """R√©cup√©ration d'un r√©sultat depuis le cache."""
        
        if cache_key in self.tool_results_cache:
            result, timestamp = self.tool_results_cache[cache_key]
            
            # V√©rification de l'expiration
            if datetime.now() - timestamp < self.cache_ttl:
                return result
            else:
                # Nettoyage du cache expir√©
                del self.tool_results_cache[cache_key]
        
        return None
    
    def _cache_result(self, cache_key: str, result: Any) -> None:
        """Mise en cache d'un r√©sultat."""
        
        self.tool_results_cache[cache_key] = (result, datetime.now())
        
        # Nettoyage p√©riodique du cache
        if len(self.tool_results_cache) % 100 == 0:
            self._cleanup_expired_cache()
    
    def _cleanup_expired_cache(self) -> None:
        """Nettoyage du cache expir√©."""
        
        current_time = datetime.now()
        expired_keys = []
        
        for key, (_, timestamp) in self.tool_results_cache.items():
            if current_time - timestamp > self.cache_ttl:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.tool_results_cache[key]
        
        if expired_keys:
            logger.debug(f"Cache nettoy√©: {len(expired_keys)} entr√©es expir√©es supprim√©es")
    
    def _evaluate_result_confidence(
        self,
        tool_call: ToolCall,
        result: Any,
        execution_time: float
    ) -> float:
        """√âvaluation de la confiance d'un r√©sultat d'outil."""
        
        confidence = 0.5  # Base
        
        # Bonus si ex√©cution rapide (dans les temps)
        if execution_time < tool_call.timeout_seconds * 0.5:
            confidence += 0.2
        
        # Bonus selon le type de r√©sultat
        if isinstance(result, dict):
            if "confidence" in result and isinstance(result["confidence"], (int, float)):
                confidence = max(confidence, result["confidence"])
            
            if "error" not in result:
                confidence += 0.1
        
        # Bonus selon l'historique de l'outil
        if tool_call.tool_name in self.tool_performance_cache:
            historical_success = self.tool_performance_cache[tool_call.tool_name]["success_rate"]
            confidence = confidence * 0.7 + historical_success * 0.3
        
        return min(1.0, max(0.0, confidence))
    
    async def _synthesize_with_tool_results(
        self,
        request: AnalysisRequest,
        initial_analysis: Dict[str, Any],
        tool_results: List[ToolCallResult]
    ) -> AnalysisResponse:
        """Synth√®se finale avec int√©gration des r√©sultats d'outils."""
        
        # Extraction des r√©sultats r√©ussis
        successful_results = [tr for tr in tool_results if tr.success]
        failed_results = [tr for tr in tool_results if not tr.success]
        
        # Agr√©gation des informations
        aggregated_info = {
            "tools_used": [tr.tool_call.tool_name for tr in successful_results],
            "tools_failed": [tr.tool_call.tool_name for tr in failed_results],
            "total_execution_time": sum(tr.execution_time for tr in tool_results),
            "avg_confidence": np.mean([tr.confidence for tr in successful_results]) if successful_results else 0.0
        }
        
        # Analyse du niveau de suspicion bas√©e sur les outils
        suspicion_level = self._determine_suspicion_from_tools(successful_results, initial_analysis)
        
        # D√©termination du type d'action
        action_type = self._determine_action_from_analysis(suspicion_level, successful_results)
        
        # Calcul de la confiance globale
        global_confidence = self._calculate_global_confidence(
            successful_results, failed_results, initial_analysis
        )
        
        # G√©n√©ration de la description
        description = self._generate_analysis_description(
            suspicion_level, successful_results, aggregated_info
        )
        
        # G√©n√©ration des recommandations
        recommendations = self._generate_recommendations(
            suspicion_level, action_type, successful_results, failed_results
        )
        
        return AnalysisResponse(
            suspicion_level=suspicion_level,
            action_type=action_type,
            confidence=global_confidence,
            description=description,
            tools_used=aggregated_info["tools_used"],
            recommendations=recommendations
        )
    
    def _determine_suspicion_from_tools(
        self,
        successful_results: List[ToolCallResult],
        initial_analysis: Dict[str, Any]
    ) -> SuspicionLevel:
        """D√©termination du niveau de suspicion bas√© sur les r√©sultats d'outils."""
        
        suspicion_indicators = []
        
        for result in successful_results:
            tool_name = result.tool_call.tool_name
            tool_result = result.result
            
            if isinstance(tool_result, dict):
                # Analyse des indicateurs sp√©cifiques par outil
                if tool_name == "behavior_assessment":
                    if tool_result.get("suspicious_behavior", False):
                        suspicion_indicators.append("suspicious_behavior")
                
                if tool_name == "risk_evaluation":
                    risk_score = tool_result.get("risk_score", 0)
                    if risk_score > 0.7:
                        suspicion_indicators.append("high_risk_context")
                
                if tool_name == "pose_analysis":
                    if tool_result.get("concealment_detected", False):
                        suspicion_indicators.append("concealment_behavior")
                
                if tool_name == "temporal_analysis":
                    if tool_result.get("unusual_timing", False):
                        suspicion_indicators.append("temporal_anomaly")
        
        # D√©termination du niveau selon les indicateurs
        indicator_count = len(suspicion_indicators)
        priority_score = initial_analysis["priority_score"]
        
        if indicator_count >= 3 or priority_score >= 4:
            return SuspicionLevel.CRITICAL
        elif indicator_count >= 2 or priority_score >= 3:
            return SuspicionLevel.HIGH
        elif indicator_count >= 1 or priority_score >= 2:
            return SuspicionLevel.MEDIUM
        else:
            return SuspicionLevel.LOW
    
    def _determine_action_from_analysis(
        self,
        suspicion_level: SuspicionLevel,
        successful_results: List[ToolCallResult]
    ) -> ActionType:
        """D√©termination du type d'action recommand√©."""
        
        # Analyse des r√©sultats pour indices sp√©cifiques
        theft_indicators = 0
        suspicious_indicators = 0
        
        for result in successful_results:
            if isinstance(result.result, dict):
                tool_result = result.result
                
                # Indicateurs de vol
                if tool_result.get("concealment_detected", False):
                    theft_indicators += 1
                
                if tool_result.get("suspicious_behavior", False):
                    suspicious_indicators += 1
                
                if tool_result.get("high_risk_context", False):
                    theft_indicators += 1
        
        # D√©termination de l'action
        if suspicion_level == SuspicionLevel.CRITICAL or theft_indicators >= 2:
            return ActionType.THEFT_ATTEMPT
        elif suspicion_level == SuspicionLevel.HIGH or suspicious_indicators >= 2:
            return ActionType.SUSPICIOUS_ACTIVITY
        elif suspicion_level == SuspicionLevel.MEDIUM:
            return ActionType.LOITERING
        else:
            return ActionType.NORMAL_SHOPPING
    
    def _calculate_global_confidence(
        self,
        successful_results: List[ToolCallResult],
        failed_results: List[ToolCallResult],
        initial_analysis: Dict[str, Any]
    ) -> float:
        """Calcul de la confiance globale de l'analyse."""
        
        if not successful_results:
            return 0.1  # Tr√®s faible confiance si aucun outil r√©ussi
        
        # Confiance moyenne des outils r√©ussis
        tools_confidence = np.mean([tr.confidence for tr in successful_results])
        
        # P√©nalit√© pour outils √©chou√©s
        failure_penalty = len(failed_results) * 0.1
        
        # Bonus pour coh√©rence (si plusieurs outils donnent r√©sultats similaires)
        coherence_bonus = self._calculate_coherence_bonus(successful_results)
        
        # Ajustement selon complexit√© initiale
        complexity_factor = 1.0 - (initial_analysis["priority_score"] * 0.05)
        
        global_confidence = (
            tools_confidence * complexity_factor + 
            coherence_bonus - 
            failure_penalty
        )
        
        return min(1.0, max(0.1, global_confidence))
    
    def _calculate_coherence_bonus(self, successful_results: List[ToolCallResult]) -> float:
        """Calcul du bonus de coh√©rence entre outils."""
        
        if len(successful_results) < 2:
            return 0.0
        
        # Analyse de la coh√©rence des r√©sultats
        coherent_indicators = 0
        total_comparisons = 0
        
        for i, result1 in enumerate(successful_results):
            for result2 in successful_results[i+1:]:
                total_comparisons += 1
                
                # V√©rification de coh√©rence simple
                if (isinstance(result1.result, dict) and isinstance(result2.result, dict)):
                    r1, r2 = result1.result, result2.result
                    
                    # Coh√©rence des niveaux de confiance
                    if abs(r1.get("confidence", 0.5) - r2.get("confidence", 0.5)) < 0.3:
                        coherent_indicators += 1
        
        if total_comparisons > 0:
            coherence_ratio = coherent_indicators / total_comparisons
            return coherence_ratio * 0.2  # Bonus max de 0.2
        
        return 0.0
    
    def _generate_analysis_description(
        self,
        suspicion_level: SuspicionLevel,
        successful_results: List[ToolCallResult],
        aggregated_info: Dict[str, Any]
    ) -> str:
        """G√©n√©ration de la description de l'analyse."""
        
        tools_summary = f"Analyse effectu√©e avec {len(successful_results)} outils"
        
        if aggregated_info["tools_failed"]:
            tools_summary += f" ({len(aggregated_info['tools_failed'])} √©checs)"
        
        suspicion_desc = {
            SuspicionLevel.LOW: "Situation normale d√©tect√©e",
            SuspicionLevel.MEDIUM: "Activit√© potentiellement suspecte observ√©e",
            SuspicionLevel.HIGH: "Comportement suspect d√©tect√©",
            SuspicionLevel.CRITICAL: "Situation critique identifi√©e"
        }
        
        base_description = suspicion_desc.get(suspicion_level, "Analyse effectu√©e")
        
        # Ajout d'informations sp√©cifiques des outils
        specific_info = []
        for result in successful_results:
            if isinstance(result.result, dict) and "summary" in result.result:
                specific_info.append(result.result["summary"])
        
        full_description = f"{base_description}. {tools_summary}."
        
        if specific_info:
            full_description += f" D√©tails: {'; '.join(specific_info[:2])}."
        
        return full_description
    
    def _generate_recommendations(
        self,
        suspicion_level: SuspicionLevel,
        action_type: ActionType,
        successful_results: List[ToolCallResult],
        failed_results: List[ToolCallResult]
    ) -> List[str]:
        """G√©n√©ration des recommandations."""
        
        recommendations = []
        
        # Recommandations selon le niveau de suspicion
        if suspicion_level == SuspicionLevel.CRITICAL:
            recommendations.extend([
                "Alerte imm√©diate du personnel de s√©curit√©",
                "D√©marrage de l'enregistrement vid√©o",
                "Surveillance rapproch√©e de la zone"
            ])
        elif suspicion_level == SuspicionLevel.HIGH:
            recommendations.extend([
                "Surveillance accrue de la zone",
                "Notification du superviseur",
                "Conservation des images"
            ])
        elif suspicion_level == SuspicionLevel.MEDIUM:
            recommendations.extend([
                "Observation continue",
                "V√©rification contextuelle"
            ])
        
        # Recommandations selon les √©checs d'outils
        if failed_results:
            recommendations.append("V√©rification du syst√®me d'analyse recommand√©e")
        
        # Recommandations sp√©cifiques des outils
        for result in successful_results:
            if isinstance(result.result, dict) and "recommendations" in result.result:
                tool_recs = result.result["recommendations"]
                if isinstance(tool_recs, list):
                    recommendations.extend(tool_recs[:2])  # Max 2 par outil
        
        return list(set(recommendations))  # D√©duplication
    
    def _update_tool_performance_metrics(self, tool_results: List[ToolCallResult]) -> None:
        """Mise √† jour des m√©triques de performance des outils."""
        
        for result in tool_results:
            tool_name = result.tool_call.tool_name
            
            if tool_name in self.tool_performance_cache:
                metrics = self.tool_performance_cache[tool_name]
                
                # Mise √† jour du compte d'utilisation
                metrics["usage_count"] += 1
                
                # Mise √† jour du taux de succ√®s (moyenne mobile)
                alpha = 0.1
                success = 1.0 if result.success else 0.0
                metrics["success_rate"] = (
                    metrics["success_rate"] * (1 - alpha) + success * alpha
                )
                
                # Mise √† jour du temps d'ex√©cution moyen
                metrics["avg_execution_time"] = (
                    metrics["avg_execution_time"] * (1 - alpha) + 
                    result.execution_time * alpha
                )
                
                # Mise √† jour de la corr√©lation de confiance
                if result.success and result.confidence > 0:
                    metrics["confidence_correlation"] = (
                        metrics["confidence_correlation"] * (1 - alpha) + 
                        result.confidence * alpha
                    )
                
                metrics["last_used"] = datetime.now()
                
                # Ajout √† l'historique
                self.tool_call_history.append(result)
                
                # Limitation de l'historique
                if len(self.tool_call_history) > 1000:
                    self.tool_call_history = self.tool_call_history[-500:]
    
    def get_tool_calling_stats(self) -> Dict[str, Any]:
        """Statistiques du syst√®me de tool calling."""
        
        total_calls = len(self.tool_call_history)
        successful_calls = len([r for r in self.tool_call_history if r.success])
        
        # Statistiques par outil
        tool_stats = {}
        for tool_name, metrics in self.tool_performance_cache.items():
            tool_stats[tool_name] = {
                "usage_count": metrics["usage_count"],
                "success_rate": f"{metrics['success_rate']:.1%}",
                "avg_execution_time": f"{metrics['avg_execution_time']:.2f}s",
                "confidence_correlation": f"{metrics['confidence_correlation']:.2f}"
            }
        
        return {
            "total_tool_calls": total_calls,
            "success_rate": f"{successful_calls/max(total_calls, 1):.1%}",
            "cache_entries": len(self.tool_results_cache),
            "registered_tools": len(self.tool_registry),
            "tool_performance": tool_stats
        }
    
    # Impl√©mentations des outils par d√©faut (stubs - √† impl√©menter selon besoins)
    
    async def _tool_object_detection(self, **kwargs) -> Dict[str, Any]:
        """Outil de d√©tection d'objets."""
        await asyncio.sleep(0.1)  # Simulation
        return {
            "objects_detected": ["person", "handbag"],
            "confidence": 0.85,
            "summary": "2 objets d√©tect√©s"
        }
    
    async def _tool_pose_analysis(self, **kwargs) -> Dict[str, Any]:
        """Outil d'analyse des poses."""
        await asyncio.sleep(0.2)
        return {
            "poses_detected": ["standing", "walking"],
            "concealment_detected": False,
            "confidence": 0.7,
            "summary": "Poses normales d√©tect√©es"
        }
    
    async def _tool_behavior_assessment(self, **kwargs) -> Dict[str, Any]:
        """Outil d'√©valuation comportementale."""
        await asyncio.sleep(0.3)
        return {
            "suspicious_behavior": False,
            "behavior_type": "normal_shopping",
            "confidence": 0.8,
            "summary": "Comportement normal observ√©"
        }
    
    async def _tool_context_analysis(self, **kwargs) -> Dict[str, Any]:
        """Outil d'analyse contextuelle."""
        await asyncio.sleep(0.1)
        return {
            "context_type": "retail_environment",
            "time_appropriate": True,
            "confidence": 0.9,
            "summary": "Contexte environnemental normal"
        }
    
    async def _tool_risk_evaluation(self, **kwargs) -> Dict[str, Any]:
        """Outil d'√©valuation des risques."""
        await asyncio.sleep(0.2)
        return {
            "risk_score": 0.3,
            "risk_factors": [],
            "confidence": 0.75,
            "summary": "Niveau de risque faible"
        }
    
    async def _tool_temporal_analysis(self, **kwargs) -> Dict[str, Any]:
        """Outil d'analyse temporelle."""
        await asyncio.sleep(0.2)
        return {
            "temporal_patterns": [],
            "unusual_timing": False,
            "confidence": 0.6,
            "summary": "Aucune anomalie temporelle"
        }
    
    async def _tool_spatial_analysis(self, **kwargs) -> Dict[str, Any]:
        """Outil d'analyse spatiale."""
        await asyncio.sleep(0.25)
        return {
            "spatial_anomalies": [],
            "crowd_density": "normal",
            "confidence": 0.65,
            "summary": "Distribution spatiale normale"
        }
    
    async def _tool_confidence_validation(self, **kwargs) -> Dict[str, Any]:
        """Outil de validation de confiance."""
        await asyncio.sleep(0.15)
        return {
            "validation_passed": True,
            "confidence_score": 0.8,
            "confidence": 0.85,
            "summary": "Validation de confiance r√©ussie"
        }


# Factory function
def create_tool_calling_vlm(
    model_name: str = "kimi-vl-a3b-thinking",
    enable_caching: bool = True
) -> ToolCallingVLM:
    """Factory pour cr√©er un VLM avec tool calling."""
    
    return ToolCallingVLM(
        default_model=model_name,
        enable_fallback=True,
        tool_cache_enabled=enable_caching
    )