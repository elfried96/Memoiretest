#!/usr/bin/env python3
"""
üß™ Test VLM Chatbot Symbiosis
============================

Script de test pour valider la symbiose VLM-Chatbot avec thinking/reasoning.
"""

import asyncio
import sys
from pathlib import Path
from loguru import logger

# Ajout chemin
sys.path.append(str(Path(__file__).parent / "dashboard"))

try:
    from vlm_chatbot_symbiosis import get_vlm_chatbot, process_vlm_chat_query
    from real_pipeline_integration import initialize_real_pipeline, get_real_pipeline
    IMPORTS_OK = True
except ImportError as e:
    logger.error(f"Erreur imports: {e}")
    IMPORTS_OK = False


async def test_vlm_chatbot_symbiosis():
    """Test complet de la symbiose VLM-Chatbot."""
    
    if not IMPORTS_OK:
        print("‚ùå Imports √©chou√©s - Test impossible")
        return False
    
    print("üß™ Test VLM Chatbot Symbiosis")
    print("=" * 50)
    
    # 1. Test initialization
    print("\n1Ô∏è‚É£ Test initialisation chatbot...")
    try:
        chatbot = get_vlm_chatbot()
        print(f"‚úÖ Chatbot initialis√©: {type(chatbot)}")
        print(f"   - Pipeline connect√©e: {chatbot.pipeline is not None}")
        print(f"   - Thinking activ√©: {chatbot.thinking_enabled}")
        print(f"   - Reasoning activ√©: {chatbot.reasoning_enabled}")
    except Exception as e:
        print(f"‚ùå Erreur initialisation: {e}")
        return False
    
    # 2. Test pipeline connection
    print("\n2Ô∏è‚É£ Test connexion pipeline VLM...")
    try:
        # Tentative initialisation pipeline (peut √©chouer sans GPU)
        pipeline_success = initialize_real_pipeline(
            vlm_model_name="Qwen/Qwen2.5-VL-7B-Instruct",  # Plus l√©ger pour test
            enable_optimization=False,
            max_concurrent_analysis=1
        )
        
        if pipeline_success:
            print("‚úÖ Pipeline VLM initialis√©e")
            pipeline = get_real_pipeline()
            chatbot.pipeline = pipeline
        else:
            print("‚ö†Ô∏è Pipeline VLM indisponible - Test en mode simulation")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Pipeline VLM non disponible: {e}")
        print("   Continuing with simulation mode...")
    
    # 3. Test questions basiques
    print("\n3Ô∏è‚É£ Test questions chatbot...")
    
    test_questions = [
        ("Quelle est la performance actuelle du syst√®me ?", "surveillance"),
        ("Quels outils VLM sont optimaux ?", "surveillance"),
        ("R√©sume les d√©tections r√©centes", "surveillance"),
        ("Comment optimiser la configuration ?", "video"),
        ("Explique le processus de thinking VLM", "surveillance")
    ]
    
    # Contexte simul√©
    mock_context = {
        "stats": {
            "frames_processed": 156,
            "current_performance_score": 0.87,
            "current_optimal_tools": ["sam2_segmentator", "dino_features", "pose_estimator"],
            "average_processing_time": 2.3,
            "optimization_cycles": 7,
            "total_detections": 23
        },
        "detections": [
            type('MockDetection', (), {
                'confidence': 0.91,
                'description': 'Personne avec comportement suspect pr√®s v√©hicule',
                'tools_used': ['sam2_segmentator', 'pose_estimator']
            })(),
            type('MockDetection', (), {
                'confidence': 0.76,
                'description': 'Objet dissimul√© d√©tect√©',
                'tools_used': ['dino_features', 'trajectory_analyzer']
            })()
        ],
        "optimizations": [
            {
                "best_combination": ["sam2_segmentator", "pose_estimator", "dino_features"],
                "performance_improvement": 0.12
            }
        ]
    }
    
    for i, (question, chat_type) in enumerate(test_questions, 1):
        print(f"\n   Question {i}: {question}")
        
        try:
            # Test r√©ponse VLM avec thinking
            response = await process_vlm_chat_query(
                question=question,
                chat_type=chat_type,
                vlm_context=mock_context
            )
            
            print(f"   ‚úÖ R√©ponse re√ßue (type: {response.get('type', 'unknown')})")
            print(f"   üìù Extrait: {response.get('response', '')[:100]}...")
            
            # V√©rification thinking si disponible
            if response.get('thinking'):
                print(f"   üß† Thinking d√©tect√©: {len(response['thinking'])} chars")
            
            if response.get('confidence'):
                print(f"   üìä Confiance: {response['confidence']:.1%}")
                
        except Exception as e:
            print(f"   ‚ùå Erreur question {i}: {e}")
    
    # 4. Test conversation history
    print("\n4Ô∏è‚É£ Test historique conversation...")
    try:
        summary = chatbot.get_conversation_summary()
        print(f"‚úÖ R√©sum√© conversation:")
        print(f"   - Total √©changes: {summary.get('total_exchanges', 0)}")
        print(f"   - R√©ponses VLM: {summary.get('vlm_responses', 0)}")
        print(f"   - R√©ponses fallback: {summary.get('fallback_responses', 0)}")
        print(f"   - Pipeline connect√©e: {summary.get('pipeline_connected', False)}")
    except Exception as e:
        print(f"‚ùå Erreur historique: {e}")
    
    # 5. Test performance
    print("\n5Ô∏è‚É£ Test performance chatbot...")
    
    import time
    start_time = time.time()
    
    try:
        # Test question complexe
        complex_response = await process_vlm_chat_query(
            question="Analyse compl√®te: performance, outils, optimisations et recommandations",
            chat_type="surveillance", 
            vlm_context=mock_context
        )
        
        processing_time = time.time() - start_time
        
        print(f"‚úÖ Question complexe trait√©e en {processing_time:.2f}s")
        print(f"   Type r√©ponse: {complex_response.get('type', 'unknown')}")
        print(f"   Longueur r√©ponse: {len(complex_response.get('response', ''))} chars")
        
    except Exception as e:
        print(f"‚ùå Erreur test performance: {e}")
    
    print("\nüéØ Test VLM Chatbot Symbiosis termin√©!")
    return True


if __name__ == "__main__":
    logger.remove()
    logger.add(sys.stdout, level="INFO")
    
    print("üöÄ D√©marrage tests VLM Chatbot Symbiosis")
    
    try:
        success = asyncio.run(test_vlm_chatbot_symbiosis())
        
        if success:
            print("\n‚úÖ Tests r√©ussis - VLM Chatbot Symbiosis op√©rationnel!")
            sys.exit(0)
        else:
            print("\n‚ùå Tests √©chou√©s")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Tests interrompus")
        sys.exit(0)
    except Exception as e:
        print(f"\nüí• Erreur critique: {e}")
        sys.exit(1)