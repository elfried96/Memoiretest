#!/usr/bin/env python3
"""
ğŸ–¥ï¸ Test Dashboard avec GPU - VÃ©rification Fonctionnelle
======================================================

Script pour tester le dashboard en conditions rÃ©elles GPU
et collecter des mÃ©triques de performance utilisateur.
"""

import os
import sys
import subprocess
import time
import psutil
import torch
from pathlib import Path

def check_gpu_availability():
    """VÃ©rification disponibilitÃ© GPU."""
    print("ğŸ” VÃ‰RIFICATION ENVIRONNEMENT GPU")
    print("=" * 40)
    
    # Check CUDA
    cuda_available = torch.cuda.is_available()
    print(f"ğŸ”¥ CUDA Disponible: {cuda_available}")
    
    if cuda_available:
        gpu_count = torch.cuda.device_count()
        print(f"ğŸ”¥ Nombre GPU: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory
            print(f"ğŸ”¥ GPU {i}: {gpu_name}")
            print(f"ğŸ”¥ MÃ©moire: {gpu_memory / 1e9:.1f} GB")
            
            # Test allocation mÃ©moire
            try:
                test_tensor = torch.randn(1000, 1000, device=f'cuda:{i}')
                print(f"âœ… GPU {i} fonctionnel")
                del test_tensor
                torch.cuda.empty_cache()
            except Exception as e:
                print(f"âŒ GPU {i} problÃ¨me: {e}")
    else:
        print("âš ï¸  Aucun GPU CUDA dÃ©tectÃ© - Mode CPU")
    
    # Check systÃ¨me
    memory = psutil.virtual_memory()
    print(f"ğŸ’¾ RAM SystÃ¨me: {memory.total / 1e9:.1f} GB")
    print(f"ğŸ’¾ RAM Libre: {memory.available / 1e9:.1f} GB")
    
    return cuda_available

def test_imports():
    """Test imports des modules principaux."""
    print("\nğŸ” TEST IMPORTS MODULES")
    print("=" * 30)
    
    modules_to_test = [
        'streamlit',
        'torch', 
        'transformers',
        'opencv-python',
        'numpy',
        'pandas',
        'plotly'
    ]
    
    success_count = 0
    for module in modules_to_test:
        try:
            __import__(module.replace('-', '_'))
            print(f"âœ… {module}")
            success_count += 1
        except ImportError as e:
            print(f"âŒ {module}: {e}")
    
    print(f"\nğŸ“Š Modules OK: {success_count}/{len(modules_to_test)}")
    return success_count == len(modules_to_test)

def check_project_structure():
    """VÃ©rification structure projet."""
    print("\nğŸ“ VÃ‰RIFICATION STRUCTURE PROJET")
    print("=" * 40)
    
    required_files = [
        'dashboard/production_dashboard.py',
        'dashboard/video_context_integration.py',
        'dashboard/vlm_chatbot_symbiosis.py',
        'src/core/vlm/dynamic_model.py',
        'src/core/vlm/prompt_builder.py',
        'src/core/orchestrator/vlm_orchestrator.py'
    ]
    
    missing_files = []
    for file_path in required_files:
        if Path(file_path).exists():
            print(f"âœ… {file_path}")
        else:
            print(f"âŒ {file_path} MANQUANT")
            missing_files.append(file_path)
    
    if missing_files:
        print(f"\nâš ï¸  {len(missing_files)} fichiers manquants:")
        for file in missing_files:
            print(f"   - {file}")
        return False
    
    print("âœ… Structure projet complÃ¨te")
    return True

def launch_dashboard_test():
    """Lance le dashboard pour test utilisateur."""
    print("\nğŸš€ LANCEMENT DASHBOARD TEST")
    print("=" * 35)
    
    dashboard_path = Path("dashboard/production_dashboard.py")
    
    if not dashboard_path.exists():
        print("âŒ Dashboard non trouvÃ©")
        return False
    
    print("ğŸŒ Lancement Streamlit Dashboard...")
    print("ğŸ“ URL: http://localhost:8501")
    print("\nğŸ”§ INSTRUCTIONS TEST:")
    print("1. Ouvrir http://localhost:8501 dans le navigateur")
    print("2. Tester l'onglet 'ğŸ¥ Surveillance VLM'")
    print("3. Tester l'onglet 'ğŸ“¤ Upload VidÃ©o VLM'")
    print("4. VÃ©rifier le chat VLM avec questions test")
    print("5. Mesurer temps de rÃ©ponse chat")
    print("6. Tester upload vidÃ©o avec descriptions contextuelles")
    print("\nâ¹ï¸  Ctrl+C pour arrÃªter le dashboard\n")
    
    try:
        # Lancement streamlit
        cmd = [
            sys.executable, "-m", "streamlit", "run", 
            str(dashboard_path),
            "--server.port", "8501",
            "--server.address", "localhost",
            "--server.headless", "false"
        ]
        
        process = subprocess.Popen(
            cmd, 
            cwd=Path.cwd(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        print("â³ Attente dÃ©marrage dashboard...")
        time.sleep(5)
        
        print("âœ… Dashboard dÃ©marrÃ© - Tests manuels possibles")
        print("ğŸ’¡ Presser ENTRÃ‰E pour arrÃªter le dashboard...")
        
        # Attente input utilisateur
        input()
        
        # ArrÃªt propre
        process.terminate()
        try:
            process.wait(timeout=5)
        except subprocess.TimeoutExpired:
            process.kill()
        
        print("â¹ï¸  Dashboard arrÃªtÃ©")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lancement dashboard: {e}")
        return False

def generate_test_commands():
    """GÃ©nÃ¨re commandes de test recommandÃ©es."""
    print("\nğŸ“‹ COMMANDES TEST RECOMMANDÃ‰ES")
    print("=" * 40)
    
    commands = [
        {
            'description': 'ğŸ§ª Test Performance Complet',
            'command': 'python run_performance_tests.py',
            'purpose': 'MÃ©triques complÃ¨tes pour mÃ©moire'
        },
        {
            'description': 'ğŸ¥ Test Contexte VidÃ©o',
            'command': 'python test_video_context_integration.py',
            'purpose': 'Validation intÃ©gration descriptions'
        },
        {
            'description': 'ğŸ¤– Test Chatbot VLM',
            'command': 'python test_vlm_chatbot.py',
            'purpose': 'Test chatbot avec GPU'
        },
        {
            'description': 'ğŸ–¥ï¸ Dashboard Production',
            'command': 'streamlit run dashboard/production_dashboard.py',
            'purpose': 'Interface utilisateur complÃ¨te'
        }
    ]
    
    for i, cmd in enumerate(commands, 1):
        print(f"{i}. {cmd['description']}")
        print(f"   Command: {cmd['command']}")
        print(f"   But: {cmd['purpose']}")
        print()

def main():
    """Test principal."""
    print("ğŸš€ TEST DASHBOARD GPU - ENVIRONNEMENT COMPLET")
    print("=" * 60)
    
    # Tests prÃ©liminaires
    gpu_ok = check_gpu_availability()
    imports_ok = test_imports()
    structure_ok = check_project_structure()
    
    print(f"\nğŸ“Š RÃ‰SUMÃ‰ VÃ‰RIFICATIONS:")
    print(f"   ğŸ”¥ GPU: {'âœ…' if gpu_ok else 'âŒ'}")
    print(f"   ğŸ“¦ Imports: {'âœ…' if imports_ok else 'âŒ'}")
    print(f"   ğŸ“ Structure: {'âœ…' if structure_ok else 'âŒ'}")
    
    if not all([imports_ok, structure_ok]):
        print("\nâŒ Tests prÃ©liminaires Ã©chouÃ©s")
        print("ğŸ”§ Corrigez les problÃ¨mes avant de continuer")
        return
    
    # Proposition tests
    print(f"\nğŸ¯ ENVIRONNEMENT {'PRÃŠT' if gpu_ok else 'PARTIELLEMENT PRÃŠT'}")
    
    generate_test_commands()
    
    # Proposition lancement dashboard
    launch_dashboard = input("ğŸš€ Lancer le dashboard pour test manuel ? (y/N): ")
    if launch_dashboard.lower() == 'y':
        launch_dashboard_test()
    
    print("\nâœ… Tests environnement terminÃ©s")
    print("ğŸ¯ Vous pouvez maintenant lancer les tests de performance:")
    print("   python run_performance_tests.py")

if __name__ == "__main__":
    main()