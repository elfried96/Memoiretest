# üìñ Guide d'Apprentissage de l'Architecture

## üéØ **MODULE 1: Point d'Entr√©e et Configuration**

### **Fichiers Cl√©s**
- `main_headless_refactored.py` - Point d'entr√©e modernis√©
- `config/base_config.py` - Configuration centralis√©e
- `config/config_manager.py` - Gestionnaire de configuration

### **Ce que vous devez comprendre**
1. **Arguments CLI** : Comment les param√®tres sont pars√©s et appliqu√©s
2. **Configuration hi√©rarchique** : VideoConfig, VLMConfig, DetectionConfig, etc.
3. **Variables d'environnement** : Override de configuration
4. **Validation** : Comment la configuration est valid√©e

### **Exercices pratiques - MODULE 1**

#### **üî¨ Exercice 1.1: Test des configurations CLI**
```bash
# √âtape 1: Testez les mod√®les VLM diff√©rents
python main_headless_refactored.py --model kimi-vl-a3b-thinking --video 0 --mode FAST
python main_headless_refactored.py --model qwen2-vl-7b-instruct --video 0 --mode THOROUGH

# √âtape 2: Observez les diff√©rences
# - Temps de traitement par frame
# - Qualit√© de l'analyse
# - Consommation GPU/CPU
```

#### **üî¨ Exercice 1.2: Variables d'environnement**
```bash
# √âtape 1: Test avec variables d'environnement
SURVEILLANCE_VLM_MODE=smart python main_headless_refactored.py
SURVEILLANCE_DEBUG=true python main_headless_refactored.py --max-frames 5

# √âtape 2: Cr√©ez votre fichier .env personnalis√©
echo "SURVEILLANCE_VLM_MODE=balanced" > .env
echo "SURVEILLANCE_MAX_FRAMES=20" >> .env
echo "SURVEILLANCE_SAVE_RESULTS=true" >> .env

# √âtape 3: Testez la priorit√© des configurations
python main_headless_refactored.py --mode THOROUGH  # CLI override env
```

#### **üî¨ Exercice 1.3: Analyse de la configuration**
```bash
# √âtape 1: Inspectez les configs charg√©es
python -c "
from config.config_loader import load_config
config = load_config()
print('VideoConfig:', config.video)
print('VLMConfig:', config.vlm)
print('DetectionConfig:', config.detection)
"

# √âtape 2: Validez une configuration invalide
python main_headless_refactored.py --model inexistant_model --video /path/invalid
# Observez les messages d'erreur et la validation
```

#### **üéØ Objectifs d'Apprentissage - Module 1:**
- [ ] Comprendre la hi√©rarchie de configuration (CLI > ENV > defaults)
- [ ] Identifier tous les param√®tres configurables
- [ ] Ma√Ætriser la validation et gestion d'erreurs
- [ ] Cr√©er ses propres presets de configuration

---

## üéØ **MODULE 2: Syst√®me Headless (C≈ìur)**

### **Fichiers Cl√©s**
- `src/core/headless/surveillance_system.py` - Orchestrateur principal
- `src/core/headless/video_processor.py` - Traitement vid√©o
- `src/core/headless/frame_analyzer.py` - Analyse de frames
- `src/core/headless/result_models.py` - Mod√®les de donn√©es

### **Ce que vous devez comprendre**

#### **HeadlessSurveillanceSystem**
```python
# Flow principal
async def run_surveillance(self):
    # 1. Traitement des frames (g√©n√©rateur)
    async for result in self._process_frames():
        # 2. Mise √† jour des m√©triques
        self._update_metrics(result)
        # 3. Sauvegarde conditionnelle
        if self.save_results:
            self.results.append(result)
```

#### **VideoProcessor** 
```python
# Capture vid√©o optimis√©e
def frames_generator(self):
    # 1. Gestion webcam vs fichier
    # 2. Frame skipping
    # 3. Limite max_frames
    # 4. Gestion d'erreurs
```

#### **FrameAnalyzer**
```python
async def analyze_frame(self, frame, frame_id, timestamp):
    # 1. D√©tection YOLO
    detections = await self._detect_objects(frame)
    # 2. Tracking
    tracked = self._track_objects(detections) 
    # 3. Analyse VLM conditionnelle
    vlm_analysis = await self._analyze_with_vlm(frame, tracked)
    # 4. D√©termination actions
    actions = self._determine_actions(alert_level, vlm_analysis)
```

### **Exercices pratiques - MODULE 2**

#### **üî¨ Exercice 2.1: Tra√ßage du Flow Principal**
```python
# √âtape 1: Ajoutez des logs de debug dans surveillance_system.py
import logging
logging.basicConfig(level=logging.DEBUG)

# Dans HeadlessSurveillanceSystem.__init__():
self.logger = logging.getLogger(__name__)

# Dans run_surveillance():
async def run_surveillance(self):
    self.logger.info("üöÄ D√©marrage surveillance")
    frame_count = 0
    async for result in self._process_frames():
        frame_count += 1
        self.logger.info(f"üì∏ Frame {frame_count}: {result.alert_level}")
        self._update_metrics(result)
        if self.save_results:
            self.results.append(result)
    self.logger.info(f"‚úÖ Surveillance termin√©e: {frame_count} frames")
```

```bash
# √âtape 2: Lancez avec traces d√©taill√©es
python main_headless_refactored.py --debug --max-frames 5 --video 0

# √âtape 3: Analysez les logs pour comprendre:
# - Temps entre capture et analyse
# - D√©clenchement des alertes
# - Performance de chaque √©tape
```

#### **üî¨ Exercice 2.2: Modification des seuils d'alerte**
```python
# √âtape 1: Cr√©ez un script de test des seuils
# test_alert_thresholds.py
import asyncio
from src.core.headless.frame_analyzer import FrameAnalyzer
from config.config_loader import load_config

async def test_thresholds():
    config = load_config()
    analyzer = FrameAnalyzer(config)
    
    # Test diff√©rents seuils
    test_scenarios = [
        {"confidence_threshold": 0.3, "alert_threshold": 0.5},
        {"confidence_threshold": 0.5, "alert_threshold": 0.7},
        {"confidence_threshold": 0.7, "alert_threshold": 0.9},
    ]
    
    for scenario in test_scenarios:
        print(f"Test avec seuils: {scenario}")
        # Modifiez les seuils dynamiquement
        analyzer.confidence_threshold = scenario["confidence_threshold"]
        analyzer.alert_threshold = scenario["alert_threshold"]
        
        # Testez avec une frame de test
        # ... votre logique de test
```

#### **üî¨ Exercice 2.3: VideoProcessor - Analyse d√©taill√©e**
```python
# √âtape 1: Cr√©ez un script d'analyse du processeur vid√©o
# analyze_video_processor.py
from src.core.headless.video_processor import VideoProcessor
import cv2
import time

def analyze_frame_generation():
    processor = VideoProcessor(
        video_source=0,  # Webcam
        max_frames=10,
        frame_skip=2
    )
    
    start_time = time.time()
    frame_count = 0
    
    for frame_id, frame, timestamp in processor.frames_generator():
        frame_count += 1
        print(f"Frame {frame_id}: {frame.shape}, timestamp: {timestamp}")
        
        # Analysez la qualit√© de la frame
        blur_score = cv2.Laplacian(frame, cv2.CV_64F).var()
        print(f"  Blur score: {blur_score:.2f}")
        
    total_time = time.time() - start_time
    fps = frame_count / total_time
    print(f"FPS moyen: {fps:.2f}")
```

#### **üî¨ Exercice 2.4: Monitoring temps r√©el**
```python
# √âtape 1: Cr√©ez un moniteur de performance simple
# performance_tracker.py
import time
from collections import defaultdict

class SimplePerformanceTracker:
    def __init__(self):
        self.timings = defaultdict(list)
        self.start_times = {}
    
    def start_timer(self, operation):
        self.start_times[operation] = time.time()
    
    def end_timer(self, operation):
        if operation in self.start_times:
            duration = time.time() - self.start_times[operation]
            self.timings[operation].append(duration)
            del self.start_times[operation]
            return duration
    
    def get_stats(self):
        stats = {}
        for op, times in self.timings.items():
            stats[op] = {
                'avg': sum(times) / len(times),
                'min': min(times),
                'max': max(times),
                'count': len(times)
            }
        return stats

# √âtape 2: Int√©grez dans FrameAnalyzer
tracker = SimplePerformanceTracker()

async def analyze_frame_with_timing(self, frame, frame_id, timestamp):
    tracker.start_timer('total_analysis')
    
    tracker.start_timer('detection')
    detections = await self._detect_objects(frame)
    tracker.end_timer('detection')
    
    tracker.start_timer('tracking') 
    tracked = self._track_objects(detections)
    tracker.end_timer('tracking')
    
    tracker.start_timer('vlm_analysis')
    vlm_analysis = await self._analyze_with_vlm(frame, tracked)
    tracker.end_timer('vlm_analysis')
    
    tracker.end_timer('total_analysis')
    
    # Affichez stats toutes les 10 frames
    if frame_id % 10 == 0:
        print(tracker.get_stats())
```

#### **üéØ Objectifs d'Apprentissage - Module 2:**
- [ ] Comprendre le flow complet d'une frame
- [ ] Ma√Ætriser les param√®tres de performance (frame_skip, max_frames)
- [ ] Identifier les goulots d'√©tranglement
- [ ] Personnaliser la logique d'alerte
- [ ] Cr√©er ses propres m√©triques de performance

---

## üéØ **MODULE 3: Mod√®les VLM (Intelligence)**

### **Fichiers Cl√©s**
- `src/core/vlm/dynamic_model.py` - Mod√®le VLM principal
- `src/core/vlm/model_registry.py` - Registre des mod√®les
- `src/core/vlm/prompt_builder.py` - Construction prompts
- `src/core/orchestrator/vlm_orchestrator.py` - Orchestration VLM

### **Ce que vous devez comprendre**

#### **DynamicVisionLanguageModel**
```python
class DynamicVisionLanguageModel:
    # 1. Support multi-mod√®les (Kimi, Qwen)
    # 2. Fallback automatique
    # 3. Tool calling int√©gr√©
    # 4. Gestion d'erreurs robuste
```

#### **VLMModelRegistry**
```python
# Tous les mod√®les support√©s avec leurs configurations
models = {
    "kimi-vl-a3b-thinking": ModelConfig(...),
    "qwen2-vl-7b-instruct": ModelConfig(...),
    ...
}
```

#### **ModernVLMOrchestrator**
```python
# 3 modes d'orchestration
FAST: 1.2s - Outils essentiels
BALANCED: 2.5s - √âquilibre performance/pr√©cision  
THOROUGH: 4.8s - Tous les outils avanc√©s
```

### **Exercices pratiques - MODULE 3**

#### **üî¨ Exercice 3.1: Benchmark des mod√®les VLM**
```python
# √âtape 1: Cr√©ez un script de benchmark
# benchmark_vlm_models.py
import asyncio
import time
import json
from src.core.vlm.dynamic_model import DynamicVisionLanguageModel
from src.core.vlm.model_registry import VLMModelRegistry

async def benchmark_models():
    models_to_test = [
        "kimi-vl-a3b-thinking",
        "qwen2-vl-7b-instruct",
        "claude-3-5-haiku-20241022",
        "gpt-4o-mini-2024-07-18"
    ]
    
    results = {}
    test_image_path = "path/to/test_image.jpg"
    test_prompt = "D√©crivez cette image en d√©tail."
    
    for model_name in models_to_test:
        print(f"üß™ Test du mod√®le: {model_name}")
        
        try:
            vlm = DynamicVisionLanguageModel(model_name)
            
            # Mesure du temps de chargement
            start_time = time.time()
            await vlm.initialize()
            load_time = time.time() - start_time
            
            # Mesure du temps d'inf√©rence
            start_time = time.time()
            response = await vlm.analyze_image(test_image_path, test_prompt)
            inference_time = time.time() - start_time
            
            results[model_name] = {
                "load_time": load_time,
                "inference_time": inference_time,
                "response_length": len(response),
                "status": "success"
            }
            
        except Exception as e:
            results[model_name] = {
                "error": str(e),
                "status": "failed"
            }
    
    # Sauvegarde des r√©sultats
    with open("vlm_benchmark_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    return results

# √âtape 2: Lancez le benchmark
if __name__ == "__main__":
    results = asyncio.run(benchmark_models())
    print("\nüìä R√©sultats du benchmark:")
    for model, stats in results.items():
        if stats["status"] == "success":
            print(f"{model}:")
            print(f"  ‚è±Ô∏è  Temps chargement: {stats['load_time']:.2f}s")
            print(f"  üöÄ Temps inf√©rence: {stats['inference_time']:.2f}s")
            print(f"  üìù Longueur r√©ponse: {stats['response_length']} chars")
        else:
            print(f"{model}: ‚ùå {stats['error']}")
```

#### **üî¨ Exercice 3.2: Test des modes d'orchestration**
```python
# √âtape 1: Cr√©ez un comparateur de modes
# compare_orchestration_modes.py
import asyncio
from src.core.orchestrator.vlm_orchestrator import ModernVLMOrchestrator
from src.core.types import DetectedObject, BoundingBox

async def compare_modes():
    orchestrator = ModernVLMOrchestrator()
    
    # Donn√©es de test simul√©es
    test_frame = "path/to/test_frame.jpg"
    test_objects = [
        DetectedObject(
            class_name="person",
            confidence=0.85,
            bbox=BoundingBox(x=100, y=100, width=200, height=300),
            track_id=1
        )
    ]
    
    modes = ["FAST", "BALANCED", "THOROUGH"]
    results = {}
    
    for mode in modes:
        print(f"üéØ Test mode: {mode}")
        
        start_time = time.time()
        
        # Changez le mode de l'orchestrateur
        orchestrator.set_mode(mode)
        
        # Analysez avec ce mode
        analysis = await orchestrator.analyze_scene(
            frame=test_frame,
            detected_objects=test_objects,
            context={"alert_level": "medium"}
        )
        
        execution_time = time.time() - start_time
        
        results[mode] = {
            "execution_time": execution_time,
            "tools_used": analysis.get("tools_used", []),
            "analysis_quality": len(analysis.get("description", "")),
            "actions_suggested": len(analysis.get("actions", []))
        }
    
    # Analyse comparative
    print("\nüìà Analyse comparative:")
    for mode, stats in results.items():
        print(f"\n{mode}:")
        print(f"  ‚è±Ô∏è  Temps: {stats['execution_time']:.2f}s")
        print(f"  üîß Outils: {len(stats['tools_used'])}")
        print(f"  üìù Qualit√©: {stats['analysis_quality']} chars")
        print(f"  üéØ Actions: {stats['actions_suggested']}")
    
    return results

# √âtape 2: Analysez les trade-offs
def analyze_tradeoffs(results):
    print("\n‚öñÔ∏è Analyse des trade-offs:")
    
    fastest = min(results.items(), key=lambda x: x[1]['execution_time'])
    most_detailed = max(results.items(), key=lambda x: x[1]['analysis_quality'])
    most_tools = max(results.items(), key=lambda x: len(x[1]['tools_used']))
    
    print(f"Plus rapide: {fastest[0]} ({fastest[1]['execution_time']:.2f}s)")
    print(f"Plus d√©taill√©: {most_detailed[0]} ({most_detailed[1]['analysis_quality']} chars)")
    print(f"Plus d'outils: {most_tools[0]} ({len(most_tools[1]['tools_used'])} outils)")
```

#### **üî¨ Exercice 3.3: Cr√©ation de prompts personnalis√©s**
```python
# √âtape 1: Cr√©ez un g√©n√©rateur de prompts personnalis√©s
# custom_prompt_builder.py
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class CustomPromptTemplate:
    name: str
    template: str
    variables: List[str]
    context_requirements: Dict[str, Any]

class CustomPromptBuilder:
    def __init__(self):
        self.templates = {
            "security_focus": CustomPromptTemplate(
                name="Security Focus",
                template="""
Analysez cette image de surveillance avec un focus s√©curitaire:

Contexte: {context}
Objets d√©tect√©s: {objects}
Timestamp: {timestamp}

Questions sp√©cifiques:
1. Y a-t-il des comportements suspects?
2. Les acc√®s sont-ils autoris√©s?
3. Des objets dangereux sont-ils pr√©sents?
4. Le niveau d'alerte recommand√©?

R√©pondez en format JSON avec:
- risk_level: LOW/MEDIUM/HIGH/CRITICAL
- suspicious_activities: []
- recommended_actions: []
- confidence_score: 0.0-1.0
                """,
                variables=["context", "objects", "timestamp"],
                context_requirements={"security_zone": True}
            ),
            
            "customer_behavior": CustomPromptTemplate(
                name="Customer Behavior",
                template="""
Analysez le comportement client dans cette sc√®ne commerciale:

Zone: {zone}
Personnes d√©tect√©es: {people_count}
Objets d'int√©r√™t: {products}

Analysez:
1. Patterns de d√©placement
2. Interactions avec produits
3. Temps pass√© dans la zone
4. Indicateurs d'int√©r√™t d'achat

Format de r√©ponse:
- behavior_type: BROWSING/INTERESTED/PURCHASING/LEAVING
- engagement_score: 0.0-1.0
- product_interactions: []
- recommendations: []
                """,
                variables=["zone", "people_count", "products"],
                context_requirements={"commercial_context": True}
            )
        }
    
    def build_prompt(self, template_name: str, variables: Dict[str, Any]) -> str:
        template = self.templates[template_name]
        return template.template.format(**variables)
    
    def add_custom_template(self, template: CustomPromptTemplate):
        self.templates[template.name] = template

# √âtape 2: Testez vos prompts personnalis√©s
async def test_custom_prompts():
    builder = CustomPromptBuilder()
    vlm = DynamicVisionLanguageModel("claude-3-5-haiku-20241022")
    
    # Test du prompt s√©curitaire
    security_prompt = builder.build_prompt("security_focus", {
        "context": "Zone d'entr√©e principale - 14h30",
        "objects": "2 personnes, 1 sac, porte d'acc√®s",
        "timestamp": "2024-01-15 14:30:22"
    })
    
    response = await vlm.analyze_with_prompt(
        image_path="test_image.jpg",
        prompt=security_prompt
    )
    
    print("üîí Analyse s√©curitaire:")
    print(response)
    
    return response

# √âtape 3: Cr√©ez votre propre template
def create_your_template():
    # Exercice: Cr√©ez un template pour votre use-case sp√©cifique
    your_template = CustomPromptTemplate(
        name="your_custom_analysis",
        template="""
        # Votre template personnalis√© ici
        # Variables: {var1}, {var2}
        # Instructions sp√©cifiques √† votre cas d'usage
        """,
        variables=["var1", "var2"],
        context_requirements={"your_requirement": True}
    )
    
    builder = CustomPromptBuilder()
    builder.add_custom_template(your_template)
    
    return builder
```

#### **üî¨ Exercice 3.4: Optimisation et fallback**
```python
# √âtape 1: Testez le syst√®me de fallback
# test_fallback_system.py
import asyncio
from src.core.vlm.dynamic_model import DynamicVisionLanguageModel

async def test_fallback_mechanism():
    # Configurez un mod√®le principal qui pourrait √©chouer
    primary_model = "modele_inexistant"
    fallback_models = ["claude-3-5-haiku-20241022", "gpt-4o-mini-2024-07-18"]
    
    vlm = DynamicVisionLanguageModel(
        primary_model, 
        fallback_models=fallback_models
    )
    
    try:
        result = await vlm.analyze_image(
            "test_image.jpg",
            "D√©crivez cette image"
        )
        print(f"‚úÖ Analyse r√©ussie avec: {vlm.current_model}")
        print(f"R√©sultat: {result[:100]}...")
        
    except Exception as e:
        print(f"‚ùå Tous les mod√®les ont √©chou√©: {e}")

# √âtape 2: Optimisez les performances
async def optimize_vlm_performance():
    vlm = DynamicVisionLanguageModel("claude-3-5-haiku-20241022")
    
    # Test avec diff√©rentes tailles d'images
    image_sizes = [
        (640, 480),   # Standard
        (1280, 720),  # HD
        (1920, 1080)  # Full HD
    ]
    
    for width, height in image_sizes:
        start_time = time.time()
        # Redimensionnez et analysez l'image
        # ... votre code de redimensionnement
        processing_time = time.time() - start_time
        
        print(f"Taille {width}x{height}: {processing_time:.2f}s")
```

#### **üéØ Objectifs d'Apprentissage - Module 3:**
- [ ] Comparer les performances de diff√©rents mod√®les VLM
- [ ] Ma√Ætriser les 3 modes d'orchestration (FAST/BALANCED/THOROUGH)
- [ ] Cr√©er des prompts personnalis√©s pour votre use-case
- [ ] Comprendre le syst√®me de fallback et robustesse
- [ ] Optimiser les performances selon vos contraintes

---

## üéØ **MODULE 4: D√©tection et Tracking**

### **Fichiers Cl√©s**
- `src/detection/yolo_detector.py` - D√©tection YOLO
- `src/detection/tracking/byte_tracker.py` - Tracking BYTETracker
- `src/core/types.py` - Types DetectedObject, BoundingBox

### **Ce que vous devez comprendre**

#### **YOLODetector**
```python
def detect(self, frame):
    # 1. Pr√©processing de l'image
    # 2. Inf√©rence YOLO
    # 3. Post-processing (NMS, filtrage)
    # 4. Conversion vers DetectedObject
```

#### **BYTETracker**
```python
def update(self, detections):
    # 1. Association des d√©tections aux tracks existants
    # 2. Gestion des nouvelles d√©tections
    # 3. Suppression des tracks perdus
    # 4. Assignation d'IDs uniques
```

### **Exercices pratiques - MODULE 4**

#### **üî¨ Exercice 4.1: Optimisation YOLO**
```python
# √âtape 1: Testez diff√©rents seuils de confiance
# test_yolo_thresholds.py
import cv2
import numpy as np
from src.detection.yolo_detector import YOLODetector
import matplotlib.pyplot as plt

def test_confidence_thresholds():
    detector = YOLODetector()
    test_image = cv2.imread("path/to/test_image.jpg")
    
    # Test diff√©rents seuils
    confidence_thresholds = [0.3, 0.5, 0.7, 0.9]
    results = {}
    
    for conf_threshold in confidence_thresholds:
        detector.confidence_threshold = conf_threshold
        detections = detector.detect(test_image)
        
        results[conf_threshold] = {
            "num_detections": len(detections),
            "avg_confidence": np.mean([d.confidence for d in detections]) if detections else 0,
            "classes_detected": list(set([d.class_name for d in detections]))
        }
        
        print(f"Seuil {conf_threshold}: {len(detections)} d√©tections")
        for detection in detections[:3]:  # Top 3
            print(f"  - {detection.class_name}: {detection.confidence:.3f}")
    
    return results

# √âtape 2: Visualisez l'impact des seuils
def visualize_detection_impact(results):
    thresholds = list(results.keys())
    num_detections = [results[t]["num_detections"] for t in thresholds]
    avg_confidences = [results[t]["avg_confidence"] for t in thresholds]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Nombre de d√©tections vs seuil
    ax1.plot(thresholds, num_detections, 'bo-')
    ax1.set_xlabel('Seuil de confiance')
    ax1.set_ylabel('Nombre de d√©tections')
    ax1.set_title('Impact du seuil sur le nombre de d√©tections')
    ax1.grid(True)
    
    # Confiance moyenne vs seuil
    ax2.plot(thresholds, avg_confidences, 'ro-')
    ax2.set_xlabel('Seuil de confiance')
    ax2.set_ylabel('Confiance moyenne')
    ax2.set_title('Impact du seuil sur la qualit√©')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('yolo_threshold_analysis.png')
    plt.show()

# √âtape 3: Test du NMS (Non-Maximum Suppression)
def test_nms_parameters():
    detector = YOLODetector()
    test_image = cv2.imread("path/to/crowded_scene.jpg")
    
    nms_thresholds = [0.3, 0.5, 0.7]
    
    for nms_threshold in nms_thresholds:
        detector.nms_threshold = nms_threshold
        detections = detector.detect(test_image)
        
        print(f"NMS {nms_threshold}: {len(detections)} d√©tections finales")
        
        # Analysez la densit√© des bo√Ætes
        if len(detections) > 1:
            overlaps = detector.calculate_overlaps(detections)
            print(f"  Overlap moyen: {np.mean(overlaps):.3f}")
```

#### **üî¨ Exercice 4.2: Analyse du tracking BYTETracker**
```python
# √âtape 1: Analysez la performance du tracker
# analyze_tracking_performance.py
from src.detection.tracking.byte_tracker import BYTETracker
from src.detection.yolo_detector import YOLODetector
import cv2

class TrackingAnalyzer:
    def __init__(self):
        self.tracker = BYTETracker()
        self.detector = YOLODetector()
        self.track_history = {}
        self.metrics = {
            'new_tracks': 0,
            'lost_tracks': 0,
            'track_switches': 0,
            'avg_track_length': []
        }
    
    def analyze_video_tracking(self, video_path, max_frames=100):
        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        
        while cap.isOpened() and frame_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            # D√©tection
            detections = self.detector.detect(frame)
            
            # Tracking
            tracked_objects = self.tracker.update(detections)
            
            # Analyse des m√©triques
            self._update_metrics(tracked_objects, frame_count)
            
            # Visualisation (optionnelle)
            if frame_count % 10 == 0:
                self._visualize_tracks(frame, tracked_objects, frame_count)
            
            frame_count += 1
        
        cap.release()
        return self._compute_final_metrics()
    
    def _update_metrics(self, tracked_objects, frame_count):
        current_track_ids = set([obj.track_id for obj in tracked_objects])
        
        if hasattr(self, 'previous_track_ids'):
            # Nouveaux tracks
            new_tracks = current_track_ids - self.previous_track_ids
            self.metrics['new_tracks'] += len(new_tracks)
            
            # Tracks perdus
            lost_tracks = self.previous_track_ids - current_track_ids
            self.metrics['lost_tracks'] += len(lost_tracks)
            
            for track_id in lost_tracks:
                if track_id in self.track_history:
                    track_length = len(self.track_history[track_id])
                    self.metrics['avg_track_length'].append(track_length)
        
        # Mise √† jour de l'historique
        for obj in tracked_objects:
            if obj.track_id not in self.track_history:
                self.track_history[obj.track_id] = []
            self.track_history[obj.track_id].append({
                'frame': frame_count,
                'bbox': obj.bbox,
                'confidence': obj.confidence
            })
        
        self.previous_track_ids = current_track_ids
    
    def _visualize_tracks(self, frame, tracked_objects, frame_count):
        vis_frame = frame.copy()
        
        for obj in tracked_objects:
            # Dessinez la bounding box
            x, y, w, h = obj.bbox.x, obj.bbox.y, obj.bbox.width, obj.bbox.height
            cv2.rectangle(vis_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            # Ajoutez l'ID de tracking
            cv2.putText(vis_frame, f"ID:{obj.track_id}", 
                       (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
            
            # Dessinez la trajectoire (derniers 10 points)
            if obj.track_id in self.track_history:
                history = self.track_history[obj.track_id][-10:]
                points = [(int(h['bbox'].x + h['bbox'].width/2), 
                          int(h['bbox'].y + h['bbox'].height/2)) for h in history]
                
                for i in range(1, len(points)):
                    cv2.line(vis_frame, points[i-1], points[i], (255, 255, 0), 2)
        
        cv2.imwrite(f"tracking_frame_{frame_count:04d}.jpg", vis_frame)
    
    def _compute_final_metrics(self):
        avg_track_length = np.mean(self.metrics['avg_track_length']) if self.metrics['avg_track_length'] else 0
        
        final_metrics = {
            'total_tracks': len(self.track_history),
            'new_tracks': self.metrics['new_tracks'],
            'lost_tracks': self.metrics['lost_tracks'],
            'avg_track_length': avg_track_length,
            'track_efficiency': 1 - (self.metrics['track_switches'] / max(self.metrics['new_tracks'], 1))
        }
        
        return final_metrics

# √âtape 2: Testez diff√©rents param√®tres de tracking
def test_tracking_parameters():
    analyzer = TrackingAnalyzer()
    
    # Param√®tres √† tester
    test_configs = [
        {"track_thresh": 0.5, "track_buffer": 30, "match_thresh": 0.8},
        {"track_thresh": 0.6, "track_buffer": 50, "match_thresh": 0.7},
        {"track_thresh": 0.4, "track_buffer": 20, "match_thresh": 0.9}
    ]
    
    results = {}
    
    for i, config in enumerate(test_configs):
        print(f"üß™ Test configuration {i+1}: {config}")
        
        # Configurez le tracker
        analyzer.tracker.track_thresh = config["track_thresh"]
        analyzer.tracker.track_buffer = config["track_buffer"]
        analyzer.tracker.match_thresh = config["match_thresh"]
        
        # Analysez
        metrics = analyzer.analyze_video_tracking("test_video.mp4")
        results[f"config_{i+1}"] = {**config, **metrics}
        
        print(f"  R√©sultats: {metrics}")
    
    return results

# √âtape 3: Optimisation automatique des param√®tres
def optimize_tracking_parameters():
    # Impl√©mentation d'une recherche par grille simple
    best_score = 0
    best_params = {}
    
    track_thresh_values = [0.4, 0.5, 0.6, 0.7]
    match_thresh_values = [0.6, 0.7, 0.8, 0.9]
    
    for track_thresh in track_thresh_values:
        for match_thresh in match_thresh_values:
            analyzer = TrackingAnalyzer()
            analyzer.tracker.track_thresh = track_thresh
            analyzer.tracker.match_thresh = match_thresh
            
            metrics = analyzer.analyze_video_tracking("test_video.mp4", max_frames=50)
            
            # Score composite (√† ajuster selon vos priorit√©s)
            score = (metrics['avg_track_length'] * 0.4 + 
                    metrics['track_efficiency'] * 0.6 - 
                    metrics['lost_tracks'] * 0.01)
            
            if score > best_score:
                best_score = score
                best_params = {
                    'track_thresh': track_thresh,
                    'match_thresh': match_thresh,
                    'score': score,
                    'metrics': metrics
                }
    
    print(f"üèÜ Meilleurs param√®tres: {best_params}")
    return best_params
```

#### **üî¨ Exercice 4.3: Visualisation avanc√©e**
```python
# √âtape 1: Cr√©ez un visualiseur interactif
# interactive_detection_viewer.py
import cv2
import numpy as np
from dataclasses import dataclass
from typing import List

@dataclass
class VisualizationConfig:
    show_confidence: bool = True
    show_class_names: bool = True
    show_track_ids: bool = True
    show_trajectories: bool = True
    trajectory_length: int = 10
    bbox_thickness: int = 2
    font_scale: float = 0.6

class InteractiveViewer:
    def __init__(self, config: VisualizationConfig = None):
        self.config = config or VisualizationConfig()
        self.colors = self._generate_colors(100)  # 100 couleurs diff√©rentes
        self.track_history = {}
    
    def _generate_colors(self, num_colors):
        """G√©n√®re des couleurs distinctes pour le tracking"""
        colors = []
        for i in range(num_colors):
            hue = int(180 * i / num_colors)
            color = cv2.cvtColor(np.uint8([[[hue, 255, 255]]]), cv2.COLOR_HSV2BGR)[0][0]
            colors.append((int(color[0]), int(color[1]), int(color[2])))
        return colors
    
    def visualize_detections(self, frame, detections, tracked_objects=None):
        """Visualise les d√©tections et le tracking"""
        vis_frame = frame.copy()
        
        # Dessinez les d√©tections simples
        for detection in detections:
            self._draw_detection(vis_frame, detection, (0, 255, 0))
        
        # Dessinez les objets track√©s
        if tracked_objects:
            for obj in tracked_objects:
                color = self.colors[obj.track_id % len(self.colors)]
                self._draw_tracked_object(vis_frame, obj, color)
        
        return vis_frame
    
    def _draw_detection(self, frame, detection, color):
        """Dessine une d√©tection simple"""
        x, y, w, h = detection.bbox.x, detection.bbox.y, detection.bbox.width, detection.bbox.height
        
        # Bo√Æte
        cv2.rectangle(frame, (x, y), (x+w, y+h), color, self.config.bbox_thickness)
        
        # Texte
        if self.config.show_class_names or self.config.show_confidence:
            text = ""
            if self.config.show_class_names:
                text += detection.class_name
            if self.config.show_confidence:
                text += f" {detection.confidence:.2f}"
            
            cv2.putText(frame, text, (x, y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, self.config.font_scale, color, 2)
    
    def _draw_tracked_object(self, frame, obj, color):
        """Dessine un objet track√© avec trajectoire"""
        x, y, w, h = obj.bbox.x, obj.bbox.y, obj.bbox.width, obj.bbox.height
        
        # Bo√Æte principale
        cv2.rectangle(frame, (x, y), (x+w, y+h), color, self.config.bbox_thickness)
        
        # ID de tracking
        if self.config.show_track_ids:
            cv2.putText(frame, f"ID:{obj.track_id}", 
                       (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, self.config.font_scale, color, 2)
        
        # Classe et confiance
        if self.config.show_class_names or self.config.show_confidence:
            text = ""
            if self.config.show_class_names:
                text += obj.class_name
            if self.config.show_confidence:
                text += f" {obj.confidence:.2f}"
            
            cv2.putText(frame, text, (x, y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, self.config.font_scale, color, 2)
        
        # Trajectoire
        if self.config.show_trajectories and obj.track_id in self.track_history:
            self._draw_trajectory(frame, obj.track_id, color)
        
        # Mise √† jour de l'historique
        center = (int(x + w/2), int(y + h/2))
        if obj.track_id not in self.track_history:
            self.track_history[obj.track_id] = []
        
        self.track_history[obj.track_id].append(center)
        
        # Gardez seulement les N derniers points
        if len(self.track_history[obj.track_id]) > self.config.trajectory_length:
            self.track_history[obj.track_id] = self.track_history[obj.track_id][-self.config.trajectory_length:]
    
    def _draw_trajectory(self, frame, track_id, color):
        """Dessine la trajectoire d'un objet"""
        points = self.track_history[track_id]
        
        if len(points) < 2:
            return
        
        # Dessinez des lignes entre les points successifs
        for i in range(1, len(points)):
            # √âpaisseur d√©grad√©e (plus r√©cent = plus √©pais)
            thickness = max(1, int(3 * i / len(points)))
            cv2.line(frame, points[i-1], points[i], color, thickness)
        
        # Point actuel plus visible
        cv2.circle(frame, points[-1], 4, color, -1)

# √âtape 2: Interface de contr√¥le en temps r√©el
def create_interactive_controls():
    """Cr√©e des contr√¥les trackbar pour ajuster les param√®tres en temps r√©el"""
    cv2.namedWindow('Controls', cv2.WINDOW_NORMAL)
    
    # Cr√©ez les trackbars
    cv2.createTrackbar('Confidence', 'Controls', 50, 100, lambda x: None)
    cv2.createTrackbar('NMS Threshold', 'Controls', 50, 100, lambda x: None)
    cv2.createTrackbar('Track Threshold', 'Controls', 50, 100, lambda x: None)
    cv2.createTrackbar('Show Trajectories', 'Controls', 1, 1, lambda x: None)
    cv2.createTrackbar('Trajectory Length', 'Controls', 10, 50, lambda x: None)
    
    return True

def get_control_values():
    """R√©cup√®re les valeurs actuelles des contr√¥les"""
    return {
        'confidence': cv2.getTrackbarPos('Confidence', 'Controls') / 100.0,
        'nms_threshold': cv2.getTrackbarPos('NMS Threshold', 'Controls') / 100.0,
        'track_threshold': cv2.getTrackbarPos('Track Threshold', 'Controls') / 100.0,
        'show_trajectories': bool(cv2.getTrackbarPos('Show Trajectories', 'Controls')),
        'trajectory_length': cv2.getTrackbarPos('Trajectory Length', 'Controls')
    }

# √âtape 3: Application compl√®te interactive
def run_interactive_detection():
    """Lance une session de d√©tection/tracking interactive"""
    # Initialisation
    detector = YOLODetector()
    tracker = BYTETracker()
    viewer = InteractiveViewer()
    
    create_interactive_controls()
    
    cap = cv2.VideoCapture(0)  # Webcam
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # R√©cup√©rez les param√®tres des contr√¥les
        controls = get_control_values()
        
        # Ajustez les param√®tres du d√©tecteur
        detector.confidence_threshold = controls['confidence']
        detector.nms_threshold = controls['nms_threshold']
        tracker.track_thresh = controls['track_threshold']
        
        # Ajustez les param√®tres du visualiseur
        viewer.config.show_trajectories = controls['show_trajectories']
        viewer.config.trajectory_length = controls['trajectory_length']
        
        # Traitement
        detections = detector.detect(frame)
        tracked_objects = tracker.update(detections)
        
        # Visualisation
        vis_frame = viewer.visualize_detections(frame, detections, tracked_objects)
        
        # Affichez les m√©triques
        cv2.putText(vis_frame, f"Detections: {len(detections)}", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(vis_frame, f"Tracks: {len(tracked_objects)}", 
                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        cv2.imshow('Interactive Detection', vis_frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
```

#### **üéØ Objectifs d'Apprentissage - Module 4:**
- [ ] Optimiser les seuils YOLO pour votre environnement
- [ ] Comprendre l'impact des param√®tres NMS sur la qualit√©
- [ ] Ma√Ætriser le tuning des param√®tres de tracking BYTETracker
- [ ] Cr√©er des visualisations personnalis√©es informatives
- [ ] D√©velopper des m√©triques de performance pour √©valuer le tracking

---

## üéØ **MODULE 5: Outils Avanc√©s IA**

### **Fichiers Cl√©s**
- `src/advanced_tools/sam2_segmentation.py` - Segmentation SAM2
- `src/advanced_tools/pose_estimation.py` - Estimation de pose
- `src/advanced_tools/trajectory_analyzer.py` - Analyse trajectoires
- `src/advanced_tools/multimodal_fusion.py` - Fusion multimodale

### **Ce que vous devez comprendre**

#### **Les 8 Outils**
1. **SAM2Segmentator** - Segmentation pr√©cise
2. **DinoV2FeatureExtractor** - Features visuelles
3. **OpenPoseEstimator** - Analyse posturale
4. **TrajectoryAnalyzer** - Patterns mouvement
5. **MultiModalFusion** - Fusion donn√©es
6. **TemporalTransformer** - Analyse temporelle
7. **AdversarialDetector** - Protection attaques
8. **DomainAdapter** - Adaptation contexte

#### **S√©lection Adaptative**
```python
def select_tools(self, context):
    if context["alert_level"] == "normal":
        return ["basic_segmentation", "pose_estimation"]
    else:
        return ["all_advanced_tools"]  # 6 outils
```

### **Exercices pratiques - MODULE 5**

#### **üî¨ Exercice 5.1: Test des outils avanc√©s individuellement**
```python
# √âtape 1: Cr√©ez un testeur d'outils modulaire
# advanced_tools_tester.py
import asyncio
import time
import numpy as np
from pathlib import Path
from src.advanced_tools.sam2_segmentation import SAM2Segmentator
from src.advanced_tools.pose_estimation import OpenPoseEstimator
from src.advanced_tools.trajectory_analyzer import TrajectoryAnalyzer
from src.advanced_tools.multimodal_fusion import MultiModalFusion

class AdvancedToolsTester:
    def __init__(self):
        self.tools = {
            "sam2": SAM2Segmentator(),
            "pose": OpenPoseEstimator(), 
            "trajectory": TrajectoryAnalyzer(),
            "fusion": MultiModalFusion()
        }
        self.results = {}
    
    async def test_tool_performance(self, tool_name, test_data):
        """Teste la performance d'un outil sp√©cifique"""
        if tool_name not in self.tools:
            raise ValueError(f"Outil {tool_name} non disponible")
        
        tool = self.tools[tool_name]
        
        # Mesure de performance
        start_time = time.time()
        memory_before = self._get_memory_usage()
        
        try:
            if tool_name == "sam2":
                result = await tool.segment_objects(
                    test_data["image"], 
                    test_data["bboxes"]
                )
            elif tool_name == "pose":
                result = await tool.estimate_poses(test_data["image"])
            elif tool_name == "trajectory":
                result = await tool.analyze_trajectories(test_data["tracks"])
            elif tool_name == "fusion":
                result = await tool.fuse_modalities(
                    test_data["visual_features"],
                    test_data["temporal_features"]
                )
            
            execution_time = time.time() - start_time
            memory_after = self._get_memory_usage()
            
            self.results[tool_name] = {
                "success": True,
                "execution_time": execution_time,
                "memory_usage": memory_after - memory_before,
                "result_quality": self._evaluate_result_quality(tool_name, result),
                "result": result
            }
            
        except Exception as e:
            self.results[tool_name] = {
                "success": False,
                "error": str(e),
                "execution_time": time.time() - start_time
            }
        
        return self.results[tool_name]
    
    def _get_memory_usage(self):
        """Mesure l'utilisation m√©moire actuelle"""
        import psutil
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024  # MB
    
    def _evaluate_result_quality(self, tool_name, result):
        """√âvalue la qualit√© du r√©sultat selon l'outil"""
        if tool_name == "sam2":
            # Qualit√© bas√©e sur la pr√©cision de segmentation
            return {
                "num_segments": len(result.get("segments", [])),
                "avg_confidence": np.mean([s.confidence for s in result.get("segments", [])]),
                "detail_level": sum([len(s.mask) for s in result.get("segments", [])])
            }
        elif tool_name == "pose":
            # Qualit√© bas√©e sur la d√©tection de poses
            return {
                "num_poses": len(result.get("poses", [])),
                "avg_keypoint_confidence": np.mean([p.confidence for p in result.get("poses", [])]),
                "completeness": np.mean([p.completeness for p in result.get("poses", [])])
            }
        # Ajoutez d'autres √©valuations selon vos besoins
        
        return {"score": 1.0}  # Score par d√©faut

# √âtape 2: Benchmark complet des outils
async def benchmark_all_tools():
    tester = AdvancedToolsTester()
    
    # Donn√©es de test
    test_image = "path/to/test_image.jpg"
    test_data = {
        "sam2": {
            "image": test_image,
            "bboxes": [(100, 100, 200, 200), (300, 150, 100, 150)]
        },
        "pose": {
            "image": test_image
        },
        "trajectory": {
            "tracks": [
                {"id": 1, "points": [(100, 200), (110, 205), (120, 210)]},
                {"id": 2, "points": [(200, 300), (190, 295), (180, 290)]}
            ]
        },
        "fusion": {
            "visual_features": np.random.rand(512),
            "temporal_features": np.random.rand(256)
        }
    }
    
    print("üöÄ D√©marrage du benchmark des outils avanc√©s\n")
    
    for tool_name, data in test_data.items():
        print(f"üß™ Test de l'outil: {tool_name}")
        result = await tester.test_tool_performance(tool_name, data)
        
        if result["success"]:
            print(f"  ‚úÖ Succ√®s - {result['execution_time']:.2f}s")
            print(f"  üíæ M√©moire: {result['memory_usage']:.1f}MB")
            print(f"  üìä Qualit√©: {result['result_quality']}")
        else:
            print(f"  ‚ùå √âchec: {result['error']}")
        print()
    
    return tester.results

# √âtape 3: Comparaison des configurations d'outils
def compare_tool_configurations():
    """Compare diff√©rentes configurations d'activation d'outils"""
    configurations = [
        {
            "name": "Minimal",
            "tools": ["pose"],
            "expected_time": 1.0,
            "use_cases": ["D√©tection de mouvement basique"]
        },
        {
            "name": "Standard", 
            "tools": ["sam2", "pose"],
            "expected_time": 2.5,
            "use_cases": ["Analyse de comportement"]
        },
        {
            "name": "Advanced",
            "tools": ["sam2", "pose", "trajectory", "fusion"],
            "expected_time": 5.0,
            "use_cases": ["Analyse comportementale compl√®te"]
        }
    ]
    
    print("‚öñÔ∏è Comparaison des configurations:\n")
    
    for config in configurations:
        print(f"üìã {config['name']}:")
        print(f"  üîß Outils: {', '.join(config['tools'])}")
        print(f"  ‚è±Ô∏è  Temps estim√©: {config['expected_time']}s")
        print(f"  üéØ Cas d'usage: {', '.join(config['use_cases'])}")
        print()
```

#### **üî¨ Exercice 5.2: Cr√©ation d'un nouvel outil personnalis√©**
```python
# √âtape 1: Cr√©ez votre propre outil avanc√©
# custom_advanced_tool.py
from abc import ABC, abstractmethod
import numpy as np
import cv2
from typing import Dict, Any, List
from dataclasses import dataclass

@dataclass
class ToolResult:
    success: bool
    data: Dict[str, Any]
    confidence: float
    processing_time: float
    metadata: Dict[str, Any] = None

class BaseAdvancedTool(ABC):
    """Classe de base pour tous les outils avanc√©s"""
    
    def __init__(self, name: str):
        self.name = name
        self.initialized = False
        self.config = {}
    
    @abstractmethod
    async def initialize(self) -> bool:
        """Initialise l'outil"""
        pass
    
    @abstractmethod
    async def process(self, **kwargs) -> ToolResult:
        """Traite les donn√©es d'entr√©e"""
        pass
    
    def configure(self, **config):
        """Configure l'outil avec des param√®tres personnalis√©s"""
        self.config.update(config)

# √âtape 2: Impl√©mentez votre outil personnalis√© - D√©tecteur d'√©motions
class EmotionDetector(BaseAdvancedTool):
    """D√©tecteur d'√©motions bas√© sur l'expression faciale"""
    
    def __init__(self):
        super().__init__("emotion_detector")
        self.model = None
        self.face_cascade = None
    
    async def initialize(self) -> bool:
        """Initialise le d√©tecteur d'√©motions"""
        try:
            # Chargez votre mod√®le d'√©motion (ex: avec transformers)
            # from transformers import pipeline
            # self.model = pipeline("image-classification", model="emotion-model")
            
            # Pour cet exemple, on simule avec OpenCV
            self.face_cascade = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            
            self.initialized = True
            return True
            
        except Exception as e:
            print(f"Erreur initialisation EmotionDetector: {e}")
            return False
    
    async def process(self, image: np.ndarray, faces: List[Dict] = None) -> ToolResult:
        """D√©tecte les √©motions dans les visages"""
        import time
        start_time = time.time()
        
        try:
            if not self.initialized:
                await self.initialize()
            
            if faces is None:
                # D√©tection automatique des visages
                faces = self._detect_faces(image)
            
            emotions_detected = []
            
            for face in faces:
                x, y, w, h = face['bbox']
                face_roi = image[y:y+h, x:x+w]
                
                # Analyse d'√©motion simul√©e (remplacez par votre mod√®le r√©el)
                emotion_scores = self._analyze_emotion(face_roi)
                
                emotions_detected.append({
                    'bbox': (x, y, w, h),
                    'emotions': emotion_scores,
                    'dominant_emotion': max(emotion_scores.items(), key=lambda x: x[1])
                })
            
            processing_time = time.time() - start_time
            
            return ToolResult(
                success=True,
                data={
                    'emotions': emotions_detected,
                    'num_faces': len(faces),
                    'dominant_emotions': [e['dominant_emotion'][0] for e in emotions_detected]
                },
                confidence=np.mean([e['dominant_emotion'][1] for e in emotions_detected]) if emotions_detected else 0.0,
                processing_time=processing_time,
                metadata={'tool_version': '1.0', 'method': 'opencv_simulation'}
            )
            
        except Exception as e:
            return ToolResult(
                success=False,
                data={'error': str(e)},
                confidence=0.0,
                processing_time=time.time() - start_time
            )
    
    def _detect_faces(self, image: np.ndarray) -> List[Dict]:
        """D√©tecte les visages dans l'image"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
        
        return [{'bbox': (x, y, w, h)} for (x, y, w, h) in faces]
    
    def _analyze_emotion(self, face_roi: np.ndarray) -> Dict[str, float]:
        """Analyse l'√©motion d'un visage (version simul√©e)"""
        # Dans un vrai cas, vous utiliseriez un mod√®le pr√©-entra√Æn√©
        # Ici on simule avec des valeurs al√©atoires bas√©es sur des caract√©ristiques visuelles
        
        emotions = ['happy', 'sad', 'angry', 'surprised', 'neutral', 'fear', 'disgust']
        
        # Analyse basique bas√©e sur la luminosit√© et les contours (tr√®s simplifi√©e)
        gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray_face)
        edges = cv2.Canny(gray_face, 50, 150)
        edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
        
        # Scores simul√©s (√† remplacer par un vrai mod√®le)
        scores = {}
        base_scores = np.random.rand(len(emotions))
        
        # Ajustements bas√©s sur les caract√©ristiques visuelles
        if brightness > 120:  # Visage plus lumineux -> plus probable d'√™tre heureux
            base_scores[emotions.index('happy')] += 0.3
        
        if edge_density > 0.1:  # Beaucoup de contours -> plus de tension
            base_scores[emotions.index('angry')] += 0.2
            base_scores[emotions.index('surprised')] += 0.2
        
        # Normalisation
        base_scores = base_scores / np.sum(base_scores)
        
        return {emotion: float(score) for emotion, score in zip(emotions, base_scores)}

# √âtape 3: Int√©grez votre outil dans le syst√®me
class CustomToolsManager:
    """Gestionnaire pour les outils personnalis√©s"""
    
    def __init__(self):
        self.custom_tools = {}
    
    def register_tool(self, tool: BaseAdvancedTool):
        """Enregistre un nouvel outil personnalis√©"""
        self.custom_tools[tool.name] = tool
        print(f"‚úÖ Outil {tool.name} enregistr√©")
    
    async def initialize_all_tools(self):
        """Initialise tous les outils enregistr√©s"""
        for name, tool in self.custom_tools.items():
            success = await tool.initialize()
            print(f"{'‚úÖ' if success else '‚ùå'} Initialisation {name}: {'Succ√®s' if success else '√âchec'}")
    
    async def process_with_tool(self, tool_name: str, **kwargs) -> ToolResult:
        """Utilise un outil sp√©cifique pour traiter des donn√©es"""
        if tool_name not in self.custom_tools:
            raise ValueError(f"Outil {tool_name} non trouv√©")
        
        tool = self.custom_tools[tool_name]
        return await tool.process(**kwargs)

# √âtape 4: Test de votre outil personnalis√©
async def test_custom_emotion_tool():
    # Cr√©ez et enregistrez votre outil
    manager = CustomToolsManager()
    emotion_tool = EmotionDetector()
    manager.register_tool(emotion_tool)
    
    # Initialisez
    await manager.initialize_all_tools()
    
    # Testez avec une image
    test_image = cv2.imread("path/to/image_with_faces.jpg")
    
    result = await manager.process_with_tool(
        "emotion_detector",
        image=test_image
    )
    
    print("üé≠ R√©sultats de d√©tection d'√©motion:")
    print(f"Succ√®s: {result.success}")
    print(f"Temps de traitement: {result.processing_time:.3f}s")
    print(f"Nombre de visages: {result.data.get('num_faces', 0)}")
    print(f"√âmotions dominantes: {result.data.get('dominant_emotions', [])}")
    
    return result
```

#### **üî¨ Exercice 5.3: Optimisation s√©lective des outils**
```python
# √âtape 1: Syst√®me de s√©lection adaptative intelligent
# adaptive_tool_selector.py
from enum import Enum
from typing import List, Dict, Set
import numpy as np

class AlertLevel(Enum):
    LOW = "low"
    MEDIUM = "medium" 
    HIGH = "high"
    CRITICAL = "critical"

class ContextType(Enum):
    INDOOR = "indoor"
    OUTDOOR = "outdoor"
    CROWDED = "crowded"
    ISOLATED = "isolated"
    DAYLIGHT = "daylight"
    NIGHT = "night"

class AdaptiveToolSelector:
    """S√©lectionne intelligemment les outils selon le contexte"""
    
    def __init__(self):
        self.tool_profiles = {
            "sam2_segmentation": {
                "cost": 3.0,  # Temps relatif
                "accuracy": 0.95,
                "best_contexts": [ContextType.OUTDOOR, ContextType.DAYLIGHT],
                "alert_threshold": AlertLevel.MEDIUM
            },
            "pose_estimation": {
                "cost": 1.5,
                "accuracy": 0.85,
                "best_contexts": [ContextType.INDOOR, ContextType.ISOLATED],
                "alert_threshold": AlertLevel.LOW
            },
            "trajectory_analyzer": {
                "cost": 2.0,
                "accuracy": 0.90,
                "best_contexts": [ContextType.CROWDED, ContextType.OUTDOOR],
                "alert_threshold": AlertLevel.MEDIUM
            },
            "multimodal_fusion": {
                "cost": 4.0,
                "accuracy": 0.98,
                "best_contexts": [ContextType.CRITICAL],
                "alert_threshold": AlertLevel.HIGH
            },
            "emotion_detector": {
                "cost": 2.5,
                "accuracy": 0.80,
                "best_contexts": [ContextType.INDOOR, ContextType.DAYLIGHT],
                "alert_threshold": AlertLevel.MEDIUM
            }
        }
        
        self.usage_history = {}
        self.performance_cache = {}
    
    def select_optimal_tools(self, 
                           alert_level: AlertLevel,
                           context: Set[ContextType],
                           time_budget: float = 5.0,
                           min_accuracy: float = 0.8) -> List[str]:
        """S√©lectionne les outils optimaux selon les contraintes"""
        
        available_tools = []
        
        for tool_name, profile in self.tool_profiles.items():
            # Filtrage par niveau d'alerte
            if self._meets_alert_threshold(profile["alert_threshold"], alert_level):
                # Filtrage par contexte
                context_score = self._calculate_context_score(profile["best_contexts"], context)
                # Filtrage par pr√©cision minimum
                if profile["accuracy"] >= min_accuracy:
                    available_tools.append({
                        "name": tool_name,
                        "cost": profile["cost"],
                        "accuracy": profile["accuracy"],
                        "context_score": context_score,
                        "priority": profile["accuracy"] * context_score / profile["cost"]
                    })
        
        # Tri par priorit√© d√©croissante
        available_tools.sort(key=lambda x: x["priority"], reverse=True)
        
        # S√©lection dans le budget temps
        selected_tools = []
        total_cost = 0
        
        for tool in available_tools:
            if total_cost + tool["cost"] <= time_budget:
                selected_tools.append(tool["name"])
                total_cost += tool["cost"]
        
        return selected_tools
    
    def _meets_alert_threshold(self, tool_threshold: AlertLevel, current_alert: AlertLevel) -> bool:
        """V√©rifie si l'outil est appropri√© pour le niveau d'alerte"""
        threshold_levels = {
            AlertLevel.LOW: 0,
            AlertLevel.MEDIUM: 1, 
            AlertLevel.HIGH: 2,
            AlertLevel.CRITICAL: 3
        }
        return threshold_levels[current_alert] >= threshold_levels[tool_threshold]
    
    def _calculate_context_score(self, best_contexts: List[ContextType], current_context: Set[ContextType]) -> float:
        """Calcule un score de pertinence contextuelle"""
        if not best_contexts:
            return 1.0
        
        matches = len(set(best_contexts).intersection(current_context))
        return (matches / len(best_contexts)) + 0.5  # Score minimum de 0.5
    
    def learn_from_usage(self, tool_name: str, actual_performance: Dict):
        """Apprend des performances r√©elles pour am√©liorer les s√©lections futures"""
        if tool_name not in self.usage_history:
            self.usage_history[tool_name] = []
        
        self.usage_history[tool_name].append(actual_performance)
        
        # Mise √† jour du profil bas√©e sur l'historique
        if len(self.usage_history[tool_name]) > 10:
            recent_performances = self.usage_history[tool_name][-10:]
            avg_accuracy = np.mean([p.get("accuracy", 0.5) for p in recent_performances])
            avg_time = np.mean([p.get("execution_time", 0) for p in recent_performances])
            
            # Ajustement des profils
            self.tool_profiles[tool_name]["accuracy"] = 0.7 * self.tool_profiles[tool_name]["accuracy"] + 0.3 * avg_accuracy
            self.tool_profiles[tool_name]["cost"] = 0.7 * self.tool_profiles[tool_name]["cost"] + 0.3 * avg_time

# √âtape 2: Testez la s√©lection adaptative
def test_adaptive_selection():
    selector = AdaptiveToolSelector()
    
    # Sc√©narios de test
    test_scenarios = [
        {
            "name": "Surveillance nocturne calme",
            "alert_level": AlertLevel.LOW,
            "context": {ContextType.NIGHT, ContextType.ISOLATED},
            "time_budget": 2.0
        },
        {
            "name": "Foule en journ√©e - alerte moyenne",
            "alert_level": AlertLevel.MEDIUM,
            "context": {ContextType.DAYLIGHT, ContextType.CROWDED, ContextType.OUTDOOR},
            "time_budget": 4.0
        },
        {
            "name": "Incident critique",
            "alert_level": AlertLevel.CRITICAL,
            "context": {ContextType.INDOOR, ContextType.DAYLIGHT},
            "time_budget": 10.0
        }
    ]
    
    print("üß† Test de s√©lection adaptative d'outils:\n")
    
    for scenario in test_scenarios:
        print(f"üìã Sc√©nario: {scenario['name']}")
        print(f"   Niveau d'alerte: {scenario['alert_level'].value}")
        print(f"   Contexte: {[c.value for c in scenario['context']]}")
        print(f"   Budget temps: {scenario['time_budget']}s")
        
        selected_tools = selector.select_optimal_tools(
            alert_level=scenario['alert_level'],
            context=scenario['context'],
            time_budget=scenario['time_budget']
        )
        
        total_cost = sum([selector.tool_profiles[tool]["cost"] for tool in selected_tools])
        avg_accuracy = np.mean([selector.tool_profiles[tool]["accuracy"] for tool in selected_tools]) if selected_tools else 0
        
        print(f"   ‚úÖ Outils s√©lectionn√©s: {selected_tools}")
        print(f"   ‚è±Ô∏è  Co√ªt total: {total_cost:.1f}s")
        print(f"   üéØ Pr√©cision moyenne: {avg_accuracy:.2f}")
        print()

# √âtape 3: Monitoring des performances en temps r√©el
class ToolPerformanceMonitor:
    """Monitore les performances des outils en temps r√©el"""
    
    def __init__(self):
        self.metrics = {}
        self.alert_thresholds = {
            "max_execution_time": 10.0,
            "min_accuracy": 0.7,
            "max_memory_usage": 1000  # MB
        }
    
    def record_execution(self, tool_name: str, metrics: Dict):
        """Enregistre les m√©triques d'ex√©cution d'un outil"""
        if tool_name not in self.metrics:
            self.metrics[tool_name] = []
        
        self.metrics[tool_name].append(metrics)
        
        # V√©rification des seuils d'alerte
        self._check_performance_alerts(tool_name, metrics)
    
    def _check_performance_alerts(self, tool_name: str, metrics: Dict):
        """V√©rifie si des seuils d'alerte sont d√©pass√©s"""
        alerts = []
        
        if metrics.get("execution_time", 0) > self.alert_thresholds["max_execution_time"]:
            alerts.append(f"‚ö†Ô∏è {tool_name}: Temps d'ex√©cution √©lev√© ({metrics['execution_time']:.2f}s)")
        
        if metrics.get("accuracy", 1.0) < self.alert_thresholds["min_accuracy"]:
            alerts.append(f"‚ö†Ô∏è {tool_name}: Pr√©cision faible ({metrics['accuracy']:.2f})")
        
        if metrics.get("memory_usage", 0) > self.alert_thresholds["max_memory_usage"]:
            alerts.append(f"‚ö†Ô∏è {tool_name}: Usage m√©moire √©lev√© ({metrics['memory_usage']:.1f}MB)")
        
        for alert in alerts:
            print(alert)
    
    def get_tool_statistics(self, tool_name: str) -> Dict:
        """R√©cup√®re les statistiques d'un outil"""
        if tool_name not in self.metrics:
            return {}
        
        data = self.metrics[tool_name]
        
        return {
            "total_executions": len(data),
            "avg_execution_time": np.mean([d.get("execution_time", 0) for d in data]),
            "avg_accuracy": np.mean([d.get("accuracy", 0) for d in data]),
            "avg_memory_usage": np.mean([d.get("memory_usage", 0) for d in data]),
            "success_rate": np.mean([d.get("success", False) for d in data])
        }
```

#### **üéØ Objectifs d'Apprentissage - Module 5:**
- [ ] Comprendre les 8 outils avanc√©s et leurs sp√©cialit√©s
- [ ] Cr√©er votre propre outil personnalis√©
- [ ] Ma√Ætriser la s√©lection adaptative selon le contexte
- [ ] Optimiser les performances avec monitoring temps r√©el
- [ ] Int√©grer de nouveaux outils dans l'architecture existante

---

## üéØ **MODULE 6: Monitoring des Performances**

### **Fichiers Cl√©s**
- `src/core/monitoring/performance_monitor.py` - Moniteur principal
- `src/core/monitoring/system_metrics.py` - M√©triques syst√®me
- `src/core/monitoring/vlm_metrics.py` - M√©triques VLM

### **Ce que vous devez comprendre**

#### **PerformanceMonitor**
```python
# Collection temps r√©el de m√©triques
monitor.add_collector(SystemMetricsCollector())
monitor.add_collector(VLMMetricsCollector()) 
monitor.start()  # Thread background
```

#### **Types de M√©triques**
- **Syst√®me** : CPU, GPU, m√©moire
- **VLM** : Temps traitement, d√©bit, erreurs
- **Qualit√©** : Scores confiance, tool calls

### **Exercices pratiques - MODULE 6**

#### **üî¨ Exercice 6.1: Surveillance compl√®te des m√©triques**
```python
# √âtape 1: Cr√©ez un dashboard de monitoring complet
# comprehensive_monitoring.py
import asyncio
import time
import psutil
import GPUtil
from typing import Dict, List, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import json

@dataclass
class SystemMetrics:
    timestamp: float
    cpu_percent: float
    memory_percent: float
    memory_available_mb: float
    gpu_percent: float
    gpu_memory_percent: float
    disk_io_read: int
    disk_io_write: int

@dataclass
class VLMMetrics:
    timestamp: float
    model_name: str
    inference_time: float
    queue_size: int
    tokens_processed: int
    success_rate: float
    error_count: int

@dataclass
class DetectionMetrics:
    timestamp: float
    fps: float
    detections_count: int
    tracking_accuracy: float
    processing_latency: float
    frame_drops: int

class ComprehensiveMonitor:
    """Moniteur complet de toutes les m√©triques du syst√®me"""
    
    def __init__(self):
        self.running = False
        self.metrics_history = {
            'system': [],
            'vlm': [],
            'detection': []
        }
        self.alerts = []
        self.alert_thresholds = {
            'cpu_max': 80.0,
            'memory_max': 85.0,
            'gpu_max': 90.0,
            'inference_time_max': 5.0,
            'fps_min': 10.0
        }
    
    async def start_monitoring(self, interval: float = 1.0):
        """D√©marre le monitoring continu"""
        self.running = True
        print("üîç D√©marrage du monitoring complet...")
        
        while self.running:
            # Collecte des m√©triques
            system_metrics = self._collect_system_metrics()
            self.metrics_history['system'].append(system_metrics)
            
            # V√©rification des alertes
            self._check_system_alerts(system_metrics)
            
            # Nettoyage de l'historique (garde les 1000 derniers points)
            for key in self.metrics_history:
                if len(self.metrics_history[key]) > 1000:
                    self.metrics_history[key] = self.metrics_history[key][-1000:]
            
            await asyncio.sleep(interval)
    
    def _collect_system_metrics(self) -> SystemMetrics:
        """Collecte les m√©triques syst√®me"""
        # CPU et m√©moire
        cpu_percent = psutil.cpu_percent(interval=None)
        memory = psutil.virtual_memory()
        
        # GPU (si disponible)
        gpu_percent = 0
        gpu_memory_percent = 0
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                gpu_percent = gpu.load * 100
                gpu_memory_percent = gpu.memoryUtil * 100
        except:
            pass
        
        # I/O disque
        disk_io = psutil.disk_io_counters()
        
        return SystemMetrics(
            timestamp=time.time(),
            cpu_percent=cpu_percent,
            memory_percent=memory.percent,
            memory_available_mb=memory.available / (1024**2),
            gpu_percent=gpu_percent,
            gpu_memory_percent=gpu_memory_percent,
            disk_io_read=disk_io.read_bytes if disk_io else 0,
            disk_io_write=disk_io.write_bytes if disk_io else 0
        )
    
    def _check_system_alerts(self, metrics: SystemMetrics):
        """V√©rifie les seuils d'alerte syst√®me"""
        alerts = []
        
        if metrics.cpu_percent > self.alert_thresholds['cpu_max']:
            alerts.append(f"üö® CPU √©lev√©: {metrics.cpu_percent:.1f}%")
        
        if metrics.memory_percent > self.alert_thresholds['memory_max']:
            alerts.append(f"üö® M√©moire √©lev√©e: {metrics.memory_percent:.1f}%")
        
        if metrics.gpu_percent > self.alert_thresholds['gpu_max']:
            alerts.append(f"üö® GPU √©lev√©: {metrics.gpu_percent:.1f}%")
        
        for alert in alerts:
            print(alert)
            self.alerts.append({
                'timestamp': datetime.now().isoformat(),
                'message': alert,
                'type': 'system'
            })
    
    def record_vlm_metrics(self, metrics: VLMMetrics):
        """Enregistre les m√©triques VLM"""
        self.metrics_history['vlm'].append(metrics)
        
        # V√©rification alertes VLM
        if metrics.inference_time > self.alert_thresholds['inference_time_max']:
            alert = f"üö® VLM lent: {metrics.inference_time:.2f}s ({metrics.model_name})"
            print(alert)
            self.alerts.append({
                'timestamp': datetime.now().isoformat(),
                'message': alert,
                'type': 'vlm'
            })
    
    def record_detection_metrics(self, metrics: DetectionMetrics):
        """Enregistre les m√©triques de d√©tection"""
        self.metrics_history['detection'].append(metrics)
        
        # V√©rification alertes d√©tection
        if metrics.fps < self.alert_thresholds['fps_min']:
            alert = f"üö® FPS faible: {metrics.fps:.1f}"
            print(alert)
            self.alerts.append({
                'timestamp': datetime.now().isoformat(),
                'message': alert,
                'type': 'detection'
            })
    
    def get_current_status(self) -> Dict:
        """Retourne l'√©tat actuel du syst√®me"""
        latest_system = self.metrics_history['system'][-1] if self.metrics_history['system'] else None
        latest_vlm = self.metrics_history['vlm'][-1] if self.metrics_history['vlm'] else None
        latest_detection = self.metrics_history['detection'][-1] if self.metrics_history['detection'] else None
        
        return {
            'system': asdict(latest_system) if latest_system else None,
            'vlm': asdict(latest_vlm) if latest_vlm else None,
            'detection': asdict(latest_detection) if latest_detection else None,
            'recent_alerts': self.alerts[-5:] if self.alerts else [],
            'total_alerts': len(self.alerts)
        }
    
    def export_metrics(self, filename: str = None):
        """Exporte toutes les m√©triques vers un fichier JSON"""
        if filename is None:
            filename = f"metrics_export_{int(time.time())}.json"
        
        export_data = {
            'export_timestamp': datetime.now().isoformat(),
            'metrics_history': {
                'system': [asdict(m) for m in self.metrics_history['system']],
                'vlm': [asdict(m) for m in self.metrics_history['vlm']],
                'detection': [asdict(m) for m in self.metrics_history['detection']]
            },
            'alerts': self.alerts,
            'thresholds': self.alert_thresholds
        }
        
        with open(filename, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        print(f"üìä M√©triques export√©es vers: {filename}")
        return filename

# √âtape 2: Int√©gration avec le syst√®me de surveillance
class SurveillanceWithMonitoring:
    """Syst√®me de surveillance avec monitoring int√©gr√©"""
    
    def __init__(self):
        self.monitor = ComprehensiveMonitor()
        self.frame_count = 0
        self.start_time = time.time()
    
    async def run_with_monitoring(self):
        """Lance la surveillance avec monitoring"""
        # D√©marrage du monitoring en arri√®re-plan
        monitor_task = asyncio.create_task(self.monitor.start_monitoring())
        
        try:
            # Simulation du syst√®me de surveillance
            await self._simulate_surveillance()
        finally:
            # Arr√™t du monitoring
            self.monitor.running = False
            await monitor_task
    
    async def _simulate_surveillance(self):
        """Simule le syst√®me de surveillance avec m√©triques"""
        print("üé• Simulation surveillance avec monitoring...")
        
        for frame_id in range(100):  # 100 frames simul√©es
            frame_start = time.time()
            
            # Simulation traitement VLM
            vlm_start = time.time()
            await asyncio.sleep(0.1)  # Simule traitement VLM
            vlm_time = time.time() - vlm_start
            
            # Enregistrement m√©triques VLM
            vlm_metrics = VLMMetrics(
                timestamp=time.time(),
                model_name="claude-3-5-haiku",
                inference_time=vlm_time,
                queue_size=0,
                tokens_processed=150,
                success_rate=0.98,
                error_count=0
            )
            self.monitor.record_vlm_metrics(vlm_metrics)
            
            # Simulation d√©tection
            detection_time = time.time() - frame_start
            current_fps = self.frame_count / (time.time() - self.start_time + 1)
            
            # Enregistrement m√©triques d√©tection
            detection_metrics = DetectionMetrics(
                timestamp=time.time(),
                fps=current_fps,
                detections_count=3,
                tracking_accuracy=0.92,
                processing_latency=detection_time,
                frame_drops=0
            )
            self.monitor.record_detection_metrics(detection_metrics)
            
            self.frame_count += 1
            
            # Affichage p√©riodique du status
            if frame_id % 20 == 0:
                status = self.monitor.get_current_status()
                print(f"\nüìä Frame {frame_id} - Status:")
                if status['system']:
                    print(f"   CPU: {status['system']['cpu_percent']:.1f}% | "
                          f"GPU: {status['system']['gpu_percent']:.1f}% | "
                          f"RAM: {status['system']['memory_percent']:.1f}%")
                if status['detection']:
                    print(f"   FPS: {status['detection']['fps']:.1f} | "
                          f"Latence: {status['detection']['processing_latency']:.3f}s")
                if status['recent_alerts']:
                    print(f"   üö® Alertes r√©centes: {len(status['recent_alerts'])}")
            
            await asyncio.sleep(0.05)  # 20 FPS simul√©
        
        # Export final des m√©triques
        export_file = self.monitor.export_metrics()
        return export_file

# √âtape 3: Lancez le monitoring complet
async def run_comprehensive_monitoring():
    surveillance = SurveillanceWithMonitoring()
    export_file = await surveillance.run_with_monitoring()
    
    print(f"\n‚úÖ Monitoring termin√©. Donn√©es export√©es: {export_file}")
    return export_file
```

#### **üî¨ Exercice 6.2: Alertes et notifications personnalis√©es**
```python
# √âtape 1: Syst√®me d'alertes avanc√©
# advanced_alerting.py
from enum import Enum
from dataclasses import dataclass
from typing import Callable, Dict, List, Any
import smtplib
from email.mime.text import MimeText
import requests
import logging

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class Alert:
    id: str
    timestamp: float
    severity: AlertSeverity
    component: str
    message: str
    metrics: Dict[str, Any]
    resolved: bool = False

class AlertHandler:
    """Gestionnaire d'alertes avec notifications multiples"""
    
    def __init__(self):
        self.handlers = {}
        self.alert_history = []
        self.suppression_rules = {}
        
    def add_handler(self, name: str, handler: Callable):
        """Ajoute un gestionnaire d'alerte personnalis√©"""
        self.handlers[name] = handler
        print(f"‚úÖ Gestionnaire d'alerte '{name}' ajout√©")
    
    async def trigger_alert(self, alert: Alert):
        """D√©clenche une alerte et notifie tous les gestionnaires"""
        # V√©rification des r√®gles de suppression
        if self._is_suppressed(alert):
            return
        
        self.alert_history.append(alert)
        
        print(f"üö® [{alert.severity.value.upper()}] {alert.component}: {alert.message}")
        
        # Notification via tous les gestionnaires configur√©s
        for name, handler in self.handlers.items():
            try:
                await handler(alert)
            except Exception as e:
                logging.error(f"Erreur gestionnaire {name}: {e}")
    
    def _is_suppressed(self, alert: Alert) -> bool:
        """V√©rifie si l'alerte doit √™tre supprim√©e"""
        rule_key = f"{alert.component}_{alert.severity.value}"
        if rule_key in self.suppression_rules:
            # Logique de suppression bas√©e sur la fr√©quence
            recent_similar = [
                a for a in self.alert_history[-10:]
                if a.component == alert.component and a.severity == alert.severity
            ]
            return len(recent_similar) > self.suppression_rules[rule_key]['max_per_period']
        return False
    
    def set_suppression_rule(self, component: str, severity: AlertSeverity, max_per_period: int):
        """Configure une r√®gle de suppression d'alertes"""
        rule_key = f"{component}_{severity.value}"
        self.suppression_rules[rule_key] = {'max_per_period': max_per_period}

# √âtape 2: Gestionnaires de notification personnalis√©s
class EmailNotifier:
    """Notificateur email"""
    
    def __init__(self, smtp_server: str, smtp_port: int, username: str, password: str):
        self.smtp_server = smtp_server
        self.smtp_port = smtp_port
        self.username = username
        self.password = password
    
    async def send_alert(self, alert: Alert):
        """Envoie une alerte par email"""
        subject = f"[{alert.severity.value.upper()}] Alerte Surveillance - {alert.component}"
        
        body = f"""
        Alerte de surveillance d√©tect√©e:
        
        Composant: {alert.component}
        S√©v√©rit√©: {alert.severity.value}
        Message: {alert.message}
        Timestamp: {datetime.fromtimestamp(alert.timestamp)}
        
        M√©triques:
        {json.dumps(alert.metrics, indent=2)}
        """
        
        msg = MimeText(body)
        msg['Subject'] = subject
        msg['From'] = self.username
        msg['To'] = "admin@surveillance.com"  # Configurez votre destinataire
        
        try:
            with smtplib.SMTP(self.smtp_server, self.smtp_port) as server:
                server.starttls()
                server.login(self.username, self.password)
                server.send_message(msg)
                print(f"üìß Email envoy√© pour alerte {alert.id}")
        except Exception as e:
            print(f"‚ùå Erreur envoi email: {e}")

class SlackNotifier:
    """Notificateur Slack"""
    
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url
    
    async def send_alert(self, alert: Alert):
        """Envoie une alerte vers Slack"""
        color_map = {
            AlertSeverity.INFO: "#36a64f",
            AlertSeverity.WARNING: "#ff9900",
            AlertSeverity.ERROR: "#ff3300",
            AlertSeverity.CRITICAL: "#990000"
        }
        
        payload = {
            "attachments": [
                {
                    "color": color_map[alert.severity],
                    "title": f"Alerte {alert.severity.value.upper()} - {alert.component}",
                    "text": alert.message,
                    "fields": [
                        {"title": "Timestamp", "value": datetime.fromtimestamp(alert.timestamp).strftime("%Y-%m-%d %H:%M:%S"), "short": True},
                        {"title": "Composant", "value": alert.component, "short": True}
                    ]
                }
            ]
        }
        
        try:
            response = requests.post(self.webhook_url, json=payload)
            if response.status_code == 200:
                print(f"üí¨ Notification Slack envoy√©e pour alerte {alert.id}")
            else:
                print(f"‚ùå Erreur Slack: {response.status_code}")
        except Exception as e:
            print(f"‚ùå Erreur notification Slack: {e}")

class FileLogger:
    """Enregistreur de fichier pour les alertes"""
    
    def __init__(self, filename: str = "alerts.log"):
        self.filename = filename
    
    async def send_alert(self, alert: Alert):
        """Enregistre l'alerte dans un fichier"""
        log_entry = f"{datetime.fromtimestamp(alert.timestamp)} | {alert.severity.value.upper()} | {alert.component} | {alert.message}\n"
        
        try:
            with open(self.filename, "a") as f:
                f.write(log_entry)
            print(f"üìù Alerte enregistr√©e dans {self.filename}")
        except Exception as e:
            print(f"‚ùå Erreur enregistrement fichier: {e}")

# √âtape 3: Configuration et test du syst√®me d'alertes
async def setup_alerting_system():
    """Configure le syst√®me d'alertes complet"""
    alert_handler = AlertHandler()
    
    # Configuration des gestionnaires
    # Email (configurez avec vos vrais param√®tres)
    # email_notifier = EmailNotifier("smtp.gmail.com", 587, "your-email", "your-password")
    # alert_handler.add_handler("email", email_notifier.send_alert)
    
    # Slack (configurez avec votre webhook)
    # slack_notifier = SlackNotifier("https://hooks.slack.com/your-webhook")
    # alert_handler.add_handler("slack", slack_notifier.send_alert)
    
    # Fichier log (toujours actif)
    file_logger = FileLogger("surveillance_alerts.log")
    alert_handler.add_handler("file", file_logger.send_alert)
    
    # Configuration des r√®gles de suppression
    alert_handler.set_suppression_rule("VLM", AlertSeverity.WARNING, 3)
    alert_handler.set_suppression_rule("System", AlertSeverity.INFO, 5)
    
    return alert_handler

# √âtape 4: Test avec alertes simul√©es
async def test_alerting_system():
    alert_handler = await setup_alerting_system()
    
    # Test diff√©rents types d'alertes
    test_alerts = [
        Alert(
            id="test_001",
            timestamp=time.time(),
            severity=AlertSeverity.WARNING,
            component="VLM",
            message="Temps d'inf√©rence √©lev√© d√©tect√©",
            metrics={"inference_time": 4.5, "model": "claude-3-5-haiku"}
        ),
        Alert(
            id="test_002",
            timestamp=time.time(),
            severity=AlertSeverity.ERROR,
            component="System",
            message="Utilisation m√©moire critique",
            metrics={"memory_percent": 95.2, "available_mb": 512}
        ),
        Alert(
            id="test_003",
            timestamp=time.time(),
            severity=AlertSeverity.CRITICAL,
            component="Detection",
            message="Syst√®me de d√©tection arr√™t√©",
            metrics={"fps": 0, "error": "Camera disconnected"}
        )
    ]
    
    print("üß™ Test du syst√®me d'alertes...")
    
    for alert in test_alerts:
        await alert_handler.trigger_alert(alert)
        await asyncio.sleep(1)  # D√©lai entre alertes
    
    print(f"\nüìä {len(alert_handler.alert_history)} alertes trait√©es")
    return alert_handler
```

#### **üî¨ Exercice 6.3: Analyse et optimisation des performances**
```python
# √âtape 1: Analyseur de performance automatis√©
# performance_analyzer.py
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, List, Tuple
import json

class PerformanceAnalyzer:
    """Analyseur automatique des performances du syst√®me"""
    
    def __init__(self, metrics_file: str):
        self.metrics_file = metrics_file
        self.data = self._load_metrics()
        self.analysis_results = {}
    
    def _load_metrics(self) -> Dict:
        """Charge les m√©triques depuis le fichier d'export"""
        try:
            with open(self.metrics_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ùå Erreur chargement m√©triques: {e}")
            return {}
    
    def analyze_system_performance(self) -> Dict:
        """Analyse les performances syst√®me"""
        if not self.data.get('metrics_history', {}).get('system'):
            return {"error": "Pas de donn√©es syst√®me"}
        
        system_data = pd.DataFrame(self.data['metrics_history']['system'])
        
        analysis = {
            'cpu_stats': {
                'mean': system_data['cpu_percent'].mean(),
                'max': system_data['cpu_percent'].max(),
                'std': system_data['cpu_percent'].std(),
                'peaks': len(system_data[system_data['cpu_percent'] > 80])
            },
            'memory_stats': {
                'mean': system_data['memory_percent'].mean(),
                'max': system_data['memory_percent'].max(),
                'std': system_data['memory_percent'].std(),
                'peaks': len(system_data[system_data['memory_percent'] > 80])
            },
            'gpu_stats': {
                'mean': system_data['gpu_percent'].mean(),
                'max': system_data['gpu_percent'].max(),
                'utilization_efficiency': (system_data['gpu_percent'] > 10).sum() / len(system_data)
            },
            'recommendations': []
        }
        
        # G√©n√©ration de recommandations
        if analysis['cpu_stats']['mean'] > 70:
            analysis['recommendations'].append("‚ö†Ô∏è CPU moyen √©lev√© - Consid√©rez l'optimisation du code")
        
        if analysis['memory_stats']['peaks'] > 10:
            analysis['recommendations'].append("‚ö†Ô∏è Pics m√©moire fr√©quents - V√©rifiez les fuites m√©moire")
        
        if analysis['gpu_stats']['utilization_efficiency'] < 0.3:
            analysis['recommendations'].append("üí° GPU sous-utilis√© - Optimisez l'utilisation GPU")
        
        self.analysis_results['system'] = analysis
        return analysis
    
    def analyze_vlm_performance(self) -> Dict:
        """Analyse les performances des mod√®les VLM"""
        if not self.data.get('metrics_history', {}).get('vlm'):
            return {"error": "Pas de donn√©es VLM"}
        
        vlm_data = pd.DataFrame(self.data['metrics_history']['vlm'])
        
        # Analyse par mod√®le
        model_analysis = {}
        for model in vlm_data['model_name'].unique():
            model_data = vlm_data[vlm_data['model_name'] == model]
            
            model_analysis[model] = {
                'avg_inference_time': model_data['inference_time'].mean(),
                'p95_inference_time': model_data['inference_time'].quantile(0.95),
                'success_rate': model_data['success_rate'].mean(),
                'throughput': len(model_data) / (model_data['timestamp'].max() - model_data['timestamp'].min())
            }
        
        # Recommandations
        recommendations = []
        best_model = min(model_analysis.items(), key=lambda x: x[1]['avg_inference_time'])
        worst_model = max(model_analysis.items(), key=lambda x: x[1]['avg_inference_time'])
        
        if worst_model[1]['avg_inference_time'] > best_model[1]['avg_inference_time'] * 2:
            recommendations.append(f"üöÄ Consid√©rez utiliser {best_model[0]} au lieu de {worst_model[0]}")
        
        analysis = {
            'models': model_analysis,
            'overall_avg_time': vlm_data['inference_time'].mean(),
            'overall_success_rate': vlm_data['success_rate'].mean(),
            'recommendations': recommendations
        }
        
        self.analysis_results['vlm'] = analysis
        return analysis
    
    def analyze_detection_performance(self) -> Dict:
        """Analyse les performances de d√©tection"""
        if not self.data.get('metrics_history', {}).get('detection'):
            return {"error": "Pas de donn√©es de d√©tection"}
        
        detection_data = pd.DataFrame(self.data['metrics_history']['detection'])
        
        analysis = {
            'fps_stats': {
                'mean': detection_data['fps'].mean(),
                'min': detection_data['fps'].min(),
                'stable_periods': self._find_stable_periods(detection_data['fps'])
            },
            'latency_stats': {
                'mean': detection_data['processing_latency'].mean(),
                'p95': detection_data['processing_latency'].quantile(0.95),
                'spikes': len(detection_data[detection_data['processing_latency'] > detection_data['processing_latency'].quantile(0.95)])
            },
            'tracking_stats': {
                'mean_accuracy': detection_data['tracking_accuracy'].mean(),
                'consistency': detection_data['tracking_accuracy'].std()
            },
            'recommendations': []
        }
        
        # Recommandations
        if analysis['fps_stats']['mean'] < 15:
            analysis['recommendations'].append("‚ö†Ô∏è FPS faible - Optimisez le pipeline de d√©tection")
        
        if analysis['latency_stats']['spikes'] > 10:
            analysis['recommendations'].append("‚ö†Ô∏è Pics de latence fr√©quents - V√©rifiez la charge syst√®me")
        
        self.analysis_results['detection'] = analysis
        return analysis
    
    def _find_stable_periods(self, fps_series: pd.Series, threshold: float = 2.0) -> int:
        """Trouve les p√©riodes de FPS stable"""
        rolling_std = fps_series.rolling(window=10).std()
        return (rolling_std < threshold).sum()
    
    def generate_performance_report(self) -> str:
        """G√©n√®re un rapport complet de performance"""
        report = []
        report.append("# üìä RAPPORT D'ANALYSE DE PERFORMANCE")
        report.append(f"G√©n√©r√© le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Analyse syst√®me
        sys_analysis = self.analyze_system_performance()
        if 'error' not in sys_analysis:
            report.append("## üñ•Ô∏è Performances Syst√®me")
            report.append(f"CPU moyen: {sys_analysis['cpu_stats']['mean']:.1f}%")
            report.append(f"M√©moire moyenne: {sys_analysis['memory_stats']['mean']:.1f}%")
            report.append(f"GPU moyen: {sys_analysis['gpu_stats']['mean']:.1f}%")
            
            if sys_analysis['recommendations']:
                report.append("\n### Recommandations Syst√®me:")
                for rec in sys_analysis['recommendations']:
                    report.append(f"- {rec}")
            report.append("")
        
        # Analyse VLM
        vlm_analysis = self.analyze_vlm_performance()
        if 'error' not in vlm_analysis:
            report.append("## üß† Performances VLM")
            report.append(f"Temps inf√©rence moyen: {vlm_analysis['overall_avg_time']:.2f}s")
            report.append(f"Taux de succ√®s: {vlm_analysis['overall_success_rate']*100:.1f}%")
            
            if vlm_analysis['recommendations']:
                report.append("\n### Recommandations VLM:")
                for rec in vlm_analysis['recommendations']:
                    report.append(f"- {rec}")
            report.append("")
        
        # Analyse d√©tection
        det_analysis = self.analyze_detection_performance()
        if 'error' not in det_analysis:
            report.append("## üëÅÔ∏è Performances D√©tection")
            report.append(f"FPS moyen: {det_analysis['fps_stats']['mean']:.1f}")
            report.append(f"Latence moyenne: {det_analysis['latency_stats']['mean']:.3f}s")
            report.append(f"Pr√©cision tracking: {det_analysis['tracking_stats']['mean_accuracy']*100:.1f}%")
            
            if det_analysis['recommendations']:
                report.append("\n### Recommandations D√©tection:")
                for rec in det_analysis['recommendations']:
                    report.append(f"- {rec}")
        
        return "\n".join(report)
    
    def create_performance_dashboard(self) -> str:
        """Cr√©e un dashboard visuel des performances"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Dashboard de Performance - Syst√®me de Surveillance', fontsize=16)
        
        # Graphique CPU/M√©moire
        if self.data.get('metrics_history', {}).get('system'):
            system_data = pd.DataFrame(self.data['metrics_history']['system'])
            axes[0, 0].plot(system_data['cpu_percent'], label='CPU %', color='red')
            axes[0, 0].plot(system_data['memory_percent'], label='M√©moire %', color='blue')
            axes[0, 0].set_title('Utilisation CPU/M√©moire')
            axes[0, 0].set_ylabel('Pourcentage')
            axes[0, 0].legend()
            axes[0, 0].grid(True)
        
        # Graphique FPS
        if self.data.get('metrics_history', {}).get('detection'):
            detection_data = pd.DataFrame(self.data['metrics_history']['detection'])
            axes[0, 1].plot(detection_data['fps'], color='green')
            axes[0, 1].set_title('FPS de D√©tection')
            axes[0, 1].set_ylabel('FPS')
            axes[0, 1].grid(True)
        
        # Graphique temps VLM
        if self.data.get('metrics_history', {}).get('vlm'):
            vlm_data = pd.DataFrame(self.data['metrics_history']['vlm'])
            axes[1, 0].plot(vlm_data['inference_time'], color='purple')
            axes[1, 0].set_title('Temps Inf√©rence VLM')
            axes[1, 0].set_ylabel('Secondes')
            axes[1, 0].grid(True)
        
        # Graphique pr√©cision tracking
        if self.data.get('metrics_history', {}).get('detection'):
            axes[1, 1].plot(detection_data['tracking_accuracy'], color='orange')
            axes[1, 1].set_title('Pr√©cision Tracking')
            axes[1, 1].set_ylabel('Pr√©cision')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        dashboard_file = f"performance_dashboard_{int(time.time())}.png"
        plt.savefig(dashboard_file)
        plt.show()
        
        return dashboard_file

# √âtape 4: Utilisation compl√®te de l'analyseur
async def run_performance_analysis():
    """Lance une analyse compl√®te des performances"""
    # D'abord, g√©n√©rer des m√©triques
    print("üîÑ G√©n√©ration des m√©triques...")
    surveillance = SurveillanceWithMonitoring()
    metrics_file = await surveillance.run_with_monitoring()
    
    # Ensuite, analyser
    print("üìä Analyse des performances...")
    analyzer = PerformanceAnalyzer(metrics_file)
    
    # G√©n√©ration du rapport
    report = analyzer.generate_performance_report()
    
    # Sauvegarde du rapport
    report_file = f"performance_report_{int(time.time())}.md"
    with open(report_file, 'w') as f:
        f.write(report)
    
    print(f"üìÑ Rapport g√©n√©r√©: {report_file}")
    print("\n" + report)
    
    # Cr√©ation du dashboard
    dashboard_file = analyzer.create_performance_dashboard()
    print(f"üìà Dashboard cr√©√©: {dashboard_file}")
    
    return report_file, dashboard_file
```

#### **üéØ Objectifs d'Apprentissage - Module 6:**
- [ ] Ma√Ætriser la collecte de m√©triques syst√®me, VLM et d√©tection
- [ ] Cr√©er des syst√®mes d'alertes personnalis√©es multi-canaux
- [ ] Analyser automatiquement les performances et identifier les goulots
- [ ] G√©n√©rer des rapports et dashboards de monitoring
- [ ] Optimiser les performances bas√©es sur l'analyse des m√©triques

---

## üéØ **√âTAPE FINALE: Int√©gration Compl√®te**

### **Projet Pratique - Mission Finale**

#### **üéØ Mission : Syst√®me de surveillance magasin complet**
```python
# master_surveillance_project.py
# MISSION FINALE - Int√©gration de tous les modules appris

import asyncio
import time
from pathlib import Path
from config.config_loader import load_config
from src.core.headless.surveillance_system import HeadlessSurveillanceSystem
from src.core.vlm.dynamic_model import DynamicVisionLanguageModel
from src.detection.yolo_detector import YOLODetector
from src.advanced_tools.sam2_segmentation import SAM2Segmentator
from src.core.monitoring.performance_monitor import ComprehensiveMonitor

class RetailSurveillanceSystem:
    """Syst√®me de surveillance retail complet - Projet final"""
    
    def __init__(self):
        self.config = load_config()
        self.performance_monitor = ComprehensiveMonitor()
        self.surveillance_zones = {
            'entrance': {'alert_threshold': 0.7, 'tools': ['pose', 'sam2']},
            'cashier': {'alert_threshold': 0.9, 'tools': ['pose', 'trajectory', 'emotion']},
            'aisles': {'alert_threshold': 0.5, 'tools': ['pose', 'tracking']},
            'stockroom': {'alert_threshold': 0.8, 'tools': ['sam2', 'trajectory']}
        }
        
    async def deploy_retail_surveillance(self):
        """D√©ploie le syst√®me de surveillance retail"""
        print("üè™ D√©ploiement du syst√®me de surveillance retail...")
        
        # 1. Configuration personnalis√©e
        await self._configure_for_retail()
        
        # 2. Initialisation des composants
        surveillance_system = await self._initialize_components()
        
        # 3. D√©marrage monitoring
        monitor_task = asyncio.create_task(
            self.performance_monitor.start_monitoring()
        )
        
        try:
            # 4. Lancement surveillance par zones
            await self._run_multi_zone_surveillance(surveillance_system)
            
        finally:
            # 5. Export des r√©sultats et rapport final
            await self._generate_final_report()
            self.performance_monitor.running = False
            await monitor_task
    
    async def _configure_for_retail(self):
        """Configuration sp√©cialis√©e retail"""
        # Personnalisation des seuils pour environnement commercial
        self.config.detection.confidence_threshold = 0.6
        self.config.vlm.mode = "BALANCED"
        
        # Prompts sp√©cialis√©s retail
        self.retail_prompts = {
            'theft_detection': """
            Analysez cette sc√®ne de magasin pour d√©tecter des comportements suspects:
            - Vol √† l'√©talage potentiel
            - Manipulation suspecte de produits
            - Comportements d'√©vasion
            - Interactions anormales avec le personnel
            
            R√©pondez avec niveau de risque et actions recommand√©es.
            """,
            'customer_flow': """
            Analysez le flux de clients dans cette zone:
            - Densit√© de personnes
            - Patterns de mouvement
            - Zones de congestion
            - Efficacit√© de la circulation
            
            Sugg√©rez des optimisations d'agencement.
            """
        }
        
        print("‚úÖ Configuration retail appliqu√©e")
    
    async def _initialize_components(self):
        """Initialise tous les composants du syst√®me"""
        # VLM avec mod√®les multiples pour robustesse
        vlm_primary = DynamicVisionLanguageModel("claude-3-5-haiku-20241022")
        vlm_fallback = DynamicVisionLanguageModel("gpt-4o-mini-2024-07-18")
        
        # D√©tecteurs sp√©cialis√©s
        detector = YOLODetector()
        detector.confidence_threshold = 0.6
        
        # Outils avanc√©s selon les zones
        advanced_tools = {
            'sam2': SAM2Segmentator(),
            # Ajoutez d'autres outils selon vos modules pr√©c√©dents
        }
        
        # Syst√®me principal
        surveillance_system = HeadlessSurveillanceSystem(
            config=self.config,
            vlm_model=vlm_primary,
            detector=detector,
            advanced_tools=advanced_tools
        )
        
        await surveillance_system.initialize()
        print("‚úÖ Composants initialis√©s")
        
        return surveillance_system
    
    async def _run_multi_zone_surveillance(self, system):
        """Surveillance multi-zones avec logique adaptative"""
        print("üé• D√©marrage surveillance multi-zones...")
        
        zone_results = {}
        
        for zone_name, zone_config in self.surveillance_zones.items():
            print(f"\nüìç Zone: {zone_name.upper()}")
            
            # Configuration adaptative par zone
            system.configure_for_zone(
                alert_threshold=zone_config['alert_threshold'],
                active_tools=zone_config['tools']
            )
            
            # Simulation surveillance de zone (remplacez par vraie cam√©ra)
            zone_video = f"data/retail_videos/{zone_name}_sample.mp4"
            if Path(zone_video).exists():
                results = await system.process_video(zone_video, max_frames=50)
                zone_results[zone_name] = results
                
                # Analyse des r√©sultats de zone
                self._analyze_zone_results(zone_name, results)
            else:
                print(f"‚ö†Ô∏è  Vid√©o de zone non trouv√©e: {zone_video}")
        
        return zone_results
    
    def _analyze_zone_results(self, zone_name, results):
        """Analyse les r√©sultats par zone avec recommandations"""
        if not results:
            return
        
        total_frames = len(results)
        alert_frames = len([r for r in results if r.alert_level in ['HIGH', 'CRITICAL']])
        avg_confidence = sum([r.confidence_score for r in results]) / total_frames
        
        print(f"   üìä Analyse zone {zone_name}:")
        print(f"   - Frames trait√©es: {total_frames}")
        print(f"   - Alertes d√©tect√©es: {alert_frames}")
        print(f"   - Confiance moyenne: {avg_confidence:.2f}")
        
        # Recommandations sp√©cifiques par zone
        if zone_name == 'entrance' and alert_frames > total_frames * 0.3:
            print("   üí° Recommandation: Augmentez la surveillance √† l'entr√©e")
        
        elif zone_name == 'cashier' and avg_confidence < 0.7:
            print("   üí° Recommandation: Am√©liorez l'√©clairage aux caisses")
        
        elif zone_name == 'aisles' and alert_frames < total_frames * 0.1:
            print("   üí° Recommandation: Les all√©es sont s√©curis√©es")
    
    async def _generate_final_report(self):
        """G√©n√®re le rapport final complet"""
        print("\nüìã G√©n√©ration du rapport final...")
        
        # Export des m√©triques
        metrics_file = self.performance_monitor.export_metrics("retail_metrics_final.json")
        
        # Rapport personnalis√© retail
        report = self._create_retail_report()
        
        # Sauvegarde
        report_file = f"RETAIL_SURVEILLANCE_REPORT_{int(time.time())}.md"
        with open(report_file, 'w') as f:
            f.write(report)
        
        print(f"‚úÖ Rapport final g√©n√©r√©: {report_file}")
        print(f"‚úÖ M√©triques export√©es: {metrics_file}")
        
        return report_file, metrics_file
    
    def _create_retail_report(self):
        """Cr√©e un rapport sp√©cialis√© retail"""
        report_sections = [
            "# üè™ RAPPORT DE SURVEILLANCE RETAIL",
            f"G√©n√©r√© le: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## üìä R√©sum√© Ex√©cutif",
            "Ce rapport pr√©sente l'analyse compl√®te du syst√®me de surveillance retail.",
            "",
            "## üéØ Zones Surveill√©es",
            "- **Entr√©e**: D√©tection de comportements suspects",
            "- **Caisses**: Pr√©vention des vols et analyse des files",  
            "- **All√©es**: Monitoring du comportement client",
            "- **R√©serve**: S√©curit√© du stock",
            "",
            "## üìà M√©triques de Performance",
            f"- Syst√®me op√©rationnel: {self.performance_monitor.running}",
            f"- Alertes g√©n√©r√©es: {len(self.performance_monitor.alerts)}",
            "",
            "## üí° Recommandations",
            "1. ‚úÖ Le syst√®me fonctionne correctement",
            "2. üîß Ajustements sugg√©r√©s par zone (voir d√©tails)",
            "3. üìä Monitoring continu recommand√©",
            "",
            "## üéØ Prochaines √âtapes",
            "1. D√©ploiement en production",
            "2. Formation √©quipe s√©curit√©", 
            "3. Optimisation continue bas√©e sur les donn√©es",
            "",
            "---",
            "*Rapport g√©n√©r√© automatiquement par le syst√®me de surveillance intelligent*"
        ]
        
        return "\n".join(report_sections)

# Script principal pour lancer le projet final
async def run_master_project():
    """Lance le projet de ma√Ætrise complet"""
    print("üöÄ PROJET FINAL - SYST√àME DE SURVEILLANCE RETAIL")
    print("=" * 60)
    
    retail_system = RetailSurveillanceSystem()
    
    try:
        await retail_system.deploy_retail_surveillance()
        print("\nüéâ MISSION ACCOMPLIE!")
        print("Vous ma√Ætrisez maintenant l'architecture compl√®te du syst√®me de surveillance.")
        
    except Exception as e:
        print(f"‚ùå Erreur durant la mission: {e}")
        print("üí° Revoyez les modules pr√©c√©dents et r√©essayez.")

# Point d'entr√©e
if __name__ == "__main__":
    asyncio.run(run_master_project())
```

#### **üéØ Checklist de Validation de Ma√Ætrise**

Avant de consid√©rer le projet termin√©, v√©rifiez que vous pouvez :

**Configuration (Module 1)**
- [ ] Modifier les param√®tres CLI et ENV sans regarder la documentation
- [ ] Cr√©er une nouvelle configuration pour un cas d'usage sp√©cifique
- [ ] D√©bugger les probl√®mes de configuration

**Syst√®me Headless (Module 2)** 
- [ ] Expliquer le flow complet d'une frame de A √† Z
- [ ] Modifier les seuils d'alerte et pr√©dire l'impact
- [ ] Optimiser les performances en ajustant les param√®tres

**Mod√®les VLM (Module 3)**
- [ ] Changer de mod√®le VLM et ajuster les prompts
- [ ] Cr√©er des prompts personnalis√©s pour votre domaine
- [ ] Comprendre les trade-offs entre modes FAST/BALANCED/THOROUGH

**D√©tection et Tracking (Module 4)**
- [ ] Tuner YOLO pour optimiser d√©tection/performance
- [ ] Ajuster les param√®tres BYTETracker selon vos besoins
- [ ] Cr√©er des visualisations personnalis√©es

**Outils Avanc√©s (Module 5)**
- [ ] Activer/d√©sactiver des outils selon le contexte
- [ ] Cr√©er votre propre outil avanc√© personnalis√©
- [ ] Comprendre l'impact performance de chaque outil

**Monitoring (Module 6)**
- [ ] Configurer un monitoring complet temps r√©el
- [ ] Cr√©er des alertes personnalis√©es multi-canaux
- [ ] Analyser et optimiser les performances du syst√®me

#### **üèÜ Niveaux de Ma√Ætrise**

**ü•â Bronze - Utilisateur Avanc√©**
- Peut utiliser le syst√®me avec diff√©rentes configurations
- Comprend les param√®tres principaux et leur impact
- Capable de diagnostiquer les probl√®mes courants

**ü•à Argent - D√©veloppeur Syst√®me** 
- Peut modifier et √©tendre les composants existants
- Cr√©e des outils personnalis√©s pour ses besoins
- Optimise les performances selon ses contraintes

**ü•á Or - Architecte Expert**
- Ma√Ætrise l'architecture compl√®te et ses interactions
- Peut concevoir des syst√®mes d√©riv√©s pour d'autres domaines
- Contribue √† l'am√©lioration de l'architecture g√©n√©rale

#### **üéì Certification de Ma√Ætrise**

Une fois tous les exercices compl√©t√©s et le projet final r√©ussi :

1. **Portfolio de Preuves** : Conservez tous vos scripts, rapports et dashboards
2. **Projet Personnel** : Adaptez le syst√®me √† votre propre cas d'usage
3. **Documentation** : Cr√©ez votre propre guide pour votre configuration sp√©cifique
4. **Partage** : Contribuez √† l'am√©lioration du syst√®me avec vos optimisations

### **Ma√Ætrise Valid√©e Quand Vous Pouvez :**
1. ‚úÖ Modifier n'importe quel param√®tre et pr√©dire l'impact
2. ‚úÖ Ajouter un nouveau type de d√©tection
3. ‚úÖ Cr√©er un outil personnalis√©
4. ‚úÖ Optimiser les performances pour votre use-case
5. ‚úÖ D√©bugger et corriger n'importe quelle erreur
6. ‚úÖ Expliquer chaque ligne de code du flow principal

---

## üìä **Ordre d'Apprentissage Recommand√©**

### **Semaine 1 : Fondations**
- Configuration et point d'entr√©e
- Flow principal du syst√®me headless
- Mod√®les de donn√©es

### **Semaine 2 : C≈ìur Technique** 
- D√©tection YOLO + Tracking
- Syst√®me VLM et orchestration
- Tests et validation

### **Semaine 3 : Avanc√©**
- Outils IA sp√©cialis√©s
- Monitoring et performances
- Optimisations personnalis√©es

### **Semaine 4 : Ma√Ætrise**
- Projet complet de A √† Z
- Personnalisations avanc√©es
- D√©bogage et troubleshooting

## üéØ **Ressources d'Apprentissage**

### **Documentation √† Lire**
- Docstrings de chaque classe principale
- Tests unitaires (exemples d'usage)
- `ARCHITECTURE_VALIDATION_REPORT.md`

### **Commandes Essentielles**
```bash
# Tests complets
pytest tests/ -v

# Tests sp√©cifiques
pytest tests/test_vlm_extended.py -v
pytest tests/test_headless_system.py -v

# Lancement avec debug
python main_headless_refactored.py --debug --max-frames 10

# Monitoring performances
python -c "from src.core.monitoring import *; # Code monitoring"
```

---

**üéì Objectif Final :** Vous devez pouvoir modifier, √©tendre et optimiser chaque composant du syst√®me selon vos besoins sp√©cifiques de surveillance.