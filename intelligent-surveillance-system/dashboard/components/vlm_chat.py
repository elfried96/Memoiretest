"""Interface de chat avanc√©e avec VLM."""

import streamlit as st
import asyncio
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime
import json
import uuid
import base64
from pathlib import Path

from services.session_manager import get_session_manager
from config.settings import get_dashboard_config
from services.vlm_integration import get_vlm_service
from services.video_memory_system import get_video_memory_system

class VLMChatInterface:
    """Interface de chat avanc√©e avec le VLM."""
    
    def __init__(self):
        self.session = get_session_manager()
        self.config = get_dashboard_config()
        self.vlm_callback: Optional[Callable] = None
        self.context_data = {}
        
        # Service VLM int√©gr√© du projet
        self.vlm_service = get_vlm_service()
        
        # ‚úÖ NOUVEAU: Syst√®me m√©moire vid√©o avanc√©
        self.video_memory_system = get_video_memory_system()
        
        # ‚úÖ NOUVEAU: Contexte vid√©o actuel
        self.current_video_context = None
        
        # Questions pr√©d√©finies
        self.predefined_questions = {
            "Analyse g√©n√©rale": [
                "Que voyez-vous dans cette vid√©o ?",
                "Y a-t-il des comportements suspects ?",
                "Quel est le niveau de risque global ?",
                "R√©sumez les √©v√©nements principaux"
            ],
            "D√©tection sp√©cifique": [
                "Combien de personnes sont visibles ?",
                "Y a-t-il des objets abandonn√©s ?",
                "D√©tectez-vous des gestes suspects ?",
                "Y a-t-il des mouvements anormaux ?"
            ],
            "S√©curit√©": [
                "√âvaluez le niveau de menace",
                "Y a-t-il violation de zones interdites ?",
                "D√©tectez-vous des intrusions ?",
                "Analysez les comportements √† risque"
            ],
            "Contextuel": [
                "Que s'est-il pass√© √† ce moment pr√©cis ?",
                "Expliquez cette s√©quence d'√©v√©nements",
                "Pourquoi cette alerte a-t-elle √©t√© d√©clench√©e ?",
                "Analysez cette zone sp√©cifique"
            ]
        }
    
    def render_chat_interface(self):
        """Affiche l'interface de chat compl√®te."""
        
        st.subheader("Chat avec l'IA de Surveillance")
        
        # Onglets pour organiser l'interface
        tab1, tab2, tab3 = st.tabs(["Conversation", "Questions", "Contexte"])
        
        with tab1:
            self._render_conversation_tab()
        
        with tab2:
            self._render_questions_tab()
        
        with tab3:
            self._render_context_tab()
    
    def _render_conversation_tab(self):
        """Onglet conversation principale."""
        
        # Historique des messages
        messages = self.session.get_chat_history()
        
        if not messages:
            st.info("üëã Bonjour ! Je suis votre assistant IA de surveillance. Posez-moi des questions sur les vid√©os analys√©es.")
        
        # Container pour les messages avec hauteur fixe
        chat_container = st.container()
        
        with chat_container:
            for message in messages:
                self._render_message(message)
        
        # Zone de saisie
        self._render_input_area()
    
    def _render_message(self, message: Dict[str, Any]):
        """Affiche un message individuel."""
        
        role = message.get('role', 'user')
        content = message.get('content', '')
        timestamp = message.get('timestamp', '')
        metadata = message.get('metadata', {})
        
        # Formatage timestamp
        try:
            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            time_str = dt.strftime("%H:%M:%S")
        except:
            time_str = ""
        
        if role == 'user':
            # Message utilisateur
            with st.chat_message("user"):
                col1, col2 = st.columns([4, 1])
                with col1:
                    st.write(content)
                with col2:
                    if time_str:
                        st.caption(time_str)
        
        else:
            # Message assistant
            with st.chat_message("assistant"):
                col1, col2 = st.columns([4, 1])
                with col1:
                    st.write(content)
                    
                    # M√©tadonn√©es si disponibles
                    if metadata:
                        with st.expander("D√©tails de l'analyse", expanded=False):
                            if 'confidence' in metadata:
                                confidence = metadata['confidence']
                                # Syst√®me feu tricolore pour utilisateurs non-techniciens
                                if confidence > 0.8:
                                    st.success("üü¢ **Analyse tr√®s fiable** - L'IA est s√ªre de son analyse")
                                elif confidence > 0.6:
                                    st.warning("üü° **Analyse correcte** - V√©rification recommand√©e")
                                else:
                                    st.error("üî¥ **Analyse incertaine** - V√©rification manuelle n√©cessaire")
                                
                                st.progress(confidence, f"Niveau de confiance: {confidence:.0%}")
                            
                            if 'analysis_time' in metadata:
                                st.metric("Temps d'analyse", f"{metadata['analysis_time']:.2f}s")
                            
                            if 'tools_used' in metadata:
                                st.write("Outils utilis√©s:", ", ".join(metadata['tools_used']))
                
                with col2:
                    if time_str:
                        st.caption(time_str)
                    
                    # Boutons d'actions
                    self._render_message_actions(message)
    
    def _render_message_actions(self, message: Dict[str, Any]):
        """Affiche les actions pour un message."""
        
        message_id = message.get('id', '')
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("üëç", key=f"like_{message_id}", help="Utile"):
                self._rate_message(message_id, "like")
        
        with col2:
            if st.button("üëé", key=f"dislike_{message_id}", help="Pas utile"):
                self._rate_message(message_id, "dislike")
    
    def _render_input_area(self):
        """Zone de saisie des messages."""
        
        # Chat input principal
        user_input = st.chat_input("Posez votre question sur la surveillance...")
        
        # Boutons raccourcis
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("Analyser maintenant", use_container_width=True):
                # V√©rification VLM avant analyse
                if not self.get_vlm_status()['initialized']:
                    st.warning("‚ö†Ô∏è VLM non initialis√©. Allez dans l'onglet Contexte pour l'initialiser.")
                else:
                    self._quick_analyze()
        
        # ‚úÖ NOUVEAU: Indicateur contexte vid√©o actif
        if self.current_video_context:
            st.info(f"üéØ Contexte vid√©o actif: {self.current_video_context.get('video_name', 'vid√©o')}")
        
        with col2:
            if st.button("Quelles alertes ?", use_container_width=True):
                self._process_user_input("Quelles sont les alertes actives et leur niveau de priorit√© ?")
                st.rerun()
        
        with col3:
            if st.button("√âtat syst√®me", use_container_width=True):
                self._process_user_input("Quel est l'√©tat actuel du syst√®me de surveillance ?")
                st.rerun()
        
        with col4:
            if st.button("Effacer chat", use_container_width=True, type="secondary"):
                self._clear_chat()
        
        # Traitement de l'input
        if user_input:
            self._process_user_input(user_input)
    
    def _render_questions_tab(self):
        """Onglet questions pr√©d√©finies."""
        
        st.write("S√©lectionnez une question pr√©d√©finie ou parcourez par cat√©gorie :")
        
        # S√©lection par cat√©gorie
        selected_category = st.selectbox(
            "Cat√©gorie",
            list(self.predefined_questions.keys())
        )
        
        # Questions de la cat√©gorie
        if selected_category:
            questions = self.predefined_questions[selected_category]
            
            for i, question in enumerate(questions):
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    st.write(f"**{i+1}.** {question}")
                
                with col2:
                    if st.button("‚ñ∂", key=f"ask_{selected_category}_{i}"):
                        self._process_user_input(question)
                        st.rerun()
        
        # Section questions personnalis√©es
        with st.expander("Ajouter une question personnalis√©e"):
            new_category = st.text_input("Nouvelle cat√©gorie (optionnel)")
            new_question = st.text_input("Question personnalis√©e")
            
            if st.button("Sauvegarder"):
                if new_question:
                    category = new_category or "Personnalis√©es"
                    if category not in self.predefined_questions:
                        self.predefined_questions[category] = []
                    
                    self.predefined_questions[category].append(new_question)
                    st.success("Question ajout√©e !")
                    st.rerun()
    
    def _render_context_tab(self):
        """Onglet contexte et donn√©es."""
        
        # Status VLM
        st.subheader("ü§ñ Status du VLM")
        
        vlm_status = self.get_vlm_status()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            status_emoji = "‚úÖ" if vlm_status['initialized'] else "‚ùå"
            st.metric("VLM Status", f"{status_emoji} {'Actif' if vlm_status['initialized'] else 'Inactif'}")
        
        with col2:
            available_emoji = "‚úÖ" if vlm_status['vlm_available'] else "‚ùå"
            st.metric("Disponibilit√©", f"{available_emoji} {'Oui' if vlm_status['vlm_available'] else 'Non'}")
        
        with col3:
            model_emoji = "‚úÖ" if vlm_status['model_loaded'] else "‚ùå"
            st.metric("Mod√®le", f"{model_emoji} {'Charg√©' if vlm_status['model_loaded'] else 'Non charg√©'}")
        
        # Bouton d'initialisation
        if not vlm_status['initialized']:
            if st.button("üöÄ Initialiser VLM", use_container_width=True):
                self.initialize_vlm()
                st.rerun()
        
        # Statistiques si disponibles
        if 'stats' in vlm_status and vlm_status['stats']:
            with st.expander("üìä Statistiques VLM"):
                stats = vlm_status['stats']
                st.metric("Requ√™tes trait√©es", stats.get('queries_processed', 0))
                st.metric("Vid√©os analys√©es", stats.get('videos_analyzed', 0))
                if stats.get('average_response_time'):
                    st.metric("Temps moyen", f"{stats['average_response_time']:.2f}s")
        
        st.divider()
        st.write("**Contexte actuel pour l'IA :**")
        
        # Analyses vid√©o r√©centes
        video_analyses = self.session.get_all_video_analyses()
        if video_analyses:
            st.write("**Analyses vid√©o disponibles:**")
            for video_id, analysis in video_analyses.items():
                with st.expander(f"Vid√©o {video_id[:8]}..."):
                    st.json(analysis)
        
        # √âtat des cam√©ras
        cameras_state = self.session.get_all_cameras_state()
        if cameras_state:
            st.write("**√âtat des cam√©ras:**")
            for cam_id, state in cameras_state.items():
                st.write(f"- **{state.get('name', cam_id)}**: "
                        f"{'Actif' if state.get('enabled', False) else 'Inactif'}")
        
        # ‚úÖ NOUVEAU: Contexte vid√©o actuel
        if self.current_video_context:
            st.write("**Contexte vid√©o actif:**")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Vid√©o", self.current_video_context.get('video_name', 'N/A'))
            
            with col2:
                linked_time = self.current_video_context.get('linked_timestamp', '')
                if linked_time:
                    dt = datetime.fromisoformat(linked_time.replace('Z', '+00:00'))
                    time_str = dt.strftime("%H:%M:%S")
                    st.metric("Li√© √†", time_str)
            
            with col3:
                memory_stats = self.video_memory_system.get_system_stats()
                efficiency = memory_stats.get('memory_efficiency', 0)
                st.metric("Efficacit√© m√©moire", f"{efficiency:.1%}")
            
            # Bouton pour d√©sactiver contexte
            if st.button("üîÑ D√©sactiver contexte vid√©o"):
                self.current_video_context = None
                st.success("Contexte vid√©o d√©sactiv√©")
                st.rerun()
        
        # Alertes r√©centes
        alerts = self.session.get_active_alerts()
        if alerts:
            st.write("**Alertes actives:**")
            for alert in alerts[-5:]:  # 5 derni√®res
                level_emoji = {
                    'LOW': 'üü¢',
                    'MEDIUM': 'üü°', 
                    'HIGH': 'üü†',
                    'CRITICAL': 'üî¥'
                }.get(alert.get('level', 'LOW'), 'üü¢')
                
                st.write(f"{level_emoji} {alert.get('message', 'N/A')}")
        
        # Contexte personnalis√©
        with st.expander("Contexte personnalis√©"):
            custom_context = st.text_area(
                "Informations additionnelles pour l'IA",
                value=self.context_data.get('custom', ''),
                placeholder="Ex: Nous surveillons un magasin ouvert de 9h √† 21h..."
            )
            
            if st.button("Sauvegarder contexte"):
                self.context_data['custom'] = custom_context
                st.success("Contexte sauvegard√© !")
    
    def _process_user_input(self, user_input: str):
        """Traite l'input utilisateur."""
        
        if not user_input.strip():
            return
        
        # Ajout du message utilisateur
        self.session.add_chat_message("user", user_input)
        
        # Pr√©paration du contexte
        context = self._build_context()
        
        # Appel au VLM
        with st.spinner("ü§î L'IA analyse votre question..."):
            try:
                response = self._get_vlm_response(user_input, context)
                
                # Ajout de la r√©ponse
                self.session.add_chat_message(
                    "assistant",
                    response.get('content', 'D√©sol√©, je ne peux pas r√©pondre pour le moment.'),
                    response.get('metadata', {})
                )
                
            except Exception as e:
                error_msg = f"Erreur lors de l'analyse: {str(e)}"
                self.session.add_chat_message("assistant", error_msg)
                st.error(error_msg)
        
        st.rerun()
    
    def _build_context(self) -> Dict[str, Any]:
        """Construit le contexte pour le VLM."""
        
        context = {
            'timestamp': datetime.now().isoformat(),
            'session_id': self.session.get_session_id()
        }
        
        # Analyses vid√©o
        video_analyses = self.session.get_all_video_analyses()
        if video_analyses:
            context['video_analyses'] = video_analyses
        
        # √âtat des cam√©ras
        cameras_state = self.session.get_all_cameras_state()
        if cameras_state:
            context['cameras_state'] = cameras_state
        
        # Alertes
        alerts = self.session.get_active_alerts()
        if alerts:
            context['active_alerts'] = alerts
        
        # Contexte personnalis√©
        if 'custom' in self.context_data:
            context['custom_context'] = self.context_data['custom']
        
        return context
    
    def _get_vlm_response(self, question: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Obtient une r√©ponse du VLM int√©gr√© du projet avec m√©moire vid√©o."""
        
        # ‚úÖ NOUVEAU: Check si question porte sur vid√©o li√©e
        if (self.current_video_context and 
            self._is_video_related_question(question)):
            
            # Query syst√®me m√©moire vid√©o
            video_id = self.current_video_context['video_id']
            conversation_history = self.session.get_chat_history()
            
            memory_response = self.video_memory_system.query_video_memory(
                video_id, question, conversation_history
            )
            
            if memory_response['confidence'] > 0.8:
                # R√©ponse haute confiance bas√©e sur m√©moire
                return {
                    'content': memory_response['response'],
                    'metadata': {
                        'source': 'video_memory_system',
                        'confidence': memory_response['confidence'],
                        'frames_referenced': memory_response['frames_referenced'],
                        'memory_based': True,
                        **memory_response['metadata']
                    }
                }
        
        # Fallback vers VLM service normal
        try:
            # Appel asynchrone au VLM service
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            response = loop.run_until_complete(
                self.vlm_service.process_chat_query(question, context)
            )
            
            loop.close()
            return response
            
        except Exception as e:
            # Fallback avec analyse locale
            return self._local_analysis_response(question, context)
    
    def _quick_analyze(self):
        """Lance une analyse rapide."""
        self._process_user_input("Analysez la situation actuelle et donnez-moi un r√©sum√© des √©v√©nements importants.")
    
    def _clear_chat(self):
        """Efface l'historique du chat."""
        self.session.clear_chat_history()
        st.success("Historique effac√©")
        st.rerun()
    
    def _rate_message(self, message_id: str, rating: str):
        """Note un message."""
        # Sauvegarde de la notation dans la session
        ratings = self.session.get_data('message_ratings', {})
        ratings[message_id] = {
            'rating': rating,
            'timestamp': datetime.now().isoformat()
        }
        self.session.set_data('message_ratings', ratings)
        
        rating_text = "positif" if rating == "like" else "n√©gatif"
        st.success(f"Merci pour votre retour {rating_text} ! Cela nous aide √† am√©liorer l'IA.")
    
    def set_vlm_callback(self, callback: Callable[[str, Dict], Dict]):
        """D√©finit le callback VLM."""
        self.vlm_callback = callback
    
    def link_video_analysis(self, video_id: str, analysis_result: Dict) -> str:
        """
        Lie une analyse vid√©o au chat avec m√©moire persistante.
        """
        
        # Stockage dans syst√®me m√©moire avanc√©
        memory_id = self.video_memory_system.store_video_analysis(
            video_id, analysis_result
        )
        
        # Mise √† jour contexte chat actuel
        self.current_video_context = {
            'video_id': video_id,
            'memory_id': memory_id,
            'video_name': analysis_result.get('video_name', 'vid√©o'),
            'analysis_result': analysis_result,
            'linked_timestamp': datetime.now().isoformat()
        }
        
        # Message syst√®me avec contexte d√©taill√©
        video_name = analysis_result.get('video_name', 'vid√©o')
        duration = analysis_result.get('analysis_time', 'N/A')
        frame_count = len(analysis_result.get('detailed_frames', []))
        
        self.add_system_message(
            f"M√©moire vid√©o activ√©e: {video_name} "
            f"({duration}s, {frame_count} frames analys√©es). "
            f"Je peux maintenant r√©pondre avec des d√©tails pr√©cis sur cette vid√©o.",
            level='success'
        )
        
        return memory_id
    
    def _is_video_related_question(self, question: str) -> bool:
        """
        D√©tecte si question porte sur vid√©o analys√©e.
        """
        
        video_keywords = [
            'vid√©o', 'video', 'voir', 'vu', 'observ√©', 'd√©tect√©',
            'dans cette', 'exactement', 'pr√©cis', 'd√©tail',
            'timeline', 'moment', 'quand', 'combien'
        ]
        
        question_lower = question.lower()
        return any(keyword in question_lower for keyword in video_keywords)
    
    def add_system_message(self, content: str, level: str = "info"):
        """Ajoute un message syst√®me."""
        
        level_prefixes = {
            'info': '‚ÑπÔ∏è',
            'warning': '‚ö†Ô∏è',
            'error': '‚ùå',
            'success': '‚úÖ'
        }
        
        prefix = level_prefixes.get(level, '')
        formatted_content = f"{prefix} **Syst√®me**: {content}"
        
        self.session.add_chat_message(
            "assistant",
            formatted_content,
            {'system_message': True, 'level': level}
        )
    
    def export_chat_history(self, format: str = 'json') -> str:
        """Exporte l'historique du chat."""
        
        history = self.session.get_chat_history()
        
        if format.lower() == 'json':
            return json.dumps(history, indent=2, ensure_ascii=False)
        
        elif format.lower() == 'txt':
            lines = []
            for msg in history:
                timestamp = msg.get('timestamp', '')
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')
                
                lines.append(f"[{timestamp}] {role.upper()}: {content}")
                lines.append("")
            
            return "\n".join(lines)
        
        return json.dumps(history, indent=2, ensure_ascii=False)
    
    def initialize_vlm(self):
        """Initialise le VLM si n√©cessaire."""
        
        if 'vlm_initialized' not in st.session_state:
            with st.spinner("ü§ñ Initialisation du VLM..."):
                try:
                    # Initialisation asynchrone
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    success = loop.run_until_complete(
                        self.vlm_service.initialize_vlm()
                    )
                    
                    loop.close()
                    
                    if success:
                        st.session_state.vlm_initialized = True
                        st.success("‚úÖ VLM initialis√© avec succ√®s!")
                    else:
                        st.warning("‚ö†Ô∏è VLM en mode simulation")
                        
                except Exception as e:
                    st.error(f"‚ùå Erreur initialisation VLM: {e}")
        
        return st.session_state.get('vlm_initialized', False)
    
    def get_vlm_status(self) -> Dict[str, Any]:
        """Retourne le statut du VLM avec m√©moire vid√©o."""
        vlm_status = self.vlm_service.get_system_status()
        
        # ‚úÖ NOUVEAU: Ajout stats m√©moire vid√©o
        memory_stats = self.video_memory_system.get_system_stats()
        vlm_status['video_memory'] = memory_stats
        
        return vlm_status
    
    def _local_analysis_response(self, question: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse d'analyse locale intelligente (fallback)."""
        
        # Analyse des mots-cl√©s de la question
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['risque', 'danger', 'menace', 's√©curit√©']):
            response = self._analyze_security_level(context)
        elif any(word in question_lower for word in ['personne', 'gens', 'individu', 'combien']):
            response = self._analyze_people_count(context)
        elif any(word in question_lower for word in ['alerte', 'alarme', 'incident']):
            response = self._analyze_alerts(context)
        elif any(word in question_lower for word in ['√©tat', 'statut', 'syst√®me']):
            response = self._analyze_system_status(context)
        else:
            response = self._general_analysis(context)
            
        return {
            'content': f"üîÑ **Mode fallback activ√©**\n\n{response}",
            'metadata': {
                'analysis_time': 0.3,
                'fallback_mode': True,
                'confidence': 0.6
            }
        }
    
    def _analyze_security_level(self, context: Dict[str, Any]) -> str:
        """Analyse le niveau de s√©curit√©."""
        alerts = context.get('active_alerts', [])
        if not alerts:
            return "üü¢ **Niveau de s√©curit√©: NORMAL**\n\nAucune alerte active d√©tect√©e. Le syst√®me fonctionne normalement."
        
        critical_count = sum(1 for a in alerts if a.get('level') == 'CRITICAL')
        high_count = sum(1 for a in alerts if a.get('level') == 'HIGH')
        
        if critical_count > 0:
            return f"üî¥ **Niveau de s√©curit√©: CRITIQUE**\n\n{critical_count} alerte(s) critique(s) active(s). Intervention imm√©diate requise."
        elif high_count > 0:
            return f"üü† **Niveau de s√©curit√©: √âLEV√â**\n\n{high_count} alerte(s) haute priorit√©. Surveillance renforc√©e recommand√©e."
        else:
            return f"üü° **Niveau de s√©curit√©: MOYEN**\n\n{len(alerts)} alerte(s) de niveau faible √† moyen."
    
    def _analyze_people_count(self, context: Dict[str, Any]) -> str:
        """Analyse le nombre de personnes."""
        video_analyses = context.get('video_analyses', {})
        if not video_analyses:
            return "üë§ **Comptage des personnes**\n\nAucune analyse vid√©o r√©cente disponible pour le comptage."
        
        # Simulation d'une analyse (remplacez par votre logique r√©elle)
        total_people = len(video_analyses) * 2  # Exemple
        return f"üë• **Personnes d√©tect√©es: {total_people}**\n\nBas√© sur {len(video_analyses)} analyse(s) vid√©o r√©cente(s)."
    
    def _analyze_alerts(self, context: Dict[str, Any]) -> str:
        """Analyse les alertes."""
        alerts = context.get('active_alerts', [])
        if not alerts:
            return "‚úÖ **Aucune alerte active**\n\nTous les syst√®mes fonctionnent normalement."
        
        alert_summary = []
        for alert in alerts[-5:]:
            level_emoji = {
                'LOW': 'üü¢', 'MEDIUM': 'üü°', 
                'HIGH': 'üü†', 'CRITICAL': 'üî¥'
            }.get(alert.get('level', 'LOW'), 'üü¢')
            
            alert_summary.append(f"{level_emoji} **{alert.get('level', 'N/A')}**: {alert.get('message', 'Message non disponible')}")
        
        return f"üö® **{len(alerts)} alerte(s) active(s)**\n\n" + "\n".join(alert_summary)
    
    def _analyze_system_status(self, context: Dict[str, Any]) -> str:
        """Analyse l'√©tat du syst√®me."""
        cameras = context.get('cameras_state', {})
        alerts = context.get('active_alerts', [])
        videos = context.get('video_analyses', {})
        
        active_cams = sum(1 for state in cameras.values() if state.get('enabled', False))
        total_cams = len(cameras)
        
        status_parts = [
            f"üìπ **Cam√©ras**: {active_cams}/{total_cams} actives",
            f"üìä **Analyses**: {len(videos)} r√©centes",
            f"üö® **Alertes**: {len(alerts)} actives"
        ]
        
        overall_status = "üü¢ NORMAL" if len(alerts) == 0 else "üü° SURVEILLANCE"
        
        return f"üñ•Ô∏è **√âtat du syst√®me: {overall_status}**\n\n" + "\n".join(status_parts)
    
    def _general_analysis(self, context: Dict[str, Any]) -> str:
        """Analyse g√©n√©rale."""
        return f"üîç **Analyse g√©n√©rale**\n\nSyst√®me op√©rationnel avec:\n- {len(context.get('cameras_state', {}))} cam√©ra(s) configur√©e(s)\n- {len(context.get('video_analyses', {}))} analyse(s) vid√©o\n- {len(context.get('active_alerts', []))} alerte(s) active(s)\n\nPour une analyse plus pr√©cise, posez une question sp√©cifique."

# Instance globale
def get_vlm_chat() -> VLMChatInterface:
    """R√©cup√®re l'instance de chat VLM."""
    if 'vlm_chat' not in st.session_state:
        st.session_state.vlm_chat = VLMChatInterface()
    
    return st.session_state.vlm_chat

# Fonctions utilitaires

def render_chat_sidebar():
    """Affiche un chat compact dans la sidebar."""
    
    chat = get_vlm_chat()
    
    with st.sidebar:
        st.subheader("Chat Rapide")
        
        # Derniers messages
        messages = chat.session.get_chat_history()
        if messages:
            last_msg = messages[-1]
            role = last_msg.get('role', 'user')
            content = last_msg.get('content', '')
            
            if role == 'assistant':
                st.success(content[:100] + "..." if len(content) > 100 else content)
            else:
                st.info(content[:100] + "..." if len(content) > 100 else content)
        
        # Input rapide
        quick_input = st.text_input("Question rapide", key="sidebar_chat_input")
        
        if st.button("‚û§ Envoyer", use_container_width=True):
            if quick_input:
                chat._process_user_input(quick_input)
                st.rerun()

def render_quick_questions_bar():
    """Barre de questions rapides."""
    
    st.write("**Questions rapides:**")
    
    col1, col2, col3, col4 = st.columns(4)
    
    chat = get_vlm_chat()
    
    with col1:
        if st.button("Que se passe-t-il ?", use_container_width=True):
            chat._process_user_input("Analysez la situation actuelle et r√©sumez les √©v√©nements.")
    
    with col2:
        if st.button("Niveau de risque ?", use_container_width=True):
            chat._process_user_input("Quel est le niveau de risque actuel ?")
    
    with col3:
        if st.button("Combien de personnes ?", use_container_width=True):
            chat._process_user_input("Combien de personnes sont visibles sur les cam√©ras ?")
    
    with col4:
        if st.button("Alertes actives ?", use_container_width=True):
            chat._process_user_input("Quelles sont les alertes actives et leur priorit√© ?")