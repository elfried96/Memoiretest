#!/usr/bin/env python3
"""
Guide d'√©valuation R√âELLE de votre syst√®me de surveillance.
Ce script vous explique comment obtenir de vraies m√©triques.
"""

import os
import json
from datetime import datetime

class RealEvaluationGuide:
    """Guide pour √©valuer r√©ellement votre syst√®me."""
    
    def __init__(self):
        self.evaluation_methods = {
            'video_analysis': self._video_analysis_method,
            'synthetic_data': self._synthetic_data_method, 
            'benchmark_datasets': self._benchmark_datasets_method,
            'live_testing': self._live_testing_method
        }
    
    def _video_analysis_method(self):
        """M√©thode 1: √âvaluer avec vos propres vid√©os"""
        return {
            'description': 'Analyser des vid√©os de surveillance r√©elles',
            'steps': [
                '1. Collecter des vid√©os de test (normales + suspectes)',
                '2. Cr√©er un ground truth manuel (annoter les comportements)',
                '3. Faire tourner votre syst√®me sur ces vid√©os',
                '4. Comparer les r√©sultats avec vos annotations',
                '5. Calculer pr√©cision, recall, F1-score'
            ],
            'code_example': '''
# Utiliser votre script d'√©valuation:
python evaluate_system.py --video_dir /path/to/your/videos --ground_truth annotations.json

# Ou directement en Python:
from evaluate_system import SystemEvaluator
evaluator = SystemEvaluator()
results = evaluator.evaluate_video("video_test.mp4", ground_truth_data)
print(f"Pr√©cision: {results['precision']:.2%}")
print(f"Recall: {results['recall']:.2%}")
            ''',
            'metrics_you_get': [
                'Temps de traitement par frame',
                'FPS (images par seconde)',
                'Pr√©cision de d√©tection des comportements suspects',
                'Taux de faux positifs/n√©gatifs',
                'Latence du syst√®me global'
            ]
        }
    
    def _synthetic_data_method(self):
        """M√©thode 2: Donn√©es synth√©tiques contr√¥l√©es"""
        return {
            'description': 'Cr√©er des sc√©narios de test contr√¥l√©s',
            'steps': [
                '1. D√©finir des sc√©narios types (vol, shopping normal, etc.)',
                '2. Cr√©er/simuler des donn√©es pour ces sc√©narios',
                '3. Tester votre syst√®me sur ces donn√©es',
                '4. Mesurer les performances sur chaque sc√©nario',
                '5. Analyser les points faibles'
            ],
            'example_scenarios': [
                'Personne met objets dans panier (NORMAL)',
                'Personne met objets dans son sac (SUSPECT)',
                'Personne regarde autour nerveusement (SUSPECT)',
                'Personne fait ses courses calmement (NORMAL)'
            ],
            'metrics_you_get': [
                'Pr√©cision par type de comportement',
                'Temps de r√©ponse par sc√©nario',
                'Consistency score (m√™me comportement ‚Üí m√™me r√©sultat)'
            ]
        }
    
    def _benchmark_datasets_method(self):
        """M√©thode 3: Datasets publics de surveillance"""
        return {
            'description': 'Utiliser des datasets de recherche existants',
            'datasets': [
                'UCF-Crime Dataset (comportements anormaux)',
                'CCTV-Fights Dataset (d√©tection d\'incidents)',
                'ShanghaiTech Campus Dataset (d√©tection d\'anomalies)',
                'Avenue Dataset (comportements suspects)'
            ],
            'advantages': [
                'Ground truth d√©j√† disponible',
                'Comparaison avec autres syst√®mes',
                'M√©triques standardis√©es',
                'Cr√©dibilit√© acad√©mique'
            ],
            'metrics_you_get': [
                'AUC (Area Under Curve)',
                'EER (Equal Error Rate)', 
                'Pr√©cision/Recall par classe',
                'Comparaison avec √©tat de l\'art'
            ]
        }
    
    def _live_testing_method(self):
        """M√©thode 4: Test en conditions r√©elles"""
        return {
            'description': 'D√©ployer et tester en environnement r√©el',
            'setup': [
                '1. Installer dans un environnement contr√¥l√©',
                '2. Monitorer pendant une p√©riode d√©finie',
                '3. Collecter feedback des utilisateurs/security',
                '4. Analyser les incidents d√©tect√©s vs rat√©s',
                '5. Mesurer impact op√©rationnel'
            ],
            'metrics_you_get': [
                'Taux de d√©tection en conditions r√©elles',
                'Temps de r√©ponse op√©rationnel',
                'Satisfaction utilisateur',
                'R√©duction des incidents non d√©tect√©s',
                'ROI (retour sur investissement)'
            ]
        }
    
    def generate_evaluation_plan(self, method='video_analysis'):
        """G√©n√®re un plan d'√©valuation d√©taill√©."""
        
        if method not in self.evaluation_methods:
            return "M√©thode non disponible. Choisissez: " + str(list(self.evaluation_methods.keys()))
        
        plan = self.evaluation_methods[method]()
        
        # G√©n√©ration du plan complet
        evaluation_plan = {
            'method': method,
            'timestamp': datetime.now().isoformat(),
            'plan': plan,
            'next_steps': self._get_next_steps(method),
            'tools_needed': self._get_tools_needed(method)
        }
        
        return evaluation_plan
    
    def _get_next_steps(self, method):
        """Prochaines √©tapes selon la m√©thode choisie."""
        steps = {
            'video_analysis': [
                '1. Enregistrer ou trouver 5-10 vid√©os de test',
                '2. Annoter manuellement les comportements (ground truth)',
                '3. Modifier evaluate_system.py pour accept video directory',
                '4. Lancer l\'√©valuation: python evaluate_system.py --videos ./test_videos',
                '5. Analyser les r√©sultats et calculer m√©triques finales'
            ],
            'synthetic_data': [
                '1. D√©finir 10 sc√©narios types avec r√©sultats attendus',
                '2. Cr√©er dataset synth√©tique avec ces sc√©narios',
                '3. Tester votre syst√®me sur chaque sc√©nario',
                '4. Calculer accuracy par sc√©nario',
                '5. Identifier scenarios probl√©matiques'
            ]
        }
        return steps.get(method, ['Voir documentation de la m√©thode'])
    
    def _get_tools_needed(self, method):
        """Outils n√©cessaires selon la m√©thode."""
        tools = {
            'video_analysis': [
                'Cam√©ra/videos de test',
                'Outil d\'annotation (LabelImg, CVAT)',
                'evaluate_system.py (d√©j√† cr√©√©)',
                'Calculs statistiques (pandas, sklearn.metrics)'
            ],
            'synthetic_data': [
                'G√©n√©rateur de donn√©es synth√©tiques',
                'Scripts de sc√©narios pr√©d√©finis',
                'Syst√®me de validation automatis√©e'
            ]
        }
        return tools.get(method, ['Voir documentation'])
    
    def save_plan(self, plan, filename=None):
        """Sauvegarde le plan d'√©valuation."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"evaluation_plan_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(plan, f, indent=2, ensure_ascii=False)
        
        print(f"üìã Plan d'√©valuation sauv√©: {filename}")

def main():
    """Fonction principale - g√©n√®re votre plan d'√©valuation."""
    guide = RealEvaluationGuide()
    
    print("üéØ GUIDE D'√âVALUATION R√âELLE DE VOTRE SYST√àME")
    print("=" * 50)
    print()
    print("M√©thodes disponibles:")
    print("1. video_analysis - Analyser vos propres vid√©os (RECOMMAND√â)")
    print("2. synthetic_data - Donn√©es synth√©tiques contr√¥l√©es") 
    print("3. benchmark_datasets - Datasets publics de recherche")
    print("4. live_testing - Test en conditions r√©elles")
    print()
    
    # G√©n√©rer plan pour analyse vid√©o (m√©thode recommand√©e)
    method = 'video_analysis'
    plan = guide.generate_evaluation_plan(method)
    
    print(f"üìã Plan g√©n√©r√© pour: {method}")
    print("-" * 30)
    print(f"Description: {plan['plan']['description']}")
    print()
    print("√âtapes √† suivre:")
    for step in plan['plan']['steps']:
        print(f"  {step}")
    print()
    print("M√©triques que vous obtiendrez:")
    for metric in plan['plan']['metrics_you_get']:
        print(f"  ‚Ä¢ {metric}")
    print()
    print("Prochaines actions:")
    for action in plan['next_steps']:
        print(f"  ‚ñ∂ {action}")
    
    # Sauvegarder le plan
    guide.save_plan(plan)
    
    print(f"\nüöÄ VOTRE SYST√àME EST PR√äT √Ä √äTRE √âVALU√â!")
    print("Commencez par la m√©thode 'video_analysis' pour des r√©sultats rapides.")

if __name__ == "__main__":
    main()