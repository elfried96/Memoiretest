{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üß† Fine-tuning Vision-Language Model pour Surveillance\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elfried-kinzoun/intelligent-surveillance-system/blob/main/notebooks/1_fine_tuning_vlm.ipynb)\n",
    "\n",
    "**Objectif**: Fine-tuner un mod√®le Vision-Language (LLaVA/BLIP) sp√©cialement pour la surveillance en grande distribution avec capacit√©s de tool-calling.\n",
    "\n",
    "## üéØ Ce que vous allez apprendre :\n",
    "- üìä **Pr√©paration de dataset** sp√©cialis√© surveillance\n",
    "- üîß **Fine-tuning LoRA** pour optimiser les ressources\n",
    "- üõ†Ô∏è **Tool-calling integration** pour orchestration d'outils\n",
    "- üìà **√âvaluation qualitative** des performances\n",
    "- üöÄ **D√©ploiement** du mod√®le fine-tun√©\n",
    "\n",
    "## ‚öôÔ∏è Configuration Recommand√©e :\n",
    "- **GPU**: T4 (Colab gratuit) ou A100 (Colab Pro)\n",
    "- **RAM**: 12+ GB\n",
    "- **Temps**: 2-4 heures selon GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üöÄ Configuration Initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances sp√©cialis√©es pour fine-tuning\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers[torch]==4.36.0\n",
    "!pip install -q peft==0.7.0  # Parameter Efficient Fine-Tuning\n",
    "!pip install -q bitsandbytes==0.41.3  # Quantization\n",
    "!pip install -q accelerate==0.25.0\n",
    "!pip install -q datasets==2.15.0\n",
    "!pip install -q wandb  # Weights & Biases pour tracking\n",
    "!pip install -q deepspeed  # Optimisation m√©moire\n",
    "!pip install -q flash-attn --no-build-isolation  # Attention optimis√©e\n",
    "\n",
    "# Installation du projet\n",
    "!git clone -q https://github.com/elfried-kinzoun/intelligent-surveillance-system.git\n",
    "%cd intelligent-surveillance-system\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# V√©rification GPU et configuration\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers: {transformers.__version__}\")\n",
    "print(f\"üéÆ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üéØ GPU: {gpu_name}\")\n",
    "    print(f\"üíæ M√©moire GPU: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Recommandations selon GPU\n",
    "    if \"T4\" in gpu_name:\n",
    "        print(\"üí° GPU T4 d√©tect√© - Utilisation LoRA + quantization 4-bit recommand√©e\")\n",
    "        RECOMMENDED_CONFIG = {\n",
    "            \"load_in_4bit\": True,\n",
    "            \"use_lora\": True,\n",
    "            \"lora_rank\": 16,\n",
    "            \"batch_size\": 1,\n",
    "            \"gradient_checkpointing\": True\n",
    "        }\n",
    "    elif \"A100\" in gpu_name or \"V100\" in gpu_name:\n",
    "        print(\"üöÄ GPU haute performance - Configuration optimale possible\")\n",
    "        RECOMMENDED_CONFIG = {\n",
    "            \"load_in_4bit\": False,\n",
    "            \"use_lora\": True,\n",
    "            \"lora_rank\": 64,\n",
    "            \"batch_size\": 4,\n",
    "            \"gradient_checkpointing\": False\n",
    "        }\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas de GPU - Fine-tuning sur CPU non recommand√©\")\n",
    "    RECOMMENDED_CONFIG = {\n",
    "        \"load_in_4bit\": False,\n",
    "        \"use_lora\": True,\n",
    "        \"lora_rank\": 8,\n",
    "        \"batch_size\": 1,\n",
    "        \"gradient_checkpointing\": True\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Configuration recommand√©e: {RECOMMENDED_CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_wandb"
   },
   "outputs": [],
   "source": [
    "# Configuration Weights & Biases (optionnel)\n",
    "import wandb\n",
    "\n",
    "# D√©commenter et configurer si vous voulez tracker avec W&B\n",
    "# wandb.login()  # Vous devrez entrer votre cl√© API\n",
    "\n",
    "# Configuration du projet\n",
    "PROJECT_NAME = \"surveillance-vlm-finetuning\"\n",
    "RUN_NAME = f\"llava-surveillance-{torch.cuda.get_device_name(0).replace(' ', '_') if torch.cuda.is_available() else 'cpu'}\"\n",
    "\n",
    "# Initialisation W&B (en mode offline si pas de login)\n",
    "try:\n",
    "    wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        name=RUN_NAME,\n",
    "        config=RECOMMENDED_CONFIG,\n",
    "        mode=\"online\"  # Changez en \"offline\" si pas de compte W&B\n",
    "    )\n",
    "    print(\"‚úÖ W&B configur√©\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è W&B en mode offline - tracking local uniquement\")\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset"
   },
   "source": [
    "## üìä Pr√©paration du Dataset de Surveillance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Cr√©ation d'un dataset de surveillance synth√©tique pour d√©monstration\n",
    "def create_surveillance_dataset():\n",
    "    \"\"\"\n",
    "    Cr√©e un dataset d'entra√Ænement pour la surveillance.\n",
    "    En production, vous utiliseriez vos vraies donn√©es annot√©es.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Templates de conversations pour diff√©rents sc√©narios\n",
    "    conversation_templates = {\n",
    "        \"normal_shopping\": [\n",
    "            {\n",
    "                \"human\": \"Analysez cette sc√®ne de surveillance et d√©terminez le niveau de suspicion.\",\n",
    "                \"assistant\": \"Je vois une personne qui fait ses courses normalement. Elle examine des produits, les place dans son panier de mani√®re visible. Son comportement est coh√©rent avec un shopping classique. Niveau de suspicion: FAIBLE. Aucune action particuli√®re requise. Outils recommand√©s: surveillance passive avec object_detector.\"\n",
    "            },\n",
    "            {\n",
    "                \"human\": \"Que voyez-vous dans cette image de magasin ? Y a-t-il quelque chose d'inhabituel ?\",\n",
    "                \"assistant\": \"Cette sc√®ne montre une activit√© commerciale normale. Les clients se d√©placent calmement, consultent les produits et interagissent naturellement avec l'environnement. Aucun comportement suspect d√©tect√©. Recommandation: continuer la surveillance de routine.\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"suspicious_movement\": [\n",
    "            {\n",
    "                \"human\": \"Analysez le comportement des personnes dans cette zone de surveillance.\",\n",
    "                \"assistant\": \"J'observe des mouvements erratiques qui m√©ritent attention. Une personne semble √©viter les cam√©ras et fait des allers-retours sans acheter. Niveau de suspicion: MOYEN. Actions recommand√©es: surveillance renforc√©e, utilisation du tracker pour suivre les d√©placements, activation du behavior_analyzer pour analyser les patterns.\"\n",
    "            },\n",
    "            {\n",
    "                \"human\": \"Cette personne vous semble-t-elle suspecte ? Utilisez vos outils d'analyse.\",\n",
    "                \"assistant\": \"Le comportement observ√© pr√©sente plusieurs indicateurs d'alerte. Je recommande d'utiliser imm√©diatement le tracker pour suivre cette personne et le behavior_analyzer pour √©valuer ses patterns de mouvement. Surveillance discr√®te mais active requise.\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"potential_theft\": [\n",
    "            {\n",
    "                \"human\": \"Situation d'urgence - analysez imm√©diatement cette sc√®ne.\",\n",
    "                \"assistant\": \"ALERTE: Comportement hautement suspect d√©tect√©. Je vois une personne dissimuler des objets. Niveau de suspicion: CRITIQUE. Actions imm√©diates: activation de tous les outils (object_detector pour identifier les objets, tracker pour suivi continu, behavior_analyzer pour confirmation, false_positive_filter pour validation). Alerte s√©curit√© recommand√©e.\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"tool_calling_example\": [\n",
    "            {\n",
    "                \"human\": \"Analysez cette sc√®ne et utilisez les outils appropri√©s pour une √©valuation compl√®te.\",\n",
    "                \"assistant\": \"Je vais analyser cette sc√®ne syst√©matiquement. Permettez-moi d'utiliser les outils disponibles.\\n\\n[TOOL_CALL: object_detector]\\n{\\\"confidence_threshold\\\": 0.3, \\\"focus_classes\\\": [\\\"person\\\", \\\"handbag\\\", \\\"backpack\\\"]}\\n\\n[TOOL_CALL: behavior_analyzer]\\n{\\\"analysis_window\\\": 30, \\\"focus_behaviors\\\": [\\\"movement_pattern\\\", \\\"interaction_objects\\\"]}\\n\\nBas√© sur les r√©sultats des outils, je d√©tecte 2 personnes avec des sacs. Les patterns de mouvement semblent normaux. Niveau de suspicion: FAIBLE √† MOYEN selon la dur√©e d'observation.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # G√©n√©ration du dataset\n",
    "    dataset_entries = []\n",
    "    \n",
    "    # URLs d'images de d√©monstration (remplacez par vos vraies images)\n",
    "    demo_images = [\n",
    "        \"https://images.unsplash.com/photo-1556742049-0cfed4f6a45d?w=640\",  # Store\n",
    "        \"https://images.unsplash.com/photo-1441986300917-64674bd600d8?w=640\",  # Shopping\n",
    "        \"https://images.unsplash.com/photo-1472851294608-062f824d29cc?w=640\",  # Retail\n",
    "        \"https://images.unsplash.com/photo-1560472354-b33ff0c44a43?w=640\",  # Supermarket\n",
    "        \"https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=640\",  # Store aisle\n",
    "    ]\n",
    "    \n",
    "    # G√©n√©ration des entr√©es du dataset\n",
    "    for scenario_type, conversations in conversation_templates.items():\n",
    "        for i, conv in enumerate(conversations):\n",
    "            for img_idx, img_url in enumerate(demo_images):\n",
    "                entry = {\n",
    "                    \"id\": f\"{scenario_type}_{i}_{img_idx}\",\n",
    "                    \"image_url\": img_url,\n",
    "                    \"conversations\": [\n",
    "                        {\"from\": \"human\", \"value\": conv[\"human\"]},\n",
    "                        {\"from\": \"gpt\", \"value\": conv[\"assistant\"]}\n",
    "                    ],\n",
    "                    \"scenario\": scenario_type,\n",
    "                    \"metadata\": {\n",
    "                        \"domain\": \"surveillance\",\n",
    "                        \"task\": \"security_analysis\",\n",
    "                        \"has_tool_calling\": \"tool_calling\" in scenario_type\n",
    "                    }\n",
    "                }\n",
    "                dataset_entries.append(entry)\n",
    "    \n",
    "    return dataset_entries\n",
    "\n",
    "# Cr√©ation du dataset\n",
    "print(\"üìä Cr√©ation du dataset de surveillance...\")\n",
    "dataset_entries = create_surveillance_dataset()\n",
    "print(f\"‚úÖ Dataset cr√©√©: {len(dataset_entries)} exemples\")\n",
    "\n",
    "# Affichage d'un exemple\n",
    "print(\"\\nüìù Exemple de conversation:\")\n",
    "example = dataset_entries[0]\n",
    "print(f\"Sc√©nario: {example['scenario']}\")\n",
    "print(f\"Human: {example['conversations'][0]['value']}\")\n",
    "print(f\"Assistant: {example['conversations'][1]['value'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_huggingface_dataset"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "def download_and_prepare_images(dataset_entries, max_images=20):\n",
    "    \"\"\"\n",
    "    T√©l√©charge les images et pr√©pare le dataset final.\n",
    "    En production, vous travailleriez avec des images locales.\n",
    "    \"\"\"\n",
    "    \n",
    "    prepared_data = []\n",
    "    downloaded_images = {}\n",
    "    \n",
    "    print(f\"üì• T√©l√©chargement de {min(max_images, len(dataset_entries))} images...\")\n",
    "    \n",
    "    for i, entry in enumerate(dataset_entries[:max_images]):\n",
    "        try:\n",
    "            # Download image si pas d√©j√† fait\n",
    "            img_url = entry[\"image_url\"]\n",
    "            if img_url not in downloaded_images:\n",
    "                response = requests.get(img_url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                    # Resize pour optimiser\n",
    "                    image = image.resize((512, 512), Image.Resampling.LANCZOS)\n",
    "                    downloaded_images[img_url] = image\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Erreur t√©l√©chargement image {img_url}\")\n",
    "                    continue\n",
    "            \n",
    "            # Pr√©paration de l'entr√©e\n",
    "            prepared_entry = {\n",
    "                \"id\": entry[\"id\"],\n",
    "                \"image\": downloaded_images[img_url],\n",
    "                \"conversations\": entry[\"conversations\"],\n",
    "                \"scenario\": entry[\"scenario\"]\n",
    "            }\n",
    "            prepared_data.append(prepared_entry)\n",
    "            \n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  üì∑ {i + 1}/{min(max_images, len(dataset_entries))} images t√©l√©charg√©es\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur pour {entry['id']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return prepared_data\n",
    "\n",
    "# Pr√©paration des donn√©es\n",
    "prepared_data = download_and_prepare_images(dataset_entries, max_images=20)\n",
    "print(f\"‚úÖ {len(prepared_data)} exemples pr√©par√©s\")\n",
    "\n",
    "# Division train/validation\n",
    "random.shuffle(prepared_data)\n",
    "split_idx = int(0.8 * len(prepared_data))\n",
    "train_data = prepared_data[:split_idx]\n",
    "val_data = prepared_data[split_idx:]\n",
    "\n",
    "print(f\"üìä Division dataset:\")\n",
    "print(f\"  üèãÔ∏è Train: {len(train_data)} exemples\")\n",
    "print(f\"  üìã Validation: {len(val_data)} exemples\")\n",
    "\n",
    "# Cr√©ation des datasets Hugging Face\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "print(\"\\nüìà Statistiques par sc√©nario:\")\n",
    "scenario_counts = {}\n",
    "for entry in prepared_data:\n",
    "    scenario = entry[\"scenario\"]\n",
    "    scenario_counts[scenario] = scenario_counts.get(scenario, 0) + 1\n",
    "\n",
    "for scenario, count in scenario_counts.items():\n",
    "    print(f\"  {scenario}: {count} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## üß† Configuration du Mod√®le VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_base_model"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "# Configuration du mod√®le de base\n",
    "MODEL_NAME = \"llava-hf/llava-v1.6-mistral-7b-hf\"  # Mod√®le de base\n",
    "\n",
    "print(f\"üß† Chargement du mod√®le: {MODEL_NAME}\")\n",
    "print(\"‚è≥ Cela peut prendre quelques minutes...\")\n",
    "\n",
    "# Configuration de quantization pour optimiser la m√©moire\n",
    "if RECOMMENDED_CONFIG[\"load_in_4bit\"]:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    print(\"üîß Quantization 4-bit activ√©e\")\n",
    "else:\n",
    "    bnb_config = None\n",
    "    print(\"üîß Pas de quantization\")\n",
    "\n",
    "# Chargement du processeur\n",
    "processor = LlavaNextProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Chargement du mod√®le\n",
    "try:\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\",\n",
    "    )\n",
    "    print(\"‚úÖ Mod√®le charg√© avec succ√®s\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur flash attention, fallback vers eager: {e}\")\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "    print(\"‚úÖ Mod√®le charg√© (mode standard)\")\n",
    "\n",
    "# Information sur le mod√®le\n",
    "print(f\"\\nüìä Informations mod√®le:\")\n",
    "print(f\"  Param√®tres: ~7B\")\n",
    "print(f\"  Device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'auto'}\")\n",
    "print(f\"  Dtype: {model.dtype}\")\n",
    "print(f\"  Quantization: {'4-bit' if bnb_config else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_lora"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# Configuration LoRA pour fine-tuning efficace\n",
    "if RECOMMENDED_CONFIG[\"use_lora\"]:\n",
    "    print(\"üîß Configuration LoRA...\")\n",
    "    \n",
    "    # Pr√©paration du mod√®le pour quantization si n√©cessaire\n",
    "    if RECOMMENDED_CONFIG[\"load_in_4bit\"]:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        print(\"  ‚úÖ Mod√®le pr√©par√© pour quantization\")\n",
    "    \n",
    "    # Configuration LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,  # Task type pour generation\n",
    "        r=RECOMMENDED_CONFIG[\"lora_rank\"],  # Rank de la d√©composition\n",
    "        lora_alpha=32,  # Scaling parameter\n",
    "        lora_dropout=0.1,  # Dropout pour r√©gularisation\n",
    "        bias=\"none\",  # Pas de bias dans LoRA\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"v_proj\", \n",
    "            \"k_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    \n",
    "    # Application de LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(\"  ‚úÖ LoRA configur√©\")\n",
    "    print(f\"  üìä Rank: {RECOMMENDED_CONFIG['lora_rank']}\")\n",
    "    print(f\"  üìä Alpha: 32\")\n",
    "    print(f\"  üìä Dropout: 0.1\")\n",
    "    \n",
    "    # Affichage des param√®tres entra√Ænables\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\nüìà Param√®tres:\")\n",
    "    print(f\"  Entra√Ænables: {trainable_params:,}\")\n",
    "    print(f\"  Total: {total_params:,}\")\n",
    "    print(f\"  Pourcentage entra√Ænable: {100 * trainable_params / total_params:.2f}%\")\n",
    "else:\n",
    "    print(\"üîß Fine-tuning complet (pas de LoRA)\")\n",
    "\n",
    "# Configuration du processeur pour le padding\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    print(\"‚úÖ Pad token configur√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_processing"
   },
   "source": [
    "## üîÑ Traitement des Donn√©es pour l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_collator"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class LlavaDataCollator:\n",
    "    \"\"\"\n",
    "    Data collator sp√©cialis√© pour LLaVA avec conversations multi-tours.\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "    \n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extraction des images et conversations\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "        conversations = [item[\"conversations\"] for item in batch]\n",
    "        \n",
    "        # Pr√©paration des prompts pour LLaVA\n",
    "        texts = []\n",
    "        for conv in conversations:\n",
    "            # Format de conversation pour LLaVA\n",
    "            conversation_text = \"\"\n",
    "            for turn in conv:\n",
    "                if turn[\"from\"] == \"human\":\n",
    "                    conversation_text += f\"<image>\\nUSER: {turn['value']}\\nASSISTANT: \"\n",
    "                elif turn[\"from\"] == \"gpt\":\n",
    "                    conversation_text += f\"{turn['value']}\"\n",
    "            texts.append(conversation_text)\n",
    "        \n",
    "        # Traitement par le processeur LLaVA\n",
    "        inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Pr√©paration des labels pour l'entra√Ænement\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        # Masquer les tokens du prompt (ne calculer la loss que sur la r√©ponse)\n",
    "        for i, text in enumerate(texts):\n",
    "            # Trouver le d√©but de la r√©ponse de l'assistant\n",
    "            assistant_start = text.find(\"ASSISTANT: \") + len(\"ASSISTANT: \")\n",
    "            if assistant_start > len(\"ASSISTANT: \"):\n",
    "                # Tokeniser juste le prompt pour trouver o√π commencer la loss\n",
    "                prompt_text = text[:assistant_start]\n",
    "                prompt_tokens = self.processor.tokenizer(\n",
    "                    prompt_text, \n",
    "                    add_special_tokens=False\n",
    "                )[\"input_ids\"]\n",
    "                \n",
    "                # Masquer les tokens du prompt\n",
    "                labels[i, :len(prompt_tokens)] = -100\n",
    "        \n",
    "        inputs[\"labels\"] = labels\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "# Cr√©ation du data collator\n",
    "data_collator = LlavaDataCollator(processor=processor)\n",
    "print(\"‚úÖ Data collator cr√©√©\")\n",
    "\n",
    "# Test du data collator\n",
    "print(\"\\nüß™ Test du data collator...\")\n",
    "sample_batch = [train_dataset[0], train_dataset[1] if len(train_dataset) > 1 else train_dataset[0]]\n",
    "try:\n",
    "    test_batch = data_collator(sample_batch)\n",
    "    print(f\"  ‚úÖ Batch shape: {test_batch['input_ids'].shape}\")\n",
    "    print(f\"  ‚úÖ Images shape: {test_batch['pixel_values'].shape}\")\n",
    "    print(f\"  ‚úÖ Labels shape: {test_batch['labels'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erreur test batch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_setup"
   },
   "source": [
    "## üèãÔ∏è Configuration de l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_args"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import math\n",
    "\n",
    "# Configuration des param√®tres d'entra√Ænement\n",
    "OUTPUT_DIR = \"./surveillance-llava-finetuned\"\n",
    "LOGGING_DIR = \"./logs\"\n",
    "\n",
    "# Calcul automatique des steps selon le dataset\n",
    "num_train_samples = len(train_dataset)\n",
    "batch_size = RECOMMENDED_CONFIG[\"batch_size\"]\n",
    "gradient_accumulation_steps = max(1, 4 // batch_size)  # Simuler batch size 4\n",
    "effective_batch_size = batch_size * gradient_accumulation_steps\n",
    "\n",
    "# Param√®tres d'entra√Ænement adapt√©s\n",
    "num_epochs = 3  # Nombre d'√©poques\n",
    "steps_per_epoch = math.ceil(num_train_samples / effective_batch_size)\n",
    "total_steps = steps_per_epoch * num_epochs\n",
    "warmup_steps = int(0.1 * total_steps)  # 10% de warmup\n",
    "eval_steps = max(1, steps_per_epoch // 2)  # √âvaluation 2x par √©poque\n",
    "save_steps = eval_steps\n",
    "\n",
    "print(f\"üìä Configuration d'entra√Ænement:\")\n",
    "print(f\"  Samples d'entra√Ænement: {num_train_samples}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"  Batch size effectif: {effective_batch_size}\")\n",
    "print(f\"  √âpoques: {num_epochs}\")\n",
    "print(f\"  Steps par √©poque: {steps_per_epoch}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Eval/Save steps: {eval_steps}\")\n",
    "\n",
    "# Arguments d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Param√®tres de base\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    \n",
    "    # Optimisation\n",
    "    learning_rate=2e-4,  # Learning rate pour LoRA\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Scheduler\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=warmup_steps,\n",
    "    \n",
    "    # √âvaluation et sauvegarde\n",
    "    eval_steps=eval_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    logging_steps=max(1, eval_steps // 4),\n",
    "    report_to=[\"wandb\"] if \"wandb\" in globals() else [],\n",
    "    \n",
    "    # Optimisations m√©moire\n",
    "    gradient_checkpointing=RECOMMENDED_CONFIG[\"gradient_checkpointing\"],\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=2 if torch.cuda.is_available() else 0,\n",
    "    \n",
    "    # Pr√©cision mixte\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    bf16=False,  # Disponible sur A100/H100\n",
    "    \n",
    "    # Autres\n",
    "    remove_unused_columns=False,  # Important pour les donn√©es multimodales\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=None,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Arguments d'entra√Ænement configur√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_trainer"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "\n",
    "# Fonction de calcul des m√©triques d'√©valuation\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcule les m√©triques d'√©valuation pour le fine-tuning.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Pour les mod√®les g√©n√©ratifs, on se concentre sur la perplexit√©\n",
    "    # Calcul√©e automatiquement via la loss\n",
    "    \n",
    "    return {\n",
    "        \"perplexity\": np.exp(np.mean(predictions)),\n",
    "    }\n",
    "\n",
    "# Classe Trainer personnalis√©e pour surveillance VLM\n",
    "class SurveillanceVLMTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer personnalis√© avec des fonctionnalit√©s sp√©cifiques √† la surveillance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Calcul de la loss avec gestion sp√©ciale pour les mod√®les multimodaux.\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Log des m√©triques additionnelles\n",
    "        if self.state.global_step % self.args.logging_steps == 0:\n",
    "            self.log({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"learning_rate\": self.get_lr()[0],\n",
    "                \"epoch\": self.state.epoch,\n",
    "            })\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"\n",
    "        Loop d'√©valuation personnalis√©.\n",
    "        \"\"\"\n",
    "        output = super().evaluation_loop(\n",
    "            dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix\n",
    "        )\n",
    "        \n",
    "        # Calcul de m√©triques suppl√©mentaires\n",
    "        if output.metrics:\n",
    "            perplexity = np.exp(output.metrics.get(f\"{metric_key_prefix}_loss\", 0))\n",
    "            output.metrics[f\"{metric_key_prefix}_perplexity\"] = perplexity\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Cr√©ation du trainer\n",
    "trainer = SurveillanceVLMTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,  # D√©sactiv√© pour √©conomiser de la m√©moire\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configur√©\")\n",
    "print(f\"üìä Mod√®le sur device: {next(model.parameters()).device}\")\n",
    "\n",
    "# V√©rification de la m√©moire GPU si disponible\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"üíæ M√©moire GPU: {allocated_memory:.1f}/{gpu_memory:.1f} GB utilis√©e\")\n",
    "    \n",
    "    if allocated_memory / gpu_memory > 0.9:\n",
    "        print(\"‚ö†Ô∏è M√©moire GPU proche de la limite - Consid√©rez r√©duire batch_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üöÄ Lancement de l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üöÄ D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üïê Heure de d√©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Dur√©e estim√©e: ~{total_steps * 30 / 60:.1f} minutes (estimation)\")\n",
    "print(f\"üìä Configuration GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print()\n",
    "\n",
    "# Sauvegarde des m√©triques initiales\n",
    "start_time = time.time()\n",
    "initial_memory = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "try:\n",
    "    # Lancement de l'entra√Ænement\n",
    "    training_result = trainer.train()\n",
    "    \n",
    "    # Calcul des m√©triques finales\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    final_memory = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéâ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS !\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üïê Dur√©e totale: {training_duration/60:.1f} minutes\")\n",
    "    print(f\"üìà Loss finale: {training_result.training_loss:.4f}\")\n",
    "    print(f\"‚ö° Steps/seconde: {training_result.global_step/training_duration:.2f}\")\n",
    "    print(f\"üíæ M√©moire GPU finale: {final_memory:.1f} GB\")\n",
    "    \n",
    "    # Sauvegarde du mod√®le final\n",
    "    print(\"\\nüíæ Sauvegarde du mod√®le...\")\n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"‚úÖ Mod√®le sauvegard√© dans: {OUTPUT_DIR}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERREUR PENDANT L'ENTRA√éNEMENT: {e}\")\n",
    "    print(\"\\nüí° Solutions possibles:\")\n",
    "    print(\"  - R√©duire le batch_size\")\n",
    "    print(\"  - Activer gradient_checkpointing\")\n",
    "    print(\"  - Utiliser une quantization plus agressive\")\n",
    "    print(\"  - R√©duire la longueur de s√©quence\")\n",
    "    \n",
    "    # Tentative de lib√©ration m√©moire\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## üìä √âvaluation du Mod√®le Fine-tun√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# √âvaluation sur le set de validation\n",
    "print(\"üìä √âVALUATION DU MOD√àLE FINE-TUN√â\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(\"üìà R√©sultats d'√©valuation:\")\n",
    "    for key, value in eval_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur √©valuation: {e}\")\n",
    "\n",
    "# Test g√©n√©ration avec le mod√®le fine-tun√©\n",
    "print(\"\\nüß™ Test de g√©n√©ration:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Fonction de test du mod√®le\n",
    "def test_surveillance_model(image, prompt, max_new_tokens=150):\n",
    "    \"\"\"\n",
    "    Test du mod√®le fine-tun√© sur une image de surveillance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pr√©paration du prompt au format LLaVA\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": f\"<image>\\n{prompt}\"}\n",
    "        ]\n",
    "        \n",
    "        # Formatage pour le processeur\n",
    "        formatted_prompt = processor.apply_chat_template(\n",
    "            conversation, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Pr√©paration des inputs\n",
    "        inputs = processor(\n",
    "            text=formatted_prompt,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # G√©n√©ration\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # D√©codage\n",
    "        generated_text = processor.batch_decode(\n",
    "            outputs, \n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        # Extraction de la r√©ponse\n",
    "        if \"ASSISTANT:\" in generated_text:\n",
    "            response = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "        else:\n",
    "            response = generated_text.strip()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Erreur de g√©n√©ration: {e}\"\n",
    "\n",
    "# Tests sur diff√©rents sc√©narios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Analyse de s√©curit√© g√©n√©rale\",\n",
    "        \"prompt\": \"Analysez cette sc√®ne de surveillance et d√©terminez le niveau de suspicion. Utilisez vos outils d'analyse si n√©cessaire.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"D√©tection comportementale\",\n",
    "        \"prompt\": \"Y a-t-il des comportements suspects dans cette image ? Quelles actions recommandez-vous ?\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Orchestration d'outils\",\n",
    "        \"prompt\": \"Analysez cette sc√®ne en utilisant les outils object_detector et behavior_analyzer. Fournissez un rapport d√©taill√©.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test sur les images du dataset de validation\n",
    "if len(val_dataset) > 0:\n",
    "    for i, scenario in enumerate(test_scenarios):\n",
    "        if i < len(val_dataset):\n",
    "            test_image = val_dataset[i][\"image\"]\n",
    "            \n",
    "            print(f\"\\nüîç Test {i+1}: {scenario['name']}\")\n",
    "            print(f\"‚ùì Question: {scenario['prompt']}\")\n",
    "            \n",
    "            # G√©n√©ration de la r√©ponse\n",
    "            response = test_surveillance_model(test_image, scenario[\"prompt\"])\n",
    "            \n",
    "            print(f\"ü§ñ R√©ponse: {response[:300]}...\" if len(response) > 300 else f\"ü§ñ R√©ponse: {response}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ Tests de g√©n√©ration termin√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_models"
   },
   "outputs": [],
   "source": [
    "# Comparaison avec le mod√®le de base (si la m√©moire le permet)\n",
    "print(\"üÜö COMPARAISON MOD√àLE BASE vs FINE-TUN√â\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Chargement du mod√®le de base pour comparaison\n",
    "    print(\"‚è≥ Chargement du mod√®le de base pour comparaison...\")\n",
    "    base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        quantization_config=bnb_config  # M√™me quantization\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Mod√®le de base charg√©\")\n",
    "    \n",
    "    # Test comparatif sur une image\n",
    "    if len(val_dataset) > 0:\n",
    "        test_image = val_dataset[0][\"image\"]\n",
    "        test_prompt = \"Analysez cette sc√®ne de surveillance et d√©terminez s'il y a des comportements suspects.\"\n",
    "        \n",
    "        print(f\"\\nüñºÔ∏è Test comparatif sur image de validation\")\n",
    "        print(f\"‚ùì Prompt: {test_prompt}\")\n",
    "        \n",
    "        # R√©ponse du mod√®le de base\n",
    "        print(\"\\nüìä Mod√®le de BASE:\")\n",
    "        base_inputs = processor(\n",
    "            text=f\"<image>\\nUSER: {test_prompt}\\nASSISTANT:\",\n",
    "            images=test_image,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            base_outputs = base_model.generate(\n",
    "                **base_inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        base_response = processor.batch_decode(\n",
    "            base_outputs, skip_special_tokens=True\n",
    "        )[0].split(\"ASSISTANT:\")[-1].strip()\n",
    "        \n",
    "        print(f\"ü§ñ {base_response}\")\n",
    "        \n",
    "        # R√©ponse du mod√®le fine-tun√©\n",
    "        print(\"\\nüéØ Mod√®le FINE-TUN√â:\")\n",
    "        finetuned_response = test_surveillance_model(test_image, test_prompt, max_new_tokens=100)\n",
    "        print(f\"ü§ñ {finetuned_response}\")\n",
    "        \n",
    "        print(\"\\nüìù Analyse comparative:\")\n",
    "        print(\"  ‚Ä¢ Le mod√®le fine-tun√© devrait montrer:\")\n",
    "        print(\"    - Vocabulaire sp√©cialis√© surveillance\")\n",
    "        print(\"    - Mention d'outils sp√©cifiques\")\n",
    "        print(\"    - Niveaux de suspicion structur√©s\")\n",
    "        print(\"    - Recommandations d'actions concr√®tes\")\n",
    "    \n",
    "    # Nettoyage m√©moire\n",
    "    del base_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Comparaison impossible (m√©moire insuffisante): {e}\")\n",
    "    print(\"üí° Le mod√®le fine-tun√© est pr√™t √† √™tre utilis√© individuellement\")\n",
    "\n",
    "print(\"\\n‚úÖ √âvaluation termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_and_deploy"
   },
   "source": [
    "## üíæ Sauvegarde et D√©ploiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"üíæ SAUVEGARDE ET D√âPLOIEMENT DU MOD√àLE\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. Sauvegarde locale d√©taill√©e\n",
    "print(\"üóÇÔ∏è Sauvegarde locale...\")\n",
    "\n",
    "# Sauvegarde du mod√®le LoRA\n",
    "if RECOMMENDED_CONFIG[\"use_lora\"]:\n",
    "    lora_output_dir = f\"{OUTPUT_DIR}/lora_adapters\"\n",
    "    model.save_pretrained(lora_output_dir)\n",
    "    print(f\"  ‚úÖ Adaptateurs LoRA: {lora_output_dir}\")\n",
    "\n",
    "# Sauvegarde du processeur\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"  ‚úÖ Processeur: {OUTPUT_DIR}\")\n",
    "\n",
    "# M√©tadonn√©es du fine-tuning\n",
    "metadata = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"fine_tuning_date\": datetime.now().isoformat(),\n",
    "    \"training_config\": {\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"lora_rank\": RECOMMENDED_CONFIG[\"lora_rank\"] if RECOMMENDED_CONFIG[\"use_lora\"] else None,\n",
    "        \"quantization\": \"4bit\" if RECOMMENDED_CONFIG[\"load_in_4bit\"] else None,\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_samples\": len(train_dataset),\n",
    "        \"val_samples\": len(val_dataset),\n",
    "        \"scenarios\": list(scenario_counts.keys()),\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"training_loss\": training_result.training_loss if 'training_result' in locals() else None,\n",
    "        \"total_steps\": training_result.global_step if 'training_result' in locals() else None,\n",
    "    },\n",
    "    \"gpu_info\": {\n",
    "        \"device\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    "        \"memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else None,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/training_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"  ‚úÖ M√©tadonn√©es: {OUTPUT_DIR}/training_metadata.json\")\n",
    "\n",
    "# 2. Script de chargement\n",
    "loading_script = f'''\n",
    "# Script de chargement du mod√®le fine-tun√©\n",
    "from transformers import LlavaNextForConditionalGeneration, LlavaNextProcessor\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Chargement du mod√®le de base\n",
    "base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"{MODEL_NAME}\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Chargement des adaptateurs LoRA\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    \"{lora_output_dir if RECOMMENDED_CONFIG['use_lora'] else OUTPUT_DIR}\"\n",
    ")\n",
    "\n",
    "# Chargement du processeur\n",
    "processor = LlavaNextProcessor.from_pretrained(\"{OUTPUT_DIR}\")\n",
    "\n",
    "print(\"‚úÖ Mod√®le surveillance fine-tun√© charg√© !\")\n",
    "'''\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/load_model.py\", \"w\") as f:\n",
    "    f.write(loading_script)\n",
    "print(f\"  ‚úÖ Script de chargement: {OUTPUT_DIR}/load_model.py\")\n",
    "\n",
    "# 3. Documentation du mod√®le\n",
    "model_card = f'''\n",
    "# üïµÔ∏è Mod√®le VLM Fine-tun√© pour Surveillance\n",
    "\n",
    "## Description\n",
    "Mod√®le Vision-Language bas√© sur LLaVA-NeXT fine-tun√© sp√©cialement pour l'analyse de surveillance en grande distribution.\n",
    "\n",
    "## Capacit√©s\n",
    "- ‚úÖ Analyse de sc√®nes de surveillance\n",
    "- ‚úÖ D√©tection de comportements suspects\n",
    "- ‚úÖ Orchestration d'outils sp√©cialis√©s\n",
    "- ‚úÖ Recommandations d'actions s√©curis√©es\n",
    "\n",
    "## Configuration d'Entra√Ænement\n",
    "- **Mod√®le de base**: {MODEL_NAME}\n",
    "- **M√©thode**: {\"LoRA\" if RECOMMENDED_CONFIG[\"use_lora\"] else \"Fine-tuning complet\"}\n",
    "- **Rang LoRA**: {RECOMMENDED_CONFIG[\"lora_rank\"] if RECOMMENDED_CONFIG[\"use_lora\"] else \"N/A\"}\n",
    "- **Quantization**: {\"4-bit\" if RECOMMENDED_CONFIG[\"load_in_4bit\"] else \"Aucune\"}\n",
    "- **√âpoques**: {num_epochs}\n",
    "- **Taille dataset**: {len(train_dataset)} (train) + {len(val_dataset)} (val)\n",
    "\n",
    "## Utilisation\n",
    "```python\n",
    "from transformers import LlavaNextForConditionalGeneration, LlavaNextProcessor\n",
    "from peft import PeftModel\n",
    "\n",
    "# Chargement du mod√®le\n",
    "base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"{MODEL_NAME}\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"./lora_adapters\")\n",
    "processor = LlavaNextProcessor.from_pretrained(\"./\")\n",
    "\n",
    "# Analyse d'une image\n",
    "prompt = \"Analysez cette sc√®ne de surveillance et d√©terminez le niveau de suspicion.\"\n",
    "inputs = processor(text=f\"<image>\\\\nUSER: {{prompt}}\\\\nASSISTANT:\", images=image, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "```\n",
    "\n",
    "## Performance\n",
    "{f\"- **Loss finale**: {training_result.training_loss:.4f}\" if 'training_result' in locals() else \"- **Loss finale**: En cours d'√©valuation\"}\n",
    "- **GPU utilis√©**: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}\n",
    "{f\"- **Dur√©e d'entra√Ænement**: {training_duration/60:.1f} minutes\" if 'training_duration' in locals() else \"\"}\n",
    "\n",
    "## Limitations\n",
    "- Entra√Æn√© sur un dataset de d√©monstration (utiliser vos donn√©es r√©elles en production)\n",
    "- Optimis√© pour la surveillance en magasins (peut n√©cessiter adaptation pour autres contextes)\n",
    "- N√©cessite validation humaine pour d√©ploiement critique\n",
    "\n",
    "## Citation\n",
    "```\n",
    "@misc{{surveillance_vlm_2024,\n",
    "  title={{Vision-Language Model for Intelligent Surveillance}},\n",
    "  author={{Syst√®me de Surveillance Intelligente}},\n",
    "  year={{2024}},\n",
    "  note={{Fine-tuned from {MODEL_NAME}}}\n",
    "}}\n",
    "```\n",
    "'''\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "print(f\"  ‚úÖ Documentation: {OUTPUT_DIR}/README.md\")\n",
    "\n",
    "# 4. Archivage pour t√©l√©chargement\n",
    "print(\"\\nüì¶ Cr√©ation de l'archive...\")\n",
    "import shutil\n",
    "\n",
    "archive_name = f\"surveillance_vlm_finetuned_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "try:\n",
    "    shutil.make_archive(archive_name, 'zip', OUTPUT_DIR)\n",
    "    print(f\"  ‚úÖ Archive cr√©√©e: {archive_name}.zip\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Erreur cr√©ation archive: {e}\")\n",
    "\n",
    "# 5. Option de t√©l√©chargement sur Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        \n",
    "        print(\"\\nüì• T√©l√©chargement des fichiers sur Colab:\")\n",
    "        \n",
    "        # T√©l√©charger l'archive principale\n",
    "        if os.path.exists(f\"{archive_name}.zip\"):\n",
    "            files.download(f\"{archive_name}.zip\")\n",
    "            print(f\"  ‚úÖ Archive t√©l√©charg√©e\")\n",
    "        \n",
    "        # T√©l√©charger les m√©tadonn√©es\n",
    "        if os.path.exists(f\"{OUTPUT_DIR}/training_metadata.json\"):\n",
    "            files.download(f\"{OUTPUT_DIR}/training_metadata.json\")\n",
    "            print(f\"  ‚úÖ M√©tadonn√©es t√©l√©charg√©es\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è T√©l√©chargement Colab non disponible: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Sauvegarde termin√©e !\")\n",
    "print(f\"üìÇ Dossier principal: {OUTPUT_DIR}\")\n",
    "print(f\"üìã Fichiers sauvegard√©s:\")\n",
    "print(f\"  ‚Ä¢ Mod√®le/Adaptateurs LoRA\")\n",
    "print(f\"  ‚Ä¢ Processeur\")\n",
    "print(f\"  ‚Ä¢ M√©tadonn√©es d'entra√Ænement\")\n",
    "print(f\"  ‚Ä¢ Script de chargement\")\n",
    "print(f\"  ‚Ä¢ Documentation README\")\n",
    "print(f\"  ‚Ä¢ Archive ZIP (si disponible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "integration"
   },
   "source": [
    "## üîó Int√©gration dans le Syst√®me de Surveillance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "integration_example"
   },
   "outputs": [],
   "source": [
    "# Exemple d'int√©gration du mod√®le fine-tun√© dans le syst√®me principal\n",
    "\n",
    "print(\"üîó INT√âGRATION DANS LE SYST√àME DE SURVEILLANCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Code d'exemple pour remplacer le VLM dans le syst√®me principal\n",
    "integration_code = f'''\n",
    "# Modification √† apporter dans src/core/vlm/model.py\n",
    "# pour utiliser le mod√®le fine-tun√©\n",
    "\n",
    "class VisionLanguageModel:\n",
    "    def __init__(self, model_name: str = \"custom\", **kwargs):\n",
    "        if model_name == \"custom\":\n",
    "            # Chargement du mod√®le fine-tun√©\n",
    "            self.base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "                \"{MODEL_NAME}\",\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            # Chargement des adaptateurs LoRA\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                self.base_model, \n",
    "                \"{OUTPUT_DIR}/lora_adapters\"\n",
    "            )\n",
    "            \n",
    "            self.processor = LlavaNextProcessor.from_pretrained(\"{OUTPUT_DIR}\")\n",
    "        else:\n",
    "            # Mod√®le standard\n",
    "            self.model = LlavaNextForConditionalGeneration.from_pretrained(model_name)\n",
    "            self.processor = LlavaNextProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    async def analyze_frame(self, request: AnalysisRequest) -> AnalysisResponse:\n",
    "        # La logique d'analyse reste la m√™me\n",
    "        # Le mod√®le fine-tun√© donnera des r√©ponses plus pr√©cises\n",
    "        ...\n",
    "\n",
    "# Usage dans le syst√®me principal:\n",
    "vlm = VisionLanguageModel(model_name=\"custom\")\n",
    "'''\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/integration_guide.py\", \"w\") as f:\n",
    "    f.write(integration_code)\n",
    "\n",
    "print(\"‚úÖ Guide d'int√©gration cr√©√©\")\n",
    "\n",
    "# Instructions d'utilisation\n",
    "print(\"\\nüìã √âTAPES D'INT√âGRATION:\")\n",
    "print(\"1. üì• T√©l√©charger les fichiers du mod√®le fine-tun√©\")\n",
    "print(\"2. üìÇ Placer les fichiers dans le dossier du projet\")\n",
    "print(\"3. üîß Modifier src/core/vlm/model.py selon integration_guide.py\")\n",
    "print(\"4. üß™ Tester avec le syst√®me complet\")\n",
    "print(\"5. üìä √âvaluer les performances sur vos donn√©es r√©elles\")\n",
    "\n",
    "print(\"\\nüí° RECOMMANDATIONS:\")\n",
    "print(\"‚Ä¢ Utilisez des donn√©es r√©elles de votre environnement pour un meilleur fine-tuning\")\n",
    "print(\"‚Ä¢ Ajustez les prompts selon vos besoins sp√©cifiques\")\n",
    "print(\"‚Ä¢ √âvaluez r√©guli√®rement les performances en production\")\n",
    "print(\"‚Ä¢ Consid√©rez un fine-tuning it√©ratif avec feedback utilisateur\")\n",
    "\n",
    "print(\"\\nüéØ M√âTRIQUES √Ä SURVEILLER EN PRODUCTION:\")\n",
    "print(\"‚Ä¢ Pr√©cision des d√©tections de comportements suspects\")\n",
    "print(\"‚Ä¢ Taux de faux positifs vs faux n√©gatifs\")\n",
    "print(\"‚Ä¢ Temps de r√©ponse du mod√®le\")\n",
    "print(\"‚Ä¢ Coh√©rence des recommandations d'outils\")\n",
    "print(\"‚Ä¢ Satisfaction des √©quipes de s√©curit√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## üéØ Conclusion et Prochaines √âtapes\n",
    "\n",
    "### ‚úÖ Ce que vous avez accompli :\n",
    "\n",
    "1. **üß† Fine-tuning VLM** : Adaptation de LLaVA-NeXT pour la surveillance\n",
    "2. **‚ö° Optimisation LoRA** : Fine-tuning efficace avec ressources limit√©es\n",
    "3. **üõ†Ô∏è Tool-calling** : Capacit√©s d'orchestration d'outils int√©gr√©es\n",
    "4. **üìä √âvaluation** : Tests qualitatifs et comparaisons\n",
    "5. **üíæ D√©ploiement** : Sauvegarde et guide d'int√©gration\n",
    "\n",
    "### üìà Performances Attendues :\n",
    "\n",
    "Le mod√®le fine-tun√© devrait montrer :\n",
    "- **üéØ Vocabulaire sp√©cialis√©** surveillance et s√©curit√©\n",
    "- **üîß Mentions d'outils** appropri√©s selon le contexte  \n",
    "- **üìä Niveaux de suspicion** structur√©s (LOW/MEDIUM/HIGH/CRITICAL)\n",
    "- **üí° Recommandations** d'actions concr√®tes\n",
    "- **ü§ñ Tool-calling** naturel et contextuel\n",
    "\n",
    "### üöÄ Prochaines √âtapes Recommand√©es :\n",
    "\n",
    "1. **üìä Dataset R√©el** : Collectez et annotez vos propres donn√©es de surveillance\n",
    "2. **üîÑ Fine-tuning It√©ratif** : Am√©liorez progressivement avec feedback terrain\n",
    "3. **üìà √âvaluation Quantitative** : M√©triques objectives (BLEU, ROUGE, m√©triques m√©tier)\n",
    "4. **üß™ Tests A/B** : Comparaison avec mod√®le de base en conditions r√©elles\n",
    "5. **üîß Optimisation** : Quantization, distillation pour d√©ploiement edge\n",
    "\n",
    "### üí° Conseils pour la Production :\n",
    "\n",
    "- **üìù Collecte Continue** : Enrichissez le dataset avec nouveaux sc√©narios\n",
    "- **üîç Monitoring** : Surveillez la qualit√© des r√©ponses en temps r√©el\n",
    "- **üë• Feedback Loop** : Int√©grez retours des √©quipes de s√©curit√©\n",
    "- **üõ°Ô∏è Validation Humaine** : Gardez une supervision pour cas critiques\n",
    "- **üìö Documentation** : Maintenez guides d'utilisation √† jour\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations ! Vous ma√Ætrisez maintenant le fine-tuning de mod√®les VLM pour la surveillance.**\n",
    "\n",
    "**üìö Ressources Utiles :**\n",
    "- [Documentation Transformers](https://huggingface.co/transformers/)\n",
    "- [Guide PEFT/LoRA](https://github.com/huggingface/peft)\n",
    "- [LLaVA Paper](https://arxiv.org/abs/2304.08485)\n",
    "- [Syst√®me de Surveillance Complet](https://github.com/elfried-kinzoun/intelligent-surveillance-system)\n",
    "\n",
    "*D√©velopp√© pour r√©volutionner la surveillance intelligente avec l'IA*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}